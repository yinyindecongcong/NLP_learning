{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多标签模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, num_classes, batch_size, vocab_size, embed_size, sentence_len, \n",
    "                 learning_rate, decay_step, decay_rate, filter_num, filter_sizes):\n",
    "        #1.定义超参数\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.learning_rate = learning_rate\n",
    "        self.filter_num = filter_num\n",
    "        self.filter_sizes = filter_sizes #list，如[2,3,4],表示3个卷积核的长度（height）\n",
    "        self.filter_num_total = filter_num * len(filter_sizes)\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_epoch = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_epoch') #在指数衰减函数中会加一\n",
    "        self.epoch_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, self.epoch_step+tf.constant(1))\n",
    "        self.decay_step = decay_step\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        #2.设置输入\n",
    "        self.sentence = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len], name='sentence')\n",
    "        #self.label = tf.placeholder(dtype=tf.int32, shape=[None], name='label')\n",
    "        self.label_l1999 = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name='label_l1999')\n",
    "        self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "        #self.dropout_keep_prob = 0.5\n",
    "        \n",
    "        #3.参数初始化\n",
    "        self.instantiate_weight()\n",
    "        #4.定义图\n",
    "        self.logits = self.inference()\n",
    "        \n",
    "        #5.定义loss和train_op\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "#         #6.预测，计算准确率\n",
    "#         self.prediction = tf.argmax(self.logits, axis=1, name='prediction')\n",
    "#         correct_pre = tf.equal(tf.cast(self.prediction, tf.int32), self.label)\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "    \n",
    "    def instantiate_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], tf.float32, initializer=self.initializer)\n",
    "        self.W = tf.get_variable('weight', [self.filter_num_total, self.num_classes], tf.float32, initializer=self.initializer)\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        #embedding -- 卷积 -- 线性分类器\n",
    "        self.sentece_embedding = tf.nn.embedding_lookup(self.Embedding, self.sentence)\n",
    "        h = self.cnn_single_layer()\n",
    "        logits = tf.matmul(h, self.W) + self.b\n",
    "        return logits\n",
    "    \n",
    "    def cnn_single_layer(self):\n",
    "        #conv2d -- BN -- ReLU -- max_pooling -- dropout -- dense\n",
    "        #conv2d的输入与卷积核都要求是4维的，具体查看文档\n",
    "        sentece_embedding_4d = tf.expand_dims(self.sentece_embedding, -1) #增加一维，[batch_size, sentence_len, embed_size, 1]\n",
    "        pool_output = []\n",
    "        for filter_size in self.filter_sizes:\n",
    "            with tf.variable_scope('convolution-pooling-%d'%filter_size):\n",
    "                ft = tf.get_variable('filter%d'%filter_size, [filter_size, self.embed_size, 1, self.filter_num], \n",
    "                                     tf.float32, initializer=self.initializer)\n",
    "                conv = tf.nn.conv2d(sentece_embedding_4d, ft, strides=[1,1,1,1], padding='VALID')\n",
    "                conv = tf.contrib.layers.batch_norm(conv) #[batch_size, sentence_len-filter_size+1, 1, filter_num]\n",
    "                activation = tf.nn.relu(conv)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(activation, ksize=[1,self.sentence_len-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID')\n",
    "                pool_output.append(pooled) #若干个shape=[batch_size, 1, 1, filter_num]\n",
    "        pool_concat = tf.concat(pool_output, axis=3) #在第三维拼接\n",
    "        flatten_pool = tf.reshape(pool_concat, [-1, self.filter_num_total])\n",
    "        \n",
    "        dropouted = tf.nn.dropout(flatten_pool, keep_prob=self.dropout_keep_prob)\n",
    "        h = tf.layers.dense(dropouted, self.filter_num_total, activation=tf.nn.tanh)\n",
    "        return h\n",
    "        \n",
    "    def loss(self, l2_lambda=0.001):\n",
    "        loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_l1999, logits=self.logits)\n",
    "        loss1 = tf.reduce_mean(tf.reduce_sum(loss1, axis=1))\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name], name='l2_loss') * l2_lambda\n",
    "        loss = loss1 + l2_loss\n",
    "        return loss\n",
    "                \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_epoch, \n",
    "                                                   self.decay_step, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_epoch, learning_rate, optimizer='Adam')\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=10\n",
    "    learning_rate=0.01\n",
    "    batch_size=5\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextCNN(num_classes, batch_size, vocab_size, embed_size, sequence_length,\n",
    "                     learning_rate, decay_step, decay_rate, 50, [2,3,4])\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 2,size=(batch_size, num_classes), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, logits, _ = sess.run([model.loss_val, model.logits, model.train_op],\n",
    "                                            feed_dict={model.sentence: input_x, model.label_l1999: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob})\n",
    "            logits = np.argsort(logits)\n",
    "            print('loss:',loss, 'label:', input_y, 'pre:', logits)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'weight:0' shape=(150, 10) dtype=float32_ref>, <tf.Variable 'b:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/filter2:0' shape=(2, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/filter3:0' shape=(3, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/filter4:0' shape=(4, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(150, 150) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(150,) dtype=float32_ref>]\n",
      "loss: 13.15637 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[5 0 2 6 9 3 7 1 8 4]\n",
      " [0 4 2 7 6 1 5 9 3 8]\n",
      " [5 6 1 0 3 2 8 4 9 7]\n",
      " [9 6 1 2 5 7 3 0 4 8]\n",
      " [5 6 1 2 7 4 0 3 9 8]]\n",
      "loss: 9.182357 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[3 6 1 7 9 4 2 0 8 5]\n",
      " [3 1 7 6 2 0 5 9 4 8]\n",
      " [3 8 6 7 1 5 0 2 4 9]\n",
      " [3 8 1 6 7 0 9 2 5 4]\n",
      " [3 8 7 0 6 1 2 9 4 5]]\n",
      "loss: 6.588595 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[3 9 2 7 4 1 8 6 0 5]\n",
      " [3 7 1 2 6 0 9 8 5 4]\n",
      " [3 8 6 2 5 1 0 7 4 9]\n",
      " [3 8 1 6 7 2 9 5 0 4]\n",
      " [3 0 8 6 1 2 7 9 5 4]]\n",
      "loss: 4.879135 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[3 1 9 2 4 7 0 8 6 5]\n",
      " [3 1 7 2 8 6 9 0 4 5]\n",
      " [3 8 6 5 9 1 0 4 2 7]\n",
      " [3 1 8 6 2 0 5 7 4 9]\n",
      " [3 0 6 8 2 9 1 7 5 4]]\n",
      "loss: 4.3023753 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[3 1 2 7 0 4 9 6 8 5]\n",
      " [3 1 2 7 6 0 9 4 8 5]\n",
      " [3 8 6 5 0 9 1 4 2 7]\n",
      " [3 6 1 8 0 5 2 7 9 4]\n",
      " [3 6 0 8 1 2 9 7 5 4]]\n",
      "loss: 3.398108 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 3 7 9 0 2 4 6 8 5]\n",
      " [3 1 2 7 6 0 9 8 4 5]\n",
      " [3 6 8 5 0 1 9 4 2 7]\n",
      " [3 6 1 8 0 2 9 7 5 4]\n",
      " [3 6 0 2 1 7 8 9 4 5]]\n",
      "loss: 2.709392 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 3 4 0 9 6 8 5]\n",
      " [1 3 2 7 6 0 8 9 4 5]\n",
      " [3 5 6 8 0 9 1 4 2 7]\n",
      " [3 6 1 8 5 2 0 7 9 4]\n",
      " [3 6 0 2 1 9 8 7 5 4]]\n",
      "loss: 2.3352194 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 0 2 3 9 4 6 8 5]\n",
      " [1 7 2 3 9 4 0 6 5 8]\n",
      " [3 6 5 8 1 0 9 2 4 7]\n",
      " [3 6 8 1 5 0 7 2 9 4]\n",
      " [3 6 0 1 2 8 7 9 4 5]]\n",
      "loss: 1.981805 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 3 9 4 0 6 5 8]\n",
      " [1 7 3 2 0 6 4 9 8 5]\n",
      " [5 3 6 8 1 9 0 4 2 7]\n",
      " [3 6 8 1 5 0 9 4 2 7]\n",
      " [3 6 0 5 8 1 2 7 9 4]]\n",
      "loss: 1.7319801 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 4 9 3 0 6 5 8]\n",
      " [1 3 2 7 6 4 0 8 9 5]\n",
      " [3 6 5 8 0 1 4 2 9 7]\n",
      " [3 6 8 1 5 0 2 7 9 4]\n",
      " [3 6 0 1 5 8 7 2 4 9]]\n",
      "loss: 1.4598157 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 4 3 9 0 6 8 5]\n",
      " [1 7 2 3 9 4 0 6 8 5]\n",
      " [3 6 8 5 1 0 4 7 2 9]\n",
      " [3 6 1 8 5 7 2 0 4 9]\n",
      " [3 6 0 1 7 8 5 2 9 4]]\n",
      "loss: 1.3059629 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 3 4 9 0 6 5 8]\n",
      " [1 7 2 3 4 9 0 5 8 6]\n",
      " [3 6 8 5 1 0 7 2 4 9]\n",
      " [3 1 6 8 5 4 7 0 9 2]\n",
      " [3 6 0 8 5 1 7 9 2 4]]\n",
      "loss: 1.151251 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 4 9 0 3 5 6 8]\n",
      " [1 3 7 2 8 4 6 0 9 5]\n",
      " [3 6 8 5 1 0 4 7 9 2]\n",
      " [3 8 6 1 5 7 2 0 4 9]\n",
      " [3 0 6 7 1 2 9 8 5 4]]\n",
      "loss: 1.0590174 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[7 1 2 9 4 0 3 5 8 6]\n",
      " [1 7 3 2 4 9 0 8 6 5]\n",
      " [6 5 3 8 1 0 4 2 7 9]\n",
      " [3 6 8 1 5 7 2 0 4 9]\n",
      " [3 6 0 1 8 5 2 7 4 9]]\n",
      "loss: 0.9044421 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 4 9 3 0 5 6 8]\n",
      " [1 7 2 3 4 9 8 0 5 6]\n",
      " [3 6 8 5 1 0 4 2 7 9]\n",
      " [3 8 6 1 5 7 0 4 9 2]\n",
      " [3 6 0 1 8 7 2 5 4 9]]\n",
      "loss: 0.8305942 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[7 1 2 4 0 9 3 5 6 8]\n",
      " [1 7 3 2 4 0 9 8 6 5]\n",
      " [6 3 8 5 0 1 4 9 7 2]\n",
      " [3 6 8 1 5 7 0 2 4 9]\n",
      " [3 0 6 8 7 1 5 2 9 4]]\n",
      "loss: 0.7912458 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 4 2 0 9 3 6 5 8]\n",
      " [1 7 3 2 8 4 5 6 0 9]\n",
      " [6 3 8 5 1 0 4 7 9 2]\n",
      " [3 6 1 8 5 2 7 0 4 9]\n",
      " [3 6 0 8 5 1 7 2 4 9]]\n",
      "loss: 0.7811043 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 4 9 3 0 5 8 6]\n",
      " [1 7 2 3 4 0 9 8 5 6]\n",
      " [6 8 3 5 1 4 7 0 2 9]\n",
      " [3 6 8 1 5 0 7 2 4 9]\n",
      " [3 6 0 8 5 1 7 2 4 9]]\n",
      "loss: 0.6965587 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[1 7 2 4 0 9 3 5 6 8]\n",
      " [3 7 1 2 6 0 8 4 5 9]\n",
      " [6 3 8 5 1 0 4 7 2 9]\n",
      " [3 6 8 1 5 0 4 7 2 9]\n",
      " [3 6 0 8 7 1 5 2 4 9]]\n",
      "loss: 0.661488 label: [[0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1]\n",
      " [1 1 1 0 1 0 0 1 0 1]\n",
      " [1 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 0 1 1 0 1 1 1]] pre: [[7 1 2 9 4 0 3 5 8 6]\n",
      " [1 7 3 2 4 0 5 9 8 6]\n",
      " [6 8 3 5 0 1 4 7 2 9]\n",
      " [3 6 8 1 5 7 0 2 4 9]\n",
      " [3 6 0 8 7 5 1 2 4 9]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define hyperparameter\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('label_size', 1999, 'number of label')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size for training')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 200, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0003, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 1, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 1000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 15, '')\n",
    "tf.app.flags.DEFINE_integer('validation_every', 1, 'Validate every validate_every epochs.')\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"textcnn_multilabel_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"textcnn_multilabel_checkpoint/data_cache.pik\",\"data chche for the model\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"num_filters\", 128, \"number of filters\") #256--->512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main\n",
    "\n",
    "#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data & training (4.validation) \n",
    "\n",
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = '/data/chenhy/data/ieee_zhihu_cup/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    \n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "\n",
    "    #print(\"train_X.shape:\", np.array(train_X).shape)\n",
    "    #print(\"train_y.shape:\", np.array(train_y).shape)\n",
    "    print(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "    print(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "    #print(\"test_X[0]:\", test_X[0])  \n",
    "    #print(\"test_X[1]:\", test_X[1])\n",
    "    #print(\"test_y[0]:\", test_y[0])  \n",
    "\n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextCNN(FLAGS.label_size, FLAGS.batch_size, vocab_size, \n",
    "                        FLAGS.embed_size, FLAGS.sentence_len, FLAGS.learning_rate, \n",
    "                        FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.num_filters, [2,3,4])\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        CONTINUE_TRAIN = False\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            print('restore model from checkpoint')\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            print('CONTINUE_TRAIN=', CONTINUE_TRAIN)\n",
    "            sess.run(model.epoch_increment)\n",
    "            print('Continue at Epoch:', sess.run(model.epoch_step))\n",
    "        if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint') or CONTINUE_TRAIN:\n",
    "            if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "                print('initialize variables')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                #print('assign pre-trained embedding')\n",
    "                #embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "                #sess.run(embedding_assign)\n",
    "\n",
    "            #3.训练\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = sess.run(model.epoch_step)\n",
    "                loss, counter = 0.0, 0\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    if (epoch == 0 and counter == 0):\n",
    "                        print('train_X[start, end]:', train_X[start:end])\n",
    "                        print('train_y[start, end]:', train_y[start:end])\n",
    "                    l,_ = sess.run([model.loss_val, model.train_op], \n",
    "                                feed_dict={model.sentence: train_X[start:end], model.label_l1999: train_y[start:end],\n",
    "                                           model.dropout_keep_prob: 0.5})\n",
    "                    loss, counter = loss+l, counter+1\n",
    "\n",
    "                    if (counter % 100 == 0):\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\"%(epoch, counter, loss/float(counter)))\n",
    "\n",
    "                    if counter % 3000 == 0:\n",
    "                        print('run model on validation data...')\n",
    "                        loss_valid, f1_score, precision, recall = do_eval(sess, model, vaild_X, valid_y)\n",
    "                        log(\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"%(epoch, loss_valid, f1_score, precision, recall))\n",
    "                        #save the checkpoint\n",
    "                        save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                        saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "        loss_valid, f1_score, precision, recall = do_eval(sess, model, vaild_X, valid_y)\n",
    "        log(\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"%(epoch, loss_valid, f1_score, precision, recall))\n",
    "                        \n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    #embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y\n",
    "\n",
    "def do_eval(sess, model, test_X, test_y):\n",
    "    test_X, text_y = test_X[:3000], test_y[:3000]\n",
    "    num_of_data = len(test_y)\n",
    "    batch_size = 1\n",
    "    loss, F1, p, r = 0., 0., 0., 0.\n",
    "    label_dict_confuse = {'TP':0., 'FN':0., 'FP':0.}\n",
    "    for start in range(num_of_data):\n",
    "        end = start + 1\n",
    "        l,logits = sess.run([model.loss_val, model.logits], \n",
    "                        feed_dict={model.sentence: test_X[start:end], model.label_l1999: test_y[start:end],\n",
    "                                   model.dropout_keep_prob:1.0})\n",
    "        loss += l\n",
    "        pre = np.argsort(logits[0])[-5:]\n",
    "        label = [i for i in range(len(test_y[start])) if test_y[start][i] > 0]\n",
    "        if start == 0: print('label:',label, 'predict:', pre)\n",
    "        inter = len([x for x in pre if x in label])\n",
    "        label_dict_confuse['TP'] += inter\n",
    "        label_dict_confuse['FN'] += len(label) - inter\n",
    "        label_dict_confuse['FP'] += len(pre) - inter\n",
    "    p = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FP'])\n",
    "    r = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FN'])\n",
    "    if p + r == 0: return loss/num_of_data, 0, 0, 0\n",
    "    F1 = (2 * p * r)/(p + r)\n",
    "    return loss/num_of_data, F1/num_of_data, p/num_of_data, r/num_of_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "test_X.shape: (20000, 200)\n",
      "test_y.shape: (20000, 1999)\n",
      "initialize variables\n",
      "train_X[start, end]: [[832  60 256 ...   0   0   0]\n",
      " [270 154 166 ...   0   0   0]\n",
      " [186 163 284 ...   0   0   0]\n",
      " ...\n",
      " [ 96 138 117 ...   0   0   0]\n",
      " [ 56 109  96 ...   0   0   0]\n",
      " [ 32 127 420 ...   0   0   0]]\n",
      "train_y[start, end]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[2019/02/28 17:25:04] Epoch 0\\Batch 100\\ Train Loss:349.865\n",
      "[2019/02/28 17:25:25] Epoch 0\\Batch 200\\ Train Loss:191.907\n",
      "[2019/02/28 17:25:46] Epoch 0\\Batch 300\\ Train Loss:137.243\n",
      "[2019/02/28 17:26:07] Epoch 0\\Batch 400\\ Train Loss:109.365\n",
      "[2019/02/28 17:26:27] Epoch 0\\Batch 500\\ Train Loss:92.445\n",
      "[2019/02/28 17:26:47] Epoch 0\\Batch 600\\ Train Loss:81.074\n",
      "[2019/02/28 17:27:07] Epoch 0\\Batch 700\\ Train Loss:72.898\n",
      "[2019/02/28 17:27:27] Epoch 0\\Batch 800\\ Train Loss:66.731\n",
      "[2019/02/28 17:27:48] Epoch 0\\Batch 900\\ Train Loss:61.930\n",
      "[2019/02/28 17:28:08] Epoch 0\\Batch 1000\\ Train Loss:58.060\n",
      "[2019/02/28 17:28:28] Epoch 0\\Batch 1100\\ Train Loss:54.895\n",
      "[2019/02/28 17:28:46] Epoch 0\\Batch 1200\\ Train Loss:52.253\n",
      "[2019/02/28 17:29:03] Epoch 0\\Batch 1300\\ Train Loss:50.004\n",
      "[2019/02/28 17:29:20] Epoch 0\\Batch 1400\\ Train Loss:48.073\n",
      "[2019/02/28 17:29:38] Epoch 0\\Batch 1500\\ Train Loss:46.393\n",
      "[2019/02/28 17:29:56] Epoch 0\\Batch 1600\\ Train Loss:44.936\n",
      "[2019/02/28 17:30:15] Epoch 0\\Batch 1700\\ Train Loss:43.649\n",
      "[2019/02/28 17:30:35] Epoch 0\\Batch 1800\\ Train Loss:42.494\n",
      "[2019/02/28 17:30:56] Epoch 0\\Batch 1900\\ Train Loss:41.455\n",
      "[2019/02/28 17:31:18] Epoch 0\\Batch 2000\\ Train Loss:40.520\n",
      "[2019/02/28 17:31:38] Epoch 0\\Batch 2100\\ Train Loss:39.675\n",
      "[2019/02/28 17:31:59] Epoch 0\\Batch 2200\\ Train Loss:38.906\n",
      "[2019/02/28 17:32:19] Epoch 0\\Batch 2300\\ Train Loss:38.203\n",
      "[2019/02/28 17:32:39] Epoch 0\\Batch 2400\\ Train Loss:37.550\n",
      "[2019/02/28 17:32:59] Epoch 0\\Batch 2500\\ Train Loss:36.950\n",
      "[2019/02/28 17:33:20] Epoch 0\\Batch 2600\\ Train Loss:36.389\n",
      "[2019/02/28 17:33:41] Epoch 0\\Batch 2700\\ Train Loss:35.872\n",
      "[2019/02/28 17:34:02] Epoch 0\\Batch 2800\\ Train Loss:35.392\n",
      "[2019/02/28 17:34:23] Epoch 0\\Batch 2900\\ Train Loss:34.936\n",
      "[2019/02/28 17:34:43] Epoch 0\\Batch 3000\\ Train Loss:34.512\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [ 6 14  0  4  1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-882d49ed749b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ae83e867246a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m3000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run model on validation data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                         \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaild_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0;31m#save the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ae83e867246a>\u001b[0m in \u001b[0;36mdo_eval\u001b[0;34m(sess, model, test_X, test_y)\u001b[0m\n\u001b[1;32m    107\u001b[0m                                    model.dropout_keep_prob:1.0})\n\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
