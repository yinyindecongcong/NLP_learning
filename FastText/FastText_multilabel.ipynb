{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义多目标分类FastText模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastText:\n",
    "    def __init__(self, label_size, batch_size, num_sampled, sentence_len, vocab_size, \n",
    "                 embed_size, learning_rate, decay_rate, decay_steps, is_training):\n",
    "        #init all hyperparameter\n",
    "        self.label_size = label_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.sentence_len = sentence_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #add placeholder \n",
    "        self.sentence = tf.placeholder(dtype=tf.int32, shape=[None, sentence_len], name='sentence') #x\n",
    "        #self.labels = tf.placeholder(dtype=tf.int32, shape=[None], name='label') #y\n",
    "        self.label_l1999 = tf.placeholder(dtype=tf.float32, shape=[None, self.label_size])\n",
    "        \n",
    "        self.global_step = tf.Variable(0, name='Global_step', trainable=False)\n",
    "        self.epoch_step = tf.Variable(0, name='Epoch_step', trainable=False)\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        self.decay_rate, self.decay_steps = decay_rate, decay_steps\n",
    "        \n",
    "        self.instantiate_weights()\n",
    "        self.logits = self.inference()\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "        #self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
    "        #correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32), self.labels)\n",
    "        #self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='Accuracy')\n",
    "        \n",
    "    def instantiate_weights(self):\n",
    "        self.Embedding = tf.get_variable(name='Embedding', shape=[self.vocab_size, self.embed_size], initializer=self.initializer)\n",
    "        self.W = tf.get_variable(name='W', shape=[self.embed_size, self.label_size], initializer=self.initializer)\n",
    "        self.b = tf.get_variable(name='b', shape=[self.label_size])\n",
    "        \n",
    "    def inference(self):\n",
    "        sentence_embeddings = tf.nn.embedding_lookup(self.Embedding, self.sentence) #每个单词查表得到词向量，[None, sentence_len, embed_size]\n",
    "        self.sentence_embeddings = tf.reduce_mean(sentence_embeddings, axis=1) #求平均，[None, embed_size]\n",
    "        logits = tf.matmul(self.sentence_embeddings, self.W) + self.b #线性分类器\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, l2_lambda=0.01):\n",
    "        #nce loss\n",
    "#         if self.is_training:\n",
    "#             labels = tf.reshape(self.labels, shape=[-1])\n",
    "#             labels = tf.expand_dims(labels, dim=1)\n",
    "#             loss = tf.reduce_mean(tf.nn.nce_loss(weights=tf.transpose(self.W), \n",
    "#                                                  biases=self.b, \n",
    "#                                                  labels=labels, \n",
    "#                                                  inputs=self.sentence_embeddings, \n",
    "#                                                  num_sampled=self.num_sampled, \n",
    "#                                                  num_classes=self.label_size, \n",
    "#                                                  partition_strategy='div'))\n",
    "            \n",
    "#         else:\n",
    "#             pass\n",
    "        '''多目标分类，使用sigmoid_cross_entropy_with_logits做损失函数'''\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_l1999, logits=self.logits)\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "        #增加l2正则项\n",
    "        print(tf.trainable_variables())\n",
    "        self.l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "        loss = loss + self.l2_loss\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=19\n",
    "    learning_rate=0.01\n",
    "    batch_size=8\n",
    "    decay_steps=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=1\n",
    "    \n",
    "    model = FastText(num_classes, batch_size, 5, sequence_length, vocab_size, \n",
    "                     embed_size, learning_rate, decay_rate, decay_steps, is_training)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 2,size=(batch_size, num_classes), dtype=np.int32)\n",
    "        for i in range(1):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, logit, _ = sess.run([model.loss_val, model.logits, model.train_op],\n",
    "                                            feed_dict={model.sentence: input_x, model.label_l1999: input_y})\n",
    "            tmp = np.zeros((batch_size, num_classes),dtype=np.int32)\n",
    "            for i in range(batch_size):\n",
    "                top_num = int(input_y[i].sum())\n",
    "                tmp[i][np.argsort(logit[i])[-top_num:]] = 1\n",
    "            print('loss:',loss, 'label:', input_y,'**\\n', 'logits:', tmp, 'acc:', (tmp==input_y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'W:0' shape=(100, 19) dtype=float32_ref>, <tf.Variable 'b:0' shape=(19,) dtype=float32_ref>]\n",
      "loss: 63.44268 label: [[1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1]\n",
      " [1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1]\n",
      " [1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1]\n",
      " [1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0]\n",
      " [0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1]\n",
      " [0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0]\n",
      " [0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0]\n",
      " [0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0]] **\n",
      " logits: [[1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1]\n",
      " [1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0]\n",
      " [1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0]\n",
      " [1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0]\n",
      " [1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0]\n",
      " [1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0]] acc: 78\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import word2vec\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'label_size' is defined twice. First from D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py, Second from D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py.  Description from first occurrence: number of label",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f9a5fd496626>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'number of label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch size for training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'num_sampled'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'number of noise sample'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m           \u001b[1;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[1;34m(name, default, help, lower_bound, upper_bound, flag_values, **args)\u001b[0m\n\u001b[0;32m    313\u001b[0m   \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegerParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m   \u001b[0mDEFINE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[1;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[0;32m     80\u001b[0m   \"\"\"\n\u001b[0;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[1;32m---> 82\u001b[1;33m               flag_values, module_name)\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[1;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[1;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m   \u001b[0mfv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m   \u001b[1;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, flag)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDuplicateFlagError\u001b[0m: The flag 'label_size' is defined twice. First from D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py, Second from D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py.  Description from first occurrence: number of label"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#define hyperparameter\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('label_size', 1999, 'number of label')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128, 'batch size for training')\n",
    "tf.app.flags.DEFINE_integer('num_sampled', 50, 'number of noise sample')\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 200, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 20000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 15, '')\n",
    "tf.app.flags.DEFINE_integer('validation_every', 1, 'Validate every validate_every epochs.')\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"D:/zhihu_data/data/ieee_zhihu_cup/fast_text_multilabel_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"D:/zhihu_data/data/ieee_zhihu_cup/fast_textmultilabel__checkpoint/data_cache.pik\",\"data chche for the model\")\n",
    "\n",
    "tf.app.flags.DEFINE_bool('use_embedding', False, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define main\n",
    "\n",
    "#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data & training (4.validation) \n",
    "\n",
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = 'D:/zhihu_data/data/ieee_zhihu_cup/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    \n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "    '''\n",
    "    print(\"train_X.shape:\", np.array(train_X).shape)\n",
    "    print(\"train_y.shape:\", np.array(train_y).shape)\n",
    "    '''\n",
    "    print(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "    print(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "    \n",
    "    print(\"test_X[0]:\", test_X[0])  \n",
    "    print(\"test_X[1]:\", test_X[1])\n",
    "    print(\"test_y[0]:\", test_y[0])  \n",
    "\n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = FastText(FLAGS.label_size, FLAGS.batch_size, FLAGS.num_sampled, FLAGS.sentence_len, \n",
    "                        vocab_size, FLAGS.embed_size, FLAGS.learning_rate, FLAGS.decay_rate, FLAGS.decay_steps, FLAGS.is_training)\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        CONTINUE_TRAIN = False\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            print('restore model from checkpoint')\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            print('CONTINUE_TRAIN=', CONTINUE_TRAIN)\n",
    "            sess.run(model.epoch_increment)\n",
    "            print('Continue at Epoch:', sess.run(model.epoch_step))\n",
    "        if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint') or CONTINUE_TRAIN:\n",
    "            if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "                log('initialize variables')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                if FLAGS.use_embedding:\n",
    "                    print('assign pre-trained embedding')\n",
    "                    #embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "                    #sess.run(embedding_assign)\n",
    "\n",
    "            #3.训练\n",
    "            num_of_data = len(train_X)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                curr_epoch = sess.run(model.epoch_step)\n",
    "                loss, acc, counter = 0.0, 0.0, 0\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    if (curr_epoch == 0 and counter == 0):\n",
    "                        print('train_X[start, end]:', train_X[start:end])\n",
    "                        print('train_y[start, end]:', train_y[start:end])\n",
    "                    loss_tmp, l2, _ = sess.run([model.loss_val, model.l2_loss, model.train_op], \n",
    "                                feed_dict={model.sentence: train_X[start:end], model.label_l1999: train_y[start:end]})\n",
    "                    loss, counter = loss+loss_tmp, counter+1\n",
    "\n",
    "                    if (counter % 200 == 0):\n",
    "                        print(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\"%(curr_epoch, counter, loss/float(counter)))\n",
    "\n",
    "                #4.验证，每迭代完FLAGS.validation_every轮，在验证集上跑一次\n",
    "                print(curr_epoch,FLAGS.validation_every,(curr_epoch % FLAGS.validation_every==0))\n",
    "                if curr_epoch % FLAGS.validation_every == 0:\n",
    "                    log('run model on validation data...')\n",
    "                    loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y)\n",
    "                    log(\"Epoch %d\\ Validation Loss:%.3f/ Validation Accuracy:%.3f\"%(curr_epoch, loss_valid, acc_valid))\n",
    "                    #save the checkpoint\n",
    "                    save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                    saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "        loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "        print(\"Validation Loss:%.3f\\ Validation Accuracy:%.3f\"%(loss_valid, acc_valid))\n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    log('cache files exist, going to load in...')\n",
    "    log('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    #embedding_final = h5_file['embedding']\n",
    "\n",
    "    log('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    log('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y\n",
    "\n",
    "def do_eval(sess, model, test_X, test_y):\n",
    "    loss, acc = 0.0, 0.0\n",
    "    batch_size = 1\n",
    "    for start, end in zip(range(0, len(test_X), batch_size), range(batch_size, len(test_X), batch_size)):\n",
    "        l,pre = sess.run([model.loss_val, model.logits], \n",
    "                        feed_dict={model.sentence: test_X[start:end], model.label_l1999: test_y[start:end]})\n",
    "        loss += l\n",
    "        acc += calc_accuracy(pre[0], test_y[start])\n",
    "    return loss/float(len(test_y)), acc/float(len(test_y))\n",
    "\n",
    "def calc_accuracy(pre_row, label_row):\n",
    "    label_row = np.array(label_row)\n",
    "    top_num = int(np.sum(label_row))\n",
    "    index_list=np.array(np.argsort(pre_row))[-top_num:]\n",
    "    return label_row[index_list].sum()/float(top_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/25 19:06:23] cache files exist, going to load in...\n",
      "[2019/02/25 19:06:23] loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "[2019/02/25 19:06:23] loading pickle file\n",
      "[2019/02/25 19:06:23] cache files load successful!\n",
      "test_X.shape: (20000, 200)\n",
      "test_y.shape: (20000, 1999)\n",
      "test_X[0]: [ 937  716  934  376  104  652  304  934  376   19  240  221  136   68\n",
      "  188   96  130  130   96  209  505   12  703  143   12  652  304  934\n",
      "  376   10   13  408   89   74   32  110  558  909 2519   12   80  181\n",
      "   10  134  204  471  462  562   16    3  937  716  934  376   13   78\n",
      "  180  531  937  307   78  245  157  937  716  934  376   10  105   13\n",
      "   61  245  157  652  304  934  376   10  168   78   13  937  716  143\n",
      "  109   10   63 1191  369   99   13  937  716  934  376   16    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "test_X[1]: [310 426 390 252 440 142 407  94  33 102  16   3   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "test_y[0]: [0. 0. 0. ... 0. 0. 0.]\n",
      "[<tf.Variable 'Embedding:0' shape=(11982, 100) dtype=float32_ref>, <tf.Variable 'W:0' shape=(100, 1999) dtype=float32_ref>, <tf.Variable 'b:0' shape=(1999,) dtype=float32_ref>]\n",
      "[2019/02/25 19:06:24] initialize variables\n",
      "train_X[start, end]: [[ 832   60  256 ...    0    0    0]\n",
      " [ 270  154  166 ...    0    0    0]\n",
      " [ 186  163  284 ...    0    0    0]\n",
      " ...\n",
      " [1051  317  164 ...    0    0    0]\n",
      " [ 137  238   13 ...    0    0    0]\n",
      " [  18   43  227 ...    0    0    0]]\n",
      "train_y[start, end]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch 0\\Batch 200\\ Train Loss:132.937\n",
      "Epoch 0\\Batch 400\\ Train Loss:94.054\n",
      "Epoch 0\\Batch 600\\ Train Loss:79.892\n",
      "Epoch 0\\Batch 800\\ Train Loss:72.544\n",
      "Epoch 0\\Batch 1000\\ Train Loss:68.058\n",
      "Epoch 0\\Batch 1200\\ Train Loss:65.075\n",
      "Epoch 0\\Batch 1400\\ Train Loss:62.933\n",
      "Epoch 0\\Batch 1600\\ Train Loss:61.323\n",
      "Epoch 0\\Batch 1800\\ Train Loss:60.082\n",
      "Epoch 0\\Batch 2000\\ Train Loss:59.085\n",
      "Epoch 0\\Batch 2200\\ Train Loss:58.272\n",
      "Epoch 0\\Batch 2400\\ Train Loss:57.595\n",
      "Epoch 0\\Batch 2600\\ Train Loss:57.024\n",
      "Epoch 0\\Batch 2800\\ Train Loss:56.530\n",
      "Epoch 0\\Batch 3000\\ Train Loss:56.112\n",
      "Epoch 0\\Batch 3200\\ Train Loss:55.739\n",
      "Epoch 0\\Batch 3400\\ Train Loss:55.419\n",
      "Epoch 0\\Batch 3600\\ Train Loss:55.127\n",
      "Epoch 0\\Batch 3800\\ Train Loss:54.864\n",
      "Epoch 0\\Batch 4000\\ Train Loss:54.626\n",
      "Epoch 0\\Batch 4200\\ Train Loss:54.413\n",
      "Epoch 0\\Batch 4400\\ Train Loss:54.229\n",
      "Epoch 0\\Batch 4600\\ Train Loss:54.052\n",
      "Epoch 0\\Batch 4800\\ Train Loss:53.885\n",
      "Epoch 0\\Batch 5000\\ Train Loss:53.735\n",
      "Epoch 0\\Batch 5200\\ Train Loss:53.595\n",
      "Epoch 0\\Batch 5400\\ Train Loss:53.460\n",
      "Epoch 0\\Batch 5600\\ Train Loss:53.331\n",
      "Epoch 0\\Batch 5800\\ Train Loss:53.218\n",
      "Epoch 0\\Batch 6000\\ Train Loss:53.113\n",
      "Epoch 0\\Batch 6200\\ Train Loss:53.014\n",
      "Epoch 0\\Batch 6400\\ Train Loss:52.915\n",
      "Epoch 0\\Batch 6600\\ Train Loss:52.824\n",
      "Epoch 0\\Batch 6800\\ Train Loss:52.744\n",
      "Epoch 0\\Batch 7000\\ Train Loss:52.662\n",
      "Epoch 0\\Batch 7200\\ Train Loss:52.586\n",
      "Epoch 0\\Batch 7400\\ Train Loss:52.511\n",
      "Epoch 0\\Batch 7600\\ Train Loss:52.438\n",
      "Epoch 0\\Batch 7800\\ Train Loss:52.373\n",
      "Epoch 0\\Batch 8000\\ Train Loss:52.310\n",
      "Epoch 0\\Batch 8200\\ Train Loss:52.247\n",
      "Epoch 0\\Batch 8400\\ Train Loss:52.191\n",
      "Epoch 0\\Batch 8600\\ Train Loss:52.140\n",
      "Epoch 0\\Batch 8800\\ Train Loss:52.085\n",
      "Epoch 0\\Batch 9000\\ Train Loss:52.034\n",
      "Epoch 0\\Batch 9200\\ Train Loss:51.983\n",
      "Epoch 0\\Batch 9400\\ Train Loss:51.936\n",
      "Epoch 0\\Batch 9600\\ Train Loss:51.888\n",
      "Epoch 0\\Batch 9800\\ Train Loss:51.844\n",
      "Epoch 0\\Batch 10000\\ Train Loss:51.801\n",
      "Epoch 0\\Batch 10200\\ Train Loss:51.760\n",
      "Epoch 0\\Batch 10400\\ Train Loss:51.725\n",
      "Epoch 0\\Batch 10600\\ Train Loss:51.685\n",
      "Epoch 0\\Batch 10800\\ Train Loss:51.648\n",
      "Epoch 0\\Batch 11000\\ Train Loss:51.611\n",
      "Epoch 0\\Batch 11200\\ Train Loss:51.579\n",
      "Epoch 0\\Batch 11400\\ Train Loss:51.546\n",
      "Epoch 0\\Batch 11600\\ Train Loss:51.514\n",
      "Epoch 0\\Batch 11800\\ Train Loss:51.485\n",
      "Epoch 0\\Batch 12000\\ Train Loss:51.458\n",
      "Epoch 0\\Batch 12200\\ Train Loss:51.428\n",
      "Epoch 0\\Batch 12400\\ Train Loss:51.399\n",
      "Epoch 0\\Batch 12600\\ Train Loss:51.371\n",
      "Epoch 0\\Batch 12800\\ Train Loss:51.343\n",
      "Epoch 0\\Batch 13000\\ Train Loss:51.318\n",
      "Epoch 0\\Batch 13200\\ Train Loss:51.292\n",
      "Epoch 0\\Batch 13400\\ Train Loss:51.269\n",
      "Epoch 0\\Batch 13600\\ Train Loss:51.245\n",
      "Epoch 0\\Batch 13800\\ Train Loss:51.222\n",
      "Epoch 0\\Batch 14000\\ Train Loss:51.200\n",
      "Epoch 0\\Batch 14200\\ Train Loss:51.180\n",
      "Epoch 0\\Batch 14400\\ Train Loss:51.160\n",
      "Epoch 0\\Batch 14600\\ Train Loss:51.140\n",
      "Epoch 0\\Batch 14800\\ Train Loss:51.119\n",
      "Epoch 0\\Batch 15000\\ Train Loss:51.102\n",
      "Epoch 0\\Batch 15200\\ Train Loss:51.085\n",
      "Epoch 0\\Batch 15400\\ Train Loss:51.066\n",
      "Epoch 0\\Batch 15600\\ Train Loss:51.050\n",
      "Epoch 0\\Batch 15800\\ Train Loss:51.033\n",
      "Epoch 0\\Batch 16000\\ Train Loss:51.014\n",
      "Epoch 0\\Batch 16200\\ Train Loss:50.995\n",
      "Epoch 0\\Batch 16400\\ Train Loss:50.978\n",
      "Epoch 0\\Batch 16600\\ Train Loss:50.962\n",
      "Epoch 0\\Batch 16800\\ Train Loss:50.948\n",
      "Epoch 0\\Batch 17000\\ Train Loss:50.933\n",
      "Epoch 0\\Batch 17200\\ Train Loss:50.917\n",
      "Epoch 0\\Batch 17400\\ Train Loss:50.905\n",
      "Epoch 0\\Batch 17600\\ Train Loss:50.891\n",
      "Epoch 0\\Batch 17800\\ Train Loss:50.876\n",
      "Epoch 0\\Batch 18000\\ Train Loss:50.862\n",
      "Epoch 0\\Batch 18200\\ Train Loss:50.849\n",
      "Epoch 0\\Batch 18400\\ Train Loss:50.835\n",
      "Epoch 0\\Batch 18600\\ Train Loss:50.822\n",
      "Epoch 0\\Batch 18800\\ Train Loss:50.809\n",
      "Epoch 0\\Batch 19000\\ Train Loss:50.797\n",
      "Epoch 0\\Batch 19200\\ Train Loss:50.785\n",
      "Epoch 0\\Batch 19400\\ Train Loss:50.774\n",
      "Epoch 0\\Batch 19600\\ Train Loss:50.762\n",
      "Epoch 0\\Batch 19800\\ Train Loss:50.751\n",
      "Epoch 0\\Batch 20000\\ Train Loss:50.742\n",
      "Epoch 0\\Batch 20200\\ Train Loss:50.731\n",
      "Epoch 0\\Batch 20400\\ Train Loss:50.719\n",
      "Epoch 0\\Batch 20600\\ Train Loss:50.708\n",
      "Epoch 0\\Batch 20800\\ Train Loss:50.696\n",
      "Epoch 0\\Batch 21000\\ Train Loss:50.685\n",
      "Epoch 0\\Batch 21200\\ Train Loss:50.677\n",
      "Epoch 0\\Batch 21400\\ Train Loss:50.667\n",
      "Epoch 0\\Batch 21600\\ Train Loss:50.656\n",
      "Epoch 0\\Batch 21800\\ Train Loss:50.646\n",
      "Epoch 0\\Batch 22000\\ Train Loss:50.637\n",
      "Epoch 0\\Batch 22200\\ Train Loss:50.628\n",
      "Epoch 0\\Batch 22400\\ Train Loss:50.619\n",
      "Epoch 0\\Batch 22600\\ Train Loss:50.610\n",
      "Epoch 0\\Batch 22800\\ Train Loss:50.601\n",
      "Epoch 0\\Batch 23000\\ Train Loss:50.593\n",
      "0 1 True\n",
      "[2019/02/25 19:29:55] run model on validation data...\n",
      "[2019/02/25 19:30:16] Epoch 0\\ Validation Loss:49.606/ Validation Accuracy:0.017\n",
      "Epoch 1\\Batch 200\\ Train Loss:49.659\n",
      "Epoch 1\\Batch 400\\ Train Loss:49.649\n",
      "Epoch 1\\Batch 600\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 800\\ Train Loss:49.544\n",
      "Epoch 1\\Batch 1000\\ Train Loss:49.537\n",
      "Epoch 1\\Batch 1200\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 1400\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 1600\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 1800\\ Train Loss:49.568\n",
      "Epoch 1\\Batch 2000\\ Train Loss:49.569\n",
      "Epoch 1\\Batch 2200\\ Train Loss:49.569\n",
      "Epoch 1\\Batch 2400\\ Train Loss:49.570\n",
      "Epoch 1\\Batch 2600\\ Train Loss:49.570\n",
      "Epoch 1\\Batch 2800\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 3000\\ Train Loss:49.570\n",
      "Epoch 1\\Batch 3200\\ Train Loss:49.566\n",
      "Epoch 1\\Batch 3400\\ Train Loss:49.571\n",
      "Epoch 1\\Batch 3600\\ Train Loss:49.570\n",
      "Epoch 1\\Batch 3800\\ Train Loss:49.566\n",
      "Epoch 1\\Batch 4000\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 4200\\ Train Loss:49.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\\Batch 4400\\ Train Loss:49.571\n",
      "Epoch 1\\Batch 4600\\ Train Loss:49.571\n",
      "Epoch 1\\Batch 4800\\ Train Loss:49.568\n",
      "Epoch 1\\Batch 5000\\ Train Loss:49.569\n",
      "Epoch 1\\Batch 5200\\ Train Loss:49.568\n",
      "Epoch 1\\Batch 5400\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 5600\\ Train Loss:49.556\n",
      "Epoch 1\\Batch 5800\\ Train Loss:49.556\n",
      "Epoch 1\\Batch 6000\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 6200\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 6400\\ Train Loss:49.557\n",
      "Epoch 1\\Batch 6600\\ Train Loss:49.556\n",
      "Epoch 1\\Batch 6800\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 7000\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 7200\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 7400\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 7600\\ Train Loss:49.559\n",
      "Epoch 1\\Batch 7800\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 8000\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 8200\\ Train Loss:49.559\n",
      "Epoch 1\\Batch 8400\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 8600\\ Train Loss:49.567\n",
      "Epoch 1\\Batch 8800\\ Train Loss:49.565\n",
      "Epoch 1\\Batch 9000\\ Train Loss:49.565\n",
      "Epoch 1\\Batch 9200\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 9400\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 9600\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 9800\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 10000\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 10200\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 10400\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 10600\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 10800\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 11000\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 11200\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 11400\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 11600\\ Train Loss:49.559\n",
      "Epoch 1\\Batch 11800\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 12000\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 12200\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 12400\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 12600\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 12800\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 13000\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 13200\\ Train Loss:49.556\n",
      "Epoch 1\\Batch 13400\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 13600\\ Train Loss:49.557\n",
      "Epoch 1\\Batch 13800\\ Train Loss:49.557\n",
      "Epoch 1\\Batch 14000\\ Train Loss:49.557\n",
      "Epoch 1\\Batch 14200\\ Train Loss:49.558\n",
      "Epoch 1\\Batch 14400\\ Train Loss:49.559\n",
      "Epoch 1\\Batch 14600\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 14800\\ Train Loss:49.559\n",
      "Epoch 1\\Batch 15000\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 15200\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 15400\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 15600\\ Train Loss:49.565\n",
      "Epoch 1\\Batch 15800\\ Train Loss:49.565\n",
      "Epoch 1\\Batch 16000\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 16200\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 16400\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 16600\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 16800\\ Train Loss:49.563\n",
      "Epoch 1\\Batch 17000\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 17200\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 17400\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 17600\\ Train Loss:49.564\n",
      "Epoch 1\\Batch 17800\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 18000\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 18200\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 18400\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 18600\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 18800\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 19000\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 19200\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 19400\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 19600\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 19800\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 20000\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 20200\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 20400\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 20600\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 20800\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 21000\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 21200\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 21400\\ Train Loss:49.562\n",
      "Epoch 1\\Batch 21600\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 21800\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 22000\\ Train Loss:49.560\n",
      "Epoch 1\\Batch 22200\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 22400\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 22600\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 22800\\ Train Loss:49.561\n",
      "Epoch 1\\Batch 23000\\ Train Loss:49.562\n",
      "1 1 True\n",
      "[2019/02/25 19:53:49] run model on validation data...\n",
      "[2019/02/25 19:54:09] Epoch 1\\ Validation Loss:49.571/ Validation Accuracy:0.017\n",
      "Epoch 2\\Batch 200\\ Train Loss:49.622\n",
      "Epoch 2\\Batch 400\\ Train Loss:49.610\n",
      "Epoch 2\\Batch 600\\ Train Loss:49.523\n",
      "Epoch 2\\Batch 800\\ Train Loss:49.505\n",
      "Epoch 2\\Batch 1000\\ Train Loss:49.497\n",
      "Epoch 2\\Batch 1200\\ Train Loss:49.518\n",
      "Epoch 2\\Batch 1400\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 1600\\ Train Loss:49.523\n",
      "Epoch 2\\Batch 1800\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 2000\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 2200\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 2400\\ Train Loss:49.530\n",
      "Epoch 2\\Batch 2600\\ Train Loss:49.530\n",
      "Epoch 2\\Batch 2800\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 3000\\ Train Loss:49.530\n",
      "Epoch 2\\Batch 3200\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 3400\\ Train Loss:49.532\n",
      "Epoch 2\\Batch 3600\\ Train Loss:49.531\n",
      "Epoch 2\\Batch 3800\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 4000\\ Train Loss:49.522\n",
      "Epoch 2\\Batch 4200\\ Train Loss:49.522\n",
      "Epoch 2\\Batch 4400\\ Train Loss:49.532\n",
      "Epoch 2\\Batch 4600\\ Train Loss:49.533\n",
      "Epoch 2\\Batch 4800\\ Train Loss:49.530\n",
      "Epoch 2\\Batch 5000\\ Train Loss:49.531\n",
      "Epoch 2\\Batch 5200\\ Train Loss:49.531\n",
      "Epoch 2\\Batch 5400\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 5600\\ Train Loss:49.519\n",
      "Epoch 2\\Batch 5800\\ Train Loss:49.519\n",
      "Epoch 2\\Batch 6000\\ Train Loss:49.521\n",
      "Epoch 2\\Batch 6200\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 6400\\ Train Loss:49.520\n",
      "Epoch 2\\Batch 6600\\ Train Loss:49.519\n",
      "Epoch 2\\Batch 6800\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 7000\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 7200\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 7400\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 7600\\ Train Loss:49.523\n",
      "Epoch 2\\Batch 7800\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 8000\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 8200\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 8400\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 8600\\ Train Loss:49.531\n",
      "Epoch 2\\Batch 8800\\ Train Loss:49.530\n",
      "Epoch 2\\Batch 9000\\ Train Loss:49.530\n",
      "Epoch 2\\Batch 9200\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 9400\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 9600\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 9800\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 10000\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 10200\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 10400\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 10600\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 10800\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 11000\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 11200\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 11400\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 11600\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 11800\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 12000\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 12200\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 12400\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 12600\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 12800\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 13000\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 13200\\ Train Loss:49.523\n",
      "Epoch 2\\Batch 13400\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 13600\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 13800\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 14000\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 14200\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 14400\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 14600\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 14800\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 15000\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 15200\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 15400\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 15600\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 15800\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 16000\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 16200\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 16400\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 16600\\ Train Loss:49.524\n",
      "Epoch 2\\Batch 16800\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 17000\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 17200\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 17400\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 17600\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 17800\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 18000\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 18200\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 18400\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 18600\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 18800\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 19000\\ Train Loss:49.525\n",
      "Epoch 2\\Batch 19200\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 19400\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 19600\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 19800\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 20000\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 20200\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 20400\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 20600\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 20800\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 21000\\ Train Loss:49.526\n",
      "Epoch 2\\Batch 21200\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 21400\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 21600\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 21800\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 22000\\ Train Loss:49.527\n",
      "Epoch 2\\Batch 22200\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 22400\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 22600\\ Train Loss:49.529\n",
      "Epoch 2\\Batch 22800\\ Train Loss:49.528\n",
      "Epoch 2\\Batch 23000\\ Train Loss:49.529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 True\n",
      "[2019/02/25 20:18:10] run model on validation data...\n",
      "[2019/02/25 20:18:31] Epoch 2\\ Validation Loss:49.552/ Validation Accuracy:0.017\n",
      "Epoch 3\\Batch 200\\ Train Loss:49.602\n",
      "Epoch 3\\Batch 400\\ Train Loss:49.587\n",
      "Epoch 3\\Batch 600\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 800\\ Train Loss:49.482\n",
      "Epoch 3\\Batch 1000\\ Train Loss:49.475\n",
      "Epoch 3\\Batch 1200\\ Train Loss:49.497\n",
      "Epoch 3\\Batch 1400\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 1600\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 1800\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 2000\\ Train Loss:49.507\n",
      "Epoch 3\\Batch 2200\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 2400\\ Train Loss:49.509\n",
      "Epoch 3\\Batch 2600\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 2800\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 3000\\ Train Loss:49.509\n",
      "Epoch 3\\Batch 3200\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 3400\\ Train Loss:49.511\n",
      "Epoch 3\\Batch 3600\\ Train Loss:49.509\n",
      "Epoch 3\\Batch 3800\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 4000\\ Train Loss:49.501\n",
      "Epoch 3\\Batch 4200\\ Train Loss:49.501\n",
      "Epoch 3\\Batch 4400\\ Train Loss:49.511\n",
      "Epoch 3\\Batch 4600\\ Train Loss:49.511\n",
      "Epoch 3\\Batch 4800\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 5000\\ Train Loss:49.509\n",
      "Epoch 3\\Batch 5200\\ Train Loss:49.509\n",
      "Epoch 3\\Batch 5400\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 5600\\ Train Loss:49.497\n",
      "Epoch 3\\Batch 5800\\ Train Loss:49.498\n",
      "Epoch 3\\Batch 6000\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 6200\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 6400\\ Train Loss:49.499\n",
      "Epoch 3\\Batch 6600\\ Train Loss:49.498\n",
      "Epoch 3\\Batch 6800\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 7000\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 7200\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 7400\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 7600\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 7800\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 8000\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 8200\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 8400\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 8600\\ Train Loss:49.509\n",
      "Epoch 3\\Batch 8800\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 9000\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 9200\\ Train Loss:49.507\n",
      "Epoch 3\\Batch 9400\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 9600\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 9800\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 10000\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 10200\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 10400\\ Train Loss:49.508\n",
      "Epoch 3\\Batch 10600\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 10800\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 11000\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 11200\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 11400\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 11600\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 11800\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 12000\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 12200\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 12400\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 12600\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 12800\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 13000\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 13200\\ Train Loss:49.498\n",
      "Epoch 3\\Batch 13400\\ Train Loss:49.499\n",
      "Epoch 3\\Batch 13600\\ Train Loss:49.499\n",
      "Epoch 3\\Batch 13800\\ Train Loss:49.498\n",
      "Epoch 3\\Batch 14000\\ Train Loss:49.498\n",
      "Epoch 3\\Batch 14200\\ Train Loss:49.499\n",
      "Epoch 3\\Batch 14400\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 14600\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 14800\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 15000\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 15200\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 15400\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 15600\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 15800\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 16000\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 16200\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 16400\\ Train Loss:49.501\n",
      "Epoch 3\\Batch 16600\\ Train Loss:49.500\n",
      "Epoch 3\\Batch 16800\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 17000\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 17200\\ Train Loss:49.501\n",
      "Epoch 3\\Batch 17400\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 17600\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 17800\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 18000\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 18200\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 18400\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 18600\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 18800\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 19000\\ Train Loss:49.502\n",
      "Epoch 3\\Batch 19200\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 19400\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 19600\\ Train Loss:49.503\n",
      "Epoch 3\\Batch 19800\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 20000\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 20200\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 20400\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 20600\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 20800\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 21000\\ Train Loss:49.504\n",
      "Epoch 3\\Batch 21200\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 21400\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 21600\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 21800\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 22000\\ Train Loss:49.505\n",
      "Epoch 3\\Batch 22200\\ Train Loss:49.506\n",
      "Epoch 3\\Batch 22400\\ Train Loss:49.507\n",
      "Epoch 3\\Batch 22600\\ Train Loss:49.507\n",
      "Epoch 3\\Batch 22800\\ Train Loss:49.507\n",
      "Epoch 3\\Batch 23000\\ Train Loss:49.507\n",
      "3 1 True\n",
      "[2019/02/25 20:42:09] run model on validation data...\n",
      "[2019/02/25 20:42:31] Epoch 3\\ Validation Loss:49.539/ Validation Accuracy:0.017\n",
      "Epoch 4\\Batch 200\\ Train Loss:49.587\n",
      "Epoch 4\\Batch 400\\ Train Loss:49.570\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
