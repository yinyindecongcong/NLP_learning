{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多标签模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, num_classes, batch_size, vocab_size, embed_size, sentence_len, \n",
    "                 learning_rate, decay_step, decay_rate, filter_num, filter_sizes):\n",
    "        #1.定义超参数\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.learning_rate = learning_rate\n",
    "        self.filter_num = filter_num\n",
    "        self.filter_sizes = filter_sizes #list，如[2,3,4],表示3个卷积核的长度（height）\n",
    "        self.filter_num_total = filter_num * len(filter_sizes)\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_epoch = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_epoch') #在指数衰减函数中会加一\n",
    "        self.epoch_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, self.epoch_step+tf.constant(1))\n",
    "        self.decay_step = decay_step\n",
    "        self.decay_rate = decay_rate\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        #2.设置输入\n",
    "        self.sentence = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len], name='sentence')\n",
    "        #self.label = tf.placeholder(dtype=tf.int32, shape=[None], name='label')\n",
    "        self.label_l1999 = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name='label_l1999')\n",
    "        self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "        #self.dropout_keep_prob = 0.5\n",
    "        \n",
    "        #3.参数初始化\n",
    "        self.instantiate_weight()\n",
    "        #4.定义图\n",
    "        self.logits = self.inference()\n",
    "        \n",
    "        #5.定义loss和train_op\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "#         #6.预测，计算准确率\n",
    "#         self.prediction = tf.argmax(self.logits, axis=1, name='prediction')\n",
    "#         correct_pre = tf.equal(tf.cast(self.prediction, tf.int32), self.label)\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "    \n",
    "    def instantiate_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], tf.float32, initializer=self.initializer)\n",
    "        self.W = tf.get_variable('weight', [self.filter_num_total, self.num_classes], tf.float32, initializer=self.initializer)\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        #embedding -- 卷积 -- 线性分类器\n",
    "        self.sentece_embedding = tf.nn.embedding_lookup(self.Embedding, self.sentence)\n",
    "        h = self.cnn_single_layer()\n",
    "        logits = tf.matmul(h, self.W) + self.b\n",
    "        return logits\n",
    "    \n",
    "    def cnn_single_layer(self):\n",
    "        #conv2d -- BN -- ReLU -- max_pooling -- dropout -- dense\n",
    "        #conv2d的输入与卷积核都要求是4维的，具体查看文档\n",
    "        sentece_embedding_4d = tf.expand_dims(self.sentece_embedding, -1) #增加一维，[batch_size, sentence_len, embed_size, 1]\n",
    "        pool_output = []\n",
    "        for filter_size in self.filter_sizes:\n",
    "            with tf.variable_scope('convolution-pooling-%d'%filter_size):\n",
    "                ft = tf.get_variable('filter%d'%filter_size, [filter_size, self.embed_size, 1, self.filter_num], \n",
    "                                     tf.float32, initializer=self.initializer)\n",
    "                conv = tf.nn.conv2d(sentece_embedding_4d, ft, strides=[1,1,1,1], padding='VALID')\n",
    "                conv = tf.contrib.layers.batch_norm(conv, is_training=self.is_training) #[batch_size, sentence_len-filter_size+1, 1, filter_num]\n",
    "                activation = tf.nn.relu(conv)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(activation, ksize=[1,self.sentence_len-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID')\n",
    "                pool_output.append(pooled) #若干个shape=[batch_size, 1, 1, filter_num]\n",
    "        pool_concat = tf.concat(pool_output, axis=3) #在第三维拼接\n",
    "        flatten_pool = tf.reshape(pool_concat, [-1, self.filter_num_total])\n",
    "        \n",
    "        dropouted = tf.nn.dropout(flatten_pool, keep_prob=self.dropout_keep_prob)\n",
    "        h = tf.layers.dense(dropouted, self.filter_num_total, activation=tf.nn.tanh)\n",
    "        return h\n",
    "        \n",
    "    def loss(self, l2_lambda=0.0001):\n",
    "        loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_l1999, logits=self.logits)\n",
    "        loss1 = tf.reduce_mean(tf.reduce_sum(loss1, axis=1))\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name], name='l2_loss') * l2_lambda\n",
    "        loss = loss1 + l2_loss\n",
    "        return loss\n",
    "                \n",
    "    def train(self):\n",
    "        self.learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_epoch, \n",
    "                                                   self.decay_step, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_epoch, self.learning_rate, optimizer='Adam')\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=10\n",
    "    learning_rate=0.01\n",
    "    batch_size=5\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextCNN(num_classes, batch_size, vocab_size, embed_size, sequence_length,\n",
    "                     learning_rate, decay_step, decay_rate, 50, [2,3,4])\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 2,size=(batch_size, num_classes), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, logits, _ = sess.run([model.loss_val, model.logits, model.train_op],\n",
    "                                            feed_dict={model.sentence: input_x, model.label_l1999: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob, model.is_training:True})\n",
    "            logits = np.argsort(logits)\n",
    "            print('loss:',loss, 'label:', input_y, 'pre:', logits)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'weight:0' shape=(150, 10) dtype=float32_ref>, <tf.Variable 'b:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/filter2:0' shape=(2, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/filter3:0' shape=(3, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/filter4:0' shape=(4, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(150, 150) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(150,) dtype=float32_ref>]\n",
      "loss: 7.59228 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[5 2 1 4 8 9 6 3 7 0]\n",
      " [1 6 8 3 9 4 5 2 0 7]\n",
      " [9 3 4 1 6 2 5 8 0 7]\n",
      " [1 9 8 3 2 4 0 6 5 7]\n",
      " [5 9 4 0 3 7 8 1 6 2]]\n",
      "loss: 4.6913223 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[0 6 4 3 9 1 2 8 5 7]\n",
      " [4 5 1 0 9 6 3 8 2 7]\n",
      " [4 0 9 5 3 8 6 1 2 7]\n",
      " [6 4 2 5 1 3 9 0 8 7]\n",
      " [0 6 4 3 1 9 8 5 2 7]]\n",
      "loss: 3.0587118 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[4 6 3 9 8 0 2 1 5 7]\n",
      " [4 1 5 9 8 3 2 0 6 7]\n",
      " [4 9 8 5 0 6 1 2 3 7]\n",
      " [8 1 2 6 4 0 5 9 3 7]\n",
      " [6 4 3 0 9 8 5 1 2 7]]\n",
      "loss: 1.949936 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[4 9 0 3 2 6 1 8 5 7]\n",
      " [4 5 1 9 2 0 6 3 8 7]\n",
      " [8 4 0 9 5 2 6 1 3 7]\n",
      " [2 6 8 1 0 3 4 5 9 7]\n",
      " [3 6 4 0 2 9 5 8 1 7]]\n",
      "loss: 1.6415089 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[4 6 0 1 9 3 2 5 8 7]\n",
      " [4 5 1 9 2 0 6 8 3 7]\n",
      " [4 0 9 8 5 1 6 2 3 7]\n",
      " [6 1 2 8 4 9 5 0 3 7]\n",
      " [6 4 0 3 2 8 1 9 5 7]]\n",
      "loss: 0.9728183 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[0 4 9 1 6 2 3 5 8 7]\n",
      " [4 1 5 9 0 2 3 8 6 7]\n",
      " [4 5 0 9 8 1 6 2 3 7]\n",
      " [1 2 6 8 5 4 0 7 9 3]\n",
      " [6 4 0 3 2 1 8 5 9 7]]\n",
      "loss: 0.92700446 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[2 6 1 0 3 9 4 8 5 7]\n",
      " [9 4 1 5 0 2 3 7 6 8]\n",
      " [0 4 5 9 8 1 7 2 3 6]\n",
      " [8 1 2 6 5 0 4 9 3 7]\n",
      " [0 4 6 3 8 2 1 5 9 7]]\n",
      "loss: 0.6265613 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[0 4 9 6 3 1 2 5 8 7]\n",
      " [4 9 5 1 8 2 7 0 3 6]\n",
      " [0 5 4 9 8 1 3 7 6 2]\n",
      " [1 2 8 6 0 4 7 3 9 5]\n",
      " [6 0 3 4 1 2 5 9 8 7]]\n",
      "loss: 0.47089422 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[0 9 4 2 1 6 3 5 8 7]\n",
      " [9 1 4 5 3 2 8 0 6 7]\n",
      " [0 4 9 5 8 6 3 1 7 2]\n",
      " [8 1 2 6 0 7 9 5 4 3]\n",
      " [0 3 4 6 2 9 1 5 8 7]]\n",
      "loss: 0.5775089 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[0 9 4 3 6 1 2 5 8 7]\n",
      " [5 9 4 1 2 0 8 6 3 7]\n",
      " [0 4 9 5 8 1 6 3 2 7]\n",
      " [1 2 8 6 9 5 4 0 7 3]\n",
      " [0 3 4 6 1 9 5 2 8 7]]\n",
      "loss: 0.3193143 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[1 6 9 2 4 0 3 5 8 7]\n",
      " [5 9 1 4 2 3 8 0 6 7]\n",
      " [4 5 9 0 8 1 7 3 6 2]\n",
      " [6 2 1 8 0 9 3 4 5 7]\n",
      " [4 3 0 6 9 1 5 2 7 8]]\n",
      "loss: 0.29397756 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[6 0 2 1 3 4 9 8 5 7]\n",
      " [4 9 1 5 8 3 2 0 6 7]\n",
      " [5 9 8 0 4 1 2 3 6 7]\n",
      " [2 6 1 8 3 0 9 7 5 4]\n",
      " [3 0 4 6 9 2 1 5 8 7]]\n",
      "loss: 0.2613326 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[0 6 2 3 1 4 9 8 5 7]\n",
      " [9 1 5 4 8 2 0 6 3 7]\n",
      " [5 8 0 4 9 7 1 6 2 3]\n",
      " [2 1 6 8 9 5 7 0 3 4]\n",
      " [3 0 4 6 9 1 2 5 8 7]]\n",
      "loss: 0.21850592 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[1 0 6 9 2 4 3 5 8 7]\n",
      " [9 4 5 1 8 6 3 0 2 7]\n",
      " [5 8 9 4 0 7 1 2 3 6]\n",
      " [1 6 2 8 0 3 9 7 5 4]\n",
      " [3 4 6 0 9 2 5 1 8 7]]\n",
      "loss: 0.20287888 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[4 3 1 0 2 6 9 8 5 7]\n",
      " [5 9 4 1 8 3 0 2 6 7]\n",
      " [5 9 4 8 0 7 1 3 2 6]\n",
      " [6 2 1 8 0 3 7 9 4 5]\n",
      " [4 3 0 6 9 1 2 5 7 8]]\n",
      "loss: 0.24818964 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[1 2 6 9 3 4 0 8 5 7]\n",
      " [5 4 9 1 8 0 6 2 3 7]\n",
      " [5 9 4 8 0 1 7 3 2 6]\n",
      " [6 1 2 8 0 4 3 9 7 5]\n",
      " [3 4 0 6 9 1 2 5 7 8]]\n",
      "loss: 0.12858588 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[1 2 0 6 3 9 4 8 5 7]\n",
      " [9 1 5 4 2 3 8 6 7 0]\n",
      " [5 9 4 8 0 1 7 3 2 6]\n",
      " [6 1 2 8 7 0 3 9 5 4]\n",
      " [0 3 4 6 9 5 8 2 7 1]]\n",
      "loss: 0.17851269 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[2 6 3 0 1 9 4 8 5 7]\n",
      " [5 9 1 4 8 2 3 6 7 0]\n",
      " [5 4 9 0 8 3 1 2 6 7]\n",
      " [1 8 6 2 7 0 9 3 5 4]\n",
      " [4 3 0 6 9 1 5 2 8 7]]\n",
      "loss: 0.12904006 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[3 0 6 4 1 2 9 8 7 5]\n",
      " [5 9 4 1 8 0 3 2 6 7]\n",
      " [5 4 9 0 8 3 7 1 6 2]\n",
      " [6 2 1 8 3 7 0 9 5 4]\n",
      " [6 3 0 4 1 2 9 8 7 5]]\n",
      "loss: 0.1235823 label: [[0 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 0 1 1 1]] pre: [[3 0 6 4 2 1 9 8 5 7]\n",
      " [1 9 4 5 8 6 3 2 0 7]\n",
      " [5 4 0 9 8 3 1 7 6 2]\n",
      " [2 1 6 8 3 7 9 5 0 4]\n",
      " [4 3 0 6 1 9 7 5 2 8]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define hyperparameter\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('label_size', 1999, 'number of label')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size for training')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 200, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 128, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0003, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 1., '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 1000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 15, '')\n",
    "tf.app.flags.DEFINE_integer('validation_every', 1, 'Validate every validate_every epochs.')\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"textcnn_multilabel_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"textcnn_multilabel_checkpoint/data_cache.pik\",\"data chche for the model\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"num_filters\", 64, \"number of filters\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些辅助函数\n",
    "def get_target_label_short(eval_y):\n",
    "    res = [idx for idx in range(len(eval_y)) if eval_y[idx] > 0] #结果如：[45,100,1555]\n",
    "    return res\n",
    "\n",
    "def get_label_using_logits(logits, top_number=5):\n",
    "    predict_y = [idx for idx in range(len(logits)) if logits[idx] >= 0.5]\n",
    "    if len(predict_y) == 0: predict_y = [np.argmax(logits)]\n",
    "    return predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main\n",
    "\n",
    "#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data & training (4.validation) \n",
    "\n",
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = '/data/chenhy/data/ieee_zhihu_cup/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    \n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "\n",
    "#     #print(\"train_X.shape:\", np.array(train_X).shape)\n",
    "#     #print(\"train_y.shape:\", np.array(train_y).shape)\n",
    "#     print(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "#     print(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "#     #print(\"test_X[0]:\", test_X[0])  \n",
    "#     #print(\"test_X[1]:\", test_X[1])\n",
    "#     #print(\"test_y[0]:\", test_y[0])  \n",
    "    print(\"train_X[0:5]:\", train_X[0:5])\n",
    "    print(\"train_Y[0:5]:\", train_y[0:5])\n",
    "    train_y_short = get_target_label_short(train_y[0])\n",
    "    print(\"train_y_short:\", train_y_short)\n",
    "\n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextCNN(FLAGS.label_size, FLAGS.batch_size, vocab_size, \n",
    "                        FLAGS.embed_size, FLAGS.sentence_len, FLAGS.learning_rate, \n",
    "                        FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.num_filters, [6,7,8])\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        CONTINUE_TRAIN = False\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            print('restore model from checkpoint')\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            print('CONTINUE_TRAIN=', CONTINUE_TRAIN)\n",
    "            sess.run(model.epoch_increment)\n",
    "            print('Continue at Epoch:', sess.run(model.epoch_step))\n",
    "        if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint') or CONTINUE_TRAIN:\n",
    "            if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "                print('initialize variables')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                #print('assign pre-trained embedding')\n",
    "                #embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "                #sess.run(embedding_assign)\n",
    "\n",
    "            #3.训练\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = sess.run(model.epoch_step)\n",
    "                loss, counter = 0.0, 0\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    if (epoch == 0 and counter == 0):\n",
    "                        print('train_X[start, end]:', train_X[start:end])\n",
    "                        print('train_y[start, end]:', train_y[start:end])\n",
    "                    lo, lr, _ = sess.run([model.loss_val, model.learning_rate, model.train_op], \n",
    "                                feed_dict={model.sentence: train_X[start:end], model.label_l1999: train_y[start:end],\n",
    "                                           model.dropout_keep_prob: 0.8, model.is_training: True})\n",
    "                    loss, counter = loss+lo, counter+1\n",
    "\n",
    "                    if (counter % 50 == 0):\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\\ Learning rate:%.5f\"%(epoch, counter, loss/float(counter), lr))\n",
    "\n",
    "                    if counter % 3000 == 0:\n",
    "                        print('run model on validation data...')\n",
    "                        loss_valid, f1_score, precision, recall = do_eval(sess, model, vaild_X, valid_y)\n",
    "                        log(\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"%(epoch, loss_valid, f1_score, precision, recall))\n",
    "                        #save the checkpoint\n",
    "                        save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                        saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "        #loss_valid, f1_score, precision, recall = do_eval(sess, model, vaild_X, valid_y)\n",
    "        #log(\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"%(epoch, loss_valid, f1_score, precision, recall))\n",
    "                        \n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    #embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y\n",
    "\n",
    "def do_eval(sess, model, eval_X, eval_y):\n",
    "    test_X, test_y = eval_X[:3000], eval_y[:3000]\n",
    "    num_of_data = len(test_y)\n",
    "    batch_size = 1\n",
    "    loss, F1, p, r = 0., 0., 0., 0.\n",
    "    label_dict_confuse = {'TP':0.000001, 'FN':0.000001, 'FP':0.000001}\n",
    "    for start in range(num_of_data):\n",
    "        end = start + 1\n",
    "        lo,logits = sess.run([model.loss_val, model.logits], \n",
    "                        feed_dict={model.sentence: test_X[start:end], model.label_l1999: test_y[start:end],\n",
    "                                   model.dropout_keep_prob:1.0, model.is_training: False})\n",
    "        loss += lo\n",
    "        pre = get_label_using_logits(logits[0])\n",
    "        label = get_target_label_short(test_y[start])\n",
    "#         pre = np.argsort(logits[0])[-5:]\n",
    "#         label = [i for i in range(len(test_y[start])) if test_y[start][i] > 0]\n",
    "        if start == 0: print('label:',label, 'predict:', pre)\n",
    "        inter = len([x for x in pre if x in label])\n",
    "        label_dict_confuse['TP'] += inter\n",
    "        label_dict_confuse['FN'] += len(label) - inter\n",
    "        label_dict_confuse['FP'] += len(pre) - inter\n",
    "    print(label_dict_confuse)\n",
    "    p = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FP'])\n",
    "    r = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FN'])\n",
    "    if p + r == 0: return loss/num_of_data, 0, 0, 0\n",
    "    F1 = (2 * p * r)/(p + r)\n",
    "    return loss/num_of_data, F1, p, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X[0:5]: [[ 832   60  256 1172 3407  516   96  138  103 1108   16    3   96  177\n",
      "    22   11  672   53   18 1560 1560   15   65   12  180   10  342  173\n",
      "    13  103  141  707  191   12  342  173   15   13   22   11  229  264\n",
      "   163 1362  135 1249  156  156  731  115   84   10  808 1713  103  141\n",
      "   229  264  788  421  103  141   12   95  316   10  808 1713  103  141\n",
      "    12 2413 1227   15 1397  997   22  116  301  489   12   18  858   99\n",
      "   596   98   26  646  813   10  386 1093  197  767   22   11 1179 1849\n",
      "   593   84   22   11   94  102  322   60  190  220  583  355   10  153\n",
      "   103  141  192  153   12   10 1244  466  116  103  141  189   58  130\n",
      "    95  316   12  788  421  699   16    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 270  154  166  504   53  154   89   60   13  210  259  589   12  151\n",
      "    18 1682   10  154  150  113   97   26  145   67   33   16    3   61\n",
      "    18  118  151   18  948   10  151   18  550   80  123  159   93  154\n",
      "    12  150  186  104  174  537   10  116   66   41   12  117  195   10\n",
      "   104   66   41  116  154   12  117  195   16   95  316  104  590  605\n",
      "   610   19   59   19  202  502  214  516  398   16    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 186  163  284  109  108   32  437   41  232  555  121  177   47   19\n",
      "    67   33  188  309  576  948   12  670  775   16    3    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  32  143  194  453  537  903  450  608   95   10  332   15   47   97\n",
      "   892   75  231  143  238  450   10   97  444  444  314  278   12  166\n",
      "   410   16    3  177  257  338  159   32  143  194   12   18  207   56\n",
      "    75  453  537  903   75  450  608   95   10   75  389  113   12  629\n",
      "    56  342  173   99   13  157  255  629  361   10 1238  268   13  537\n",
      "   903 1950   75   10 2004 1790   53   87   92  188  309  281   75  381\n",
      "  1950   75   12  701  841   10 1917  602  703  763 2205 2574  815  909\n",
      "   909  133   84   26   22   11  177  257  103  301 2739 2162   10   32\n",
      "   143  285  397  593  576  608   54   18  208 1078   10  880 1828  454\n",
      "  1083   10  202  502   32  143  194   75  124  451  173 1426  817   22\n",
      "    11  746  194  399  609   71   13  121  177   10  444  264  424  399\n",
      "  2436  553  241  632  629 1302  847  255   10  125  542  158  724  724\n",
      "   593  650   78  236   10  502  159  168   13  650   53   26   75  389\n",
      "    12  399  609   78  236   10  200   93  612  593  972  807  628  305\n",
      "   461   10 2060 2372]\n",
      " [ 224  461 3115  912  560  833  224  333  224 1499  359  106   83   52\n",
      "    36   30   35   21  253  372  414   96  138   16    3  434  135   13\n",
      "   300  352 1078  351  614  345  189  129   18   62  295   10  230  160\n",
      "   108  437  774 1292  372   18  118  347   22  224  461  130 3115  912\n",
      "   130  560  833  130  224  333  130  224 1499  359  106   83   52   36\n",
      "    30   35   21  253  372  414   96  138   16    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "train_Y[0:5]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_y_short: [1473]\n",
      "initialize variables\n",
      "train_X[start, end]: [[832  60 256 ...   0   0   0]\n",
      " [270 154 166 ...   0   0   0]\n",
      " [186 163 284 ...   0   0   0]\n",
      " ...\n",
      " [ 96 138 117 ...   0   0   0]\n",
      " [ 56 109  96 ...   0   0   0]\n",
      " [ 32 127 420 ...   0   0   0]]\n",
      "train_y[start, end]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[2019/03/17 16:30:49] Epoch 0\\Batch 50\\ Train Loss:840.212\\ Learning rate:0.00030\n",
      "[2019/03/17 16:30:54] Epoch 0\\Batch 100\\ Train Loss:499.399\\ Learning rate:0.00030\n",
      "[2019/03/17 16:30:59] Epoch 0\\Batch 150\\ Train Loss:355.994\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:04] Epoch 0\\Batch 200\\ Train Loss:278.564\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:09] Epoch 0\\Batch 250\\ Train Loss:230.107\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:14] Epoch 0\\Batch 300\\ Train Loss:196.838\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:19] Epoch 0\\Batch 350\\ Train Loss:172.589\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:24] Epoch 0\\Batch 400\\ Train Loss:154.125\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:29] Epoch 0\\Batch 450\\ Train Loss:139.573\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:34] Epoch 0\\Batch 500\\ Train Loss:127.837\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:39] Epoch 0\\Batch 550\\ Train Loss:118.142\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:43] Epoch 0\\Batch 600\\ Train Loss:110.027\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:48] Epoch 0\\Batch 650\\ Train Loss:103.111\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:53] Epoch 0\\Batch 700\\ Train Loss:97.156\\ Learning rate:0.00030\n",
      "[2019/03/17 16:31:58] Epoch 0\\Batch 750\\ Train Loss:91.973\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:03] Epoch 0\\Batch 800\\ Train Loss:87.416\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:08] Epoch 0\\Batch 850\\ Train Loss:83.393\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:13] Epoch 0\\Batch 900\\ Train Loss:79.805\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:18] Epoch 0\\Batch 950\\ Train Loss:76.578\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:23] Epoch 0\\Batch 1000\\ Train Loss:73.672\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:27] Epoch 0\\Batch 1050\\ Train Loss:71.045\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:30] Epoch 0\\Batch 1100\\ Train Loss:68.647\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:33] Epoch 0\\Batch 1150\\ Train Loss:66.464\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:37] Epoch 0\\Batch 1200\\ Train Loss:64.450\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:40] Epoch 0\\Batch 1250\\ Train Loss:62.593\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:43] Epoch 0\\Batch 1300\\ Train Loss:60.883\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:46] Epoch 0\\Batch 1350\\ Train Loss:59.297\\ Learning rate:0.00030\n",
      "[2019/03/17 16:32:50] Epoch 0\\Batch 1400\\ Train Loss:57.820\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 16:32:55] Epoch 0\\Batch 1450\\ Train Loss:56.446\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:00] Epoch 0\\Batch 1500\\ Train Loss:55.160\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:04] Epoch 0\\Batch 1550\\ Train Loss:53.970\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:09] Epoch 0\\Batch 1600\\ Train Loss:52.845\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:14] Epoch 0\\Batch 1650\\ Train Loss:51.796\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:19] Epoch 0\\Batch 1700\\ Train Loss:50.803\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:24] Epoch 0\\Batch 1750\\ Train Loss:49.860\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:29] Epoch 0\\Batch 1800\\ Train Loss:48.978\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:34] Epoch 0\\Batch 1850\\ Train Loss:48.138\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:39] Epoch 0\\Batch 1900\\ Train Loss:47.342\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:44] Epoch 0\\Batch 1950\\ Train Loss:46.588\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:49] Epoch 0\\Batch 2000\\ Train Loss:45.871\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:54] Epoch 0\\Batch 2050\\ Train Loss:45.191\\ Learning rate:0.00030\n",
      "[2019/03/17 16:33:59] Epoch 0\\Batch 2100\\ Train Loss:44.543\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:04] Epoch 0\\Batch 2150\\ Train Loss:43.924\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:09] Epoch 0\\Batch 2200\\ Train Loss:43.338\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:14] Epoch 0\\Batch 2250\\ Train Loss:42.779\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:19] Epoch 0\\Batch 2300\\ Train Loss:42.237\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:24] Epoch 0\\Batch 2350\\ Train Loss:41.718\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:29] Epoch 0\\Batch 2400\\ Train Loss:41.222\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:34] Epoch 0\\Batch 2450\\ Train Loss:40.746\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:38] Epoch 0\\Batch 2500\\ Train Loss:40.291\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:43] Epoch 0\\Batch 2550\\ Train Loss:39.849\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:48] Epoch 0\\Batch 2600\\ Train Loss:39.427\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:53] Epoch 0\\Batch 2650\\ Train Loss:39.025\\ Learning rate:0.00030\n",
      "[2019/03/17 16:34:58] Epoch 0\\Batch 2700\\ Train Loss:38.630\\ Learning rate:0.00030\n",
      "[2019/03/17 16:35:03] Epoch 0\\Batch 2750\\ Train Loss:38.258\\ Learning rate:0.00030\n",
      "[2019/03/17 16:35:08] Epoch 0\\Batch 2800\\ Train Loss:37.892\\ Learning rate:0.00030\n",
      "[2019/03/17 16:35:13] Epoch 0\\Batch 2850\\ Train Loss:37.539\\ Learning rate:0.00030\n",
      "[2019/03/17 16:35:18] Epoch 0\\Batch 2900\\ Train Loss:37.198\\ Learning rate:0.00030\n",
      "[2019/03/17 16:35:22] Epoch 0\\Batch 2950\\ Train Loss:36.872\\ Learning rate:0.00030\n",
      "[2019/03/17 16:35:27] Epoch 0\\Batch 3000\\ Train Loss:36.554\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [4]\n",
      "{'FN': 6967.000001, 'TP': 32.000001, 'FP': 2968.000001}\n",
      "[2019/03/17 16:35:50] Epoch 0/ Validation Loss:17.804/ F1_score:0.006/ Precision:0.011/ Recall:0.005\n",
      "[2019/03/17 16:35:55] Epoch 0\\Batch 3050\\ Train Loss:36.248\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:00] Epoch 0\\Batch 3100\\ Train Loss:35.955\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:05] Epoch 0\\Batch 3150\\ Train Loss:35.668\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:10] Epoch 0\\Batch 3200\\ Train Loss:35.388\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:15] Epoch 0\\Batch 3250\\ Train Loss:35.116\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:20] Epoch 0\\Batch 3300\\ Train Loss:34.855\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:24] Epoch 0\\Batch 3350\\ Train Loss:34.599\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:29] Epoch 0\\Batch 3400\\ Train Loss:34.355\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:34] Epoch 0\\Batch 3450\\ Train Loss:34.117\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:39] Epoch 0\\Batch 3500\\ Train Loss:33.881\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:44] Epoch 0\\Batch 3550\\ Train Loss:33.655\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:49] Epoch 0\\Batch 3600\\ Train Loss:33.435\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:54] Epoch 0\\Batch 3650\\ Train Loss:33.221\\ Learning rate:0.00030\n",
      "[2019/03/17 16:36:59] Epoch 0\\Batch 3700\\ Train Loss:33.014\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:04] Epoch 0\\Batch 3750\\ Train Loss:32.815\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:09] Epoch 0\\Batch 3800\\ Train Loss:32.622\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:14] Epoch 0\\Batch 3850\\ Train Loss:32.428\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:19] Epoch 0\\Batch 3900\\ Train Loss:32.239\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:23] Epoch 0\\Batch 3950\\ Train Loss:32.056\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:28] Epoch 0\\Batch 4000\\ Train Loss:31.879\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:33] Epoch 0\\Batch 4050\\ Train Loss:31.706\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:38] Epoch 0\\Batch 4100\\ Train Loss:31.537\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:43] Epoch 0\\Batch 4150\\ Train Loss:31.371\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:48] Epoch 0\\Batch 4200\\ Train Loss:31.210\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:53] Epoch 0\\Batch 4250\\ Train Loss:31.052\\ Learning rate:0.00030\n",
      "[2019/03/17 16:37:58] Epoch 0\\Batch 4300\\ Train Loss:30.897\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:03] Epoch 0\\Batch 4350\\ Train Loss:30.748\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:07] Epoch 0\\Batch 4400\\ Train Loss:30.604\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:12] Epoch 0\\Batch 4450\\ Train Loss:30.459\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:17] Epoch 0\\Batch 4500\\ Train Loss:30.319\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:22] Epoch 0\\Batch 4550\\ Train Loss:30.181\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:27] Epoch 0\\Batch 4600\\ Train Loss:30.049\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:32] Epoch 0\\Batch 4650\\ Train Loss:29.917\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:37] Epoch 0\\Batch 4700\\ Train Loss:29.788\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:42] Epoch 0\\Batch 4750\\ Train Loss:29.661\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:47] Epoch 0\\Batch 4800\\ Train Loss:29.541\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:52] Epoch 0\\Batch 4850\\ Train Loss:29.420\\ Learning rate:0.00030\n",
      "[2019/03/17 16:38:57] Epoch 0\\Batch 4900\\ Train Loss:29.300\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:02] Epoch 0\\Batch 4950\\ Train Loss:29.182\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:07] Epoch 0\\Batch 5000\\ Train Loss:29.069\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:12] Epoch 0\\Batch 5050\\ Train Loss:28.956\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:17] Epoch 0\\Batch 5100\\ Train Loss:28.846\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:22] Epoch 0\\Batch 5150\\ Train Loss:28.740\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:27] Epoch 0\\Batch 5200\\ Train Loss:28.637\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:32] Epoch 0\\Batch 5250\\ Train Loss:28.534\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:36] Epoch 0\\Batch 5300\\ Train Loss:28.434\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:39] Epoch 0\\Batch 5350\\ Train Loss:28.335\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:42] Epoch 0\\Batch 5400\\ Train Loss:28.238\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:46] Epoch 0\\Batch 5450\\ Train Loss:28.143\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:49] Epoch 0\\Batch 5500\\ Train Loss:28.048\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:53] Epoch 0\\Batch 5550\\ Train Loss:27.955\\ Learning rate:0.00030\n",
      "[2019/03/17 16:39:56] Epoch 0\\Batch 5600\\ Train Loss:27.863\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:01] Epoch 0\\Batch 5650\\ Train Loss:27.774\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:05] Epoch 0\\Batch 5700\\ Train Loss:27.689\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:10] Epoch 0\\Batch 5750\\ Train Loss:27.604\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:15] Epoch 0\\Batch 5800\\ Train Loss:27.521\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:20] Epoch 0\\Batch 5850\\ Train Loss:27.440\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:25] Epoch 0\\Batch 5900\\ Train Loss:27.361\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:30] Epoch 0\\Batch 5950\\ Train Loss:27.282\\ Learning rate:0.00030\n",
      "[2019/03/17 16:40:35] Epoch 0\\Batch 6000\\ Train Loss:27.203\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [0]\n",
      "{'FN': 6932.000001, 'TP': 67.000001, 'FP': 2933.000001}\n",
      "[2019/03/17 16:40:58] Epoch 0/ Validation Loss:17.741/ F1_score:0.013/ Precision:0.022/ Recall:0.010\n",
      "[2019/03/17 16:41:03] Epoch 0\\Batch 6050\\ Train Loss:27.128\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:09] Epoch 0\\Batch 6100\\ Train Loss:27.053\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 16:41:14] Epoch 0\\Batch 6150\\ Train Loss:26.978\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:19] Epoch 0\\Batch 6200\\ Train Loss:26.904\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:24] Epoch 0\\Batch 6250\\ Train Loss:26.831\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:29] Epoch 0\\Batch 6300\\ Train Loss:26.760\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:34] Epoch 0\\Batch 6350\\ Train Loss:26.689\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:39] Epoch 0\\Batch 6400\\ Train Loss:26.621\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:44] Epoch 0\\Batch 6450\\ Train Loss:26.551\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:49] Epoch 0\\Batch 6500\\ Train Loss:26.484\\ Learning rate:0.00030\n",
      "[2019/03/17 16:41:54] Epoch 0\\Batch 6550\\ Train Loss:26.417\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:00] Epoch 0\\Batch 6600\\ Train Loss:26.352\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:05] Epoch 0\\Batch 6650\\ Train Loss:26.288\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:10] Epoch 0\\Batch 6700\\ Train Loss:26.226\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:15] Epoch 0\\Batch 6750\\ Train Loss:26.162\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:19] Epoch 0\\Batch 6800\\ Train Loss:26.101\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:25] Epoch 0\\Batch 6850\\ Train Loss:26.043\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:30] Epoch 0\\Batch 6900\\ Train Loss:25.983\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:35] Epoch 0\\Batch 6950\\ Train Loss:25.925\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:40] Epoch 0\\Batch 7000\\ Train Loss:25.866\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:45] Epoch 0\\Batch 7050\\ Train Loss:25.809\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:49] Epoch 0\\Batch 7100\\ Train Loss:25.754\\ Learning rate:0.00030\n",
      "[2019/03/17 16:42:54] Epoch 0\\Batch 7150\\ Train Loss:25.698\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:00] Epoch 0\\Batch 7200\\ Train Loss:25.643\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:05] Epoch 0\\Batch 7250\\ Train Loss:25.589\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:10] Epoch 0\\Batch 7300\\ Train Loss:25.536\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:15] Epoch 0\\Batch 7350\\ Train Loss:25.484\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:20] Epoch 0\\Batch 7400\\ Train Loss:25.432\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:25] Epoch 0\\Batch 7450\\ Train Loss:25.380\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:30] Epoch 0\\Batch 7500\\ Train Loss:25.330\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:35] Epoch 0\\Batch 7550\\ Train Loss:25.280\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:40] Epoch 0\\Batch 7600\\ Train Loss:25.232\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:45] Epoch 0\\Batch 7650\\ Train Loss:25.183\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:49] Epoch 0\\Batch 7700\\ Train Loss:25.134\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:54] Epoch 0\\Batch 7750\\ Train Loss:25.087\\ Learning rate:0.00030\n",
      "[2019/03/17 16:43:59] Epoch 0\\Batch 7800\\ Train Loss:25.041\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:04] Epoch 0\\Batch 7850\\ Train Loss:24.995\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:09] Epoch 0\\Batch 7900\\ Train Loss:24.949\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:14] Epoch 0\\Batch 7950\\ Train Loss:24.902\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:19] Epoch 0\\Batch 8000\\ Train Loss:24.859\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:24] Epoch 0\\Batch 8050\\ Train Loss:24.816\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:29] Epoch 0\\Batch 8100\\ Train Loss:24.772\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:34] Epoch 0\\Batch 8150\\ Train Loss:24.729\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:39] Epoch 0\\Batch 8200\\ Train Loss:24.687\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:44] Epoch 0\\Batch 8250\\ Train Loss:24.644\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:49] Epoch 0\\Batch 8300\\ Train Loss:24.603\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:54] Epoch 0\\Batch 8350\\ Train Loss:24.563\\ Learning rate:0.00030\n",
      "[2019/03/17 16:44:59] Epoch 0\\Batch 8400\\ Train Loss:24.523\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:04] Epoch 0\\Batch 8450\\ Train Loss:24.484\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:09] Epoch 0\\Batch 8500\\ Train Loss:24.444\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:14] Epoch 0\\Batch 8550\\ Train Loss:24.406\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:19] Epoch 0\\Batch 8600\\ Train Loss:24.369\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:23] Epoch 0\\Batch 8650\\ Train Loss:24.330\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:28] Epoch 0\\Batch 8700\\ Train Loss:24.292\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:33] Epoch 0\\Batch 8750\\ Train Loss:24.255\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:38] Epoch 0\\Batch 8800\\ Train Loss:24.218\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:43] Epoch 0\\Batch 8850\\ Train Loss:24.181\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:48] Epoch 0\\Batch 8900\\ Train Loss:24.145\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:53] Epoch 0\\Batch 8950\\ Train Loss:24.110\\ Learning rate:0.00030\n",
      "[2019/03/17 16:45:58] Epoch 0\\Batch 9000\\ Train Loss:24.076\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [0]\n",
      "{'FN': 6932.000001, 'TP': 67.000001, 'FP': 2933.000001}\n",
      "[2019/03/17 16:46:21] Epoch 0/ Validation Loss:17.661/ F1_score:0.013/ Precision:0.022/ Recall:0.010\n",
      "[2019/03/17 16:46:26] Epoch 0\\Batch 9050\\ Train Loss:24.040\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:31] Epoch 0\\Batch 9100\\ Train Loss:24.007\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:34] Epoch 0\\Batch 9150\\ Train Loss:23.972\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:38] Epoch 0\\Batch 9200\\ Train Loss:23.937\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:41] Epoch 0\\Batch 9250\\ Train Loss:23.904\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:44] Epoch 0\\Batch 9300\\ Train Loss:23.870\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:47] Epoch 0\\Batch 9350\\ Train Loss:23.837\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:51] Epoch 0\\Batch 9400\\ Train Loss:23.805\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:54] Epoch 0\\Batch 9450\\ Train Loss:23.775\\ Learning rate:0.00030\n",
      "[2019/03/17 16:46:58] Epoch 0\\Batch 9500\\ Train Loss:23.742\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:03] Epoch 0\\Batch 9550\\ Train Loss:23.711\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:07] Epoch 0\\Batch 9600\\ Train Loss:23.680\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:12] Epoch 0\\Batch 9650\\ Train Loss:23.649\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:17] Epoch 0\\Batch 9700\\ Train Loss:23.619\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:22] Epoch 0\\Batch 9750\\ Train Loss:23.590\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:27] Epoch 0\\Batch 9800\\ Train Loss:23.561\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:32] Epoch 0\\Batch 9850\\ Train Loss:23.531\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:37] Epoch 0\\Batch 9900\\ Train Loss:23.503\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:42] Epoch 0\\Batch 9950\\ Train Loss:23.474\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:47] Epoch 0\\Batch 10000\\ Train Loss:23.445\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:52] Epoch 0\\Batch 10050\\ Train Loss:23.416\\ Learning rate:0.00030\n",
      "[2019/03/17 16:47:57] Epoch 0\\Batch 10100\\ Train Loss:23.388\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:02] Epoch 0\\Batch 10150\\ Train Loss:23.360\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:06] Epoch 0\\Batch 10200\\ Train Loss:23.332\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:11] Epoch 0\\Batch 10250\\ Train Loss:23.305\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:16] Epoch 0\\Batch 10300\\ Train Loss:23.278\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:21] Epoch 0\\Batch 10350\\ Train Loss:23.251\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:26] Epoch 0\\Batch 10400\\ Train Loss:23.225\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:31] Epoch 0\\Batch 10450\\ Train Loss:23.199\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:36] Epoch 0\\Batch 10500\\ Train Loss:23.171\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:41] Epoch 0\\Batch 10550\\ Train Loss:23.145\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:46] Epoch 0\\Batch 10600\\ Train Loss:23.120\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:51] Epoch 0\\Batch 10650\\ Train Loss:23.095\\ Learning rate:0.00030\n",
      "[2019/03/17 16:48:56] Epoch 0\\Batch 10700\\ Train Loss:23.069\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:01] Epoch 0\\Batch 10750\\ Train Loss:23.045\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:06] Epoch 0\\Batch 10800\\ Train Loss:23.020\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:11] Epoch 0\\Batch 10850\\ Train Loss:22.995\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:16] Epoch 0\\Batch 10900\\ Train Loss:22.971\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 16:49:21] Epoch 0\\Batch 10950\\ Train Loss:22.947\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:26] Epoch 0\\Batch 11000\\ Train Loss:22.923\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:31] Epoch 0\\Batch 11050\\ Train Loss:22.899\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:36] Epoch 0\\Batch 11100\\ Train Loss:22.875\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:41] Epoch 0\\Batch 11150\\ Train Loss:22.852\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:46] Epoch 0\\Batch 11200\\ Train Loss:22.828\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:51] Epoch 0\\Batch 11250\\ Train Loss:22.806\\ Learning rate:0.00030\n",
      "[2019/03/17 16:49:56] Epoch 0\\Batch 11300\\ Train Loss:22.783\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:01] Epoch 0\\Batch 11350\\ Train Loss:22.760\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:06] Epoch 0\\Batch 11400\\ Train Loss:22.738\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:11] Epoch 0\\Batch 11450\\ Train Loss:22.716\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:16] Epoch 0\\Batch 11500\\ Train Loss:22.694\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:21] Epoch 0\\Batch 11550\\ Train Loss:22.672\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:26] Epoch 0\\Batch 11600\\ Train Loss:22.651\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:31] Epoch 0\\Batch 11650\\ Train Loss:22.630\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:36] Epoch 0\\Batch 11700\\ Train Loss:22.609\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:41] Epoch 0\\Batch 11750\\ Train Loss:22.588\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:46] Epoch 0\\Batch 11800\\ Train Loss:22.567\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:51] Epoch 0\\Batch 11850\\ Train Loss:22.547\\ Learning rate:0.00030\n",
      "[2019/03/17 16:50:56] Epoch 0\\Batch 11900\\ Train Loss:22.526\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:00] Epoch 0\\Batch 11950\\ Train Loss:22.505\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:05] Epoch 0\\Batch 12000\\ Train Loss:22.485\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [1]\n",
      "{'FN': 6931.000001, 'TP': 68.000001, 'FP': 2932.000001}\n",
      "[2019/03/17 16:51:28] Epoch 0/ Validation Loss:17.575/ F1_score:0.014/ Precision:0.023/ Recall:0.010\n",
      "[2019/03/17 16:51:33] Epoch 0\\Batch 12050\\ Train Loss:22.465\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:38] Epoch 0\\Batch 12100\\ Train Loss:22.445\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:43] Epoch 0\\Batch 12150\\ Train Loss:22.426\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:48] Epoch 0\\Batch 12200\\ Train Loss:22.407\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:53] Epoch 0\\Batch 12250\\ Train Loss:22.387\\ Learning rate:0.00030\n",
      "[2019/03/17 16:51:58] Epoch 0\\Batch 12300\\ Train Loss:22.367\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:03] Epoch 0\\Batch 12350\\ Train Loss:22.348\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:08] Epoch 0\\Batch 12400\\ Train Loss:22.330\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:13] Epoch 0\\Batch 12450\\ Train Loss:22.310\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:18] Epoch 0\\Batch 12500\\ Train Loss:22.291\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:22] Epoch 0\\Batch 12550\\ Train Loss:22.271\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:27] Epoch 0\\Batch 12600\\ Train Loss:22.252\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:32] Epoch 0\\Batch 12650\\ Train Loss:22.234\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:37] Epoch 0\\Batch 12700\\ Train Loss:22.215\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:42] Epoch 0\\Batch 12750\\ Train Loss:22.196\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:47] Epoch 0\\Batch 12800\\ Train Loss:22.177\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:52] Epoch 0\\Batch 12850\\ Train Loss:22.158\\ Learning rate:0.00030\n",
      "[2019/03/17 16:52:57] Epoch 0\\Batch 12900\\ Train Loss:22.140\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:02] Epoch 0\\Batch 12950\\ Train Loss:22.122\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:06] Epoch 0\\Batch 13000\\ Train Loss:22.103\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:12] Epoch 0\\Batch 13050\\ Train Loss:22.086\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:17] Epoch 0\\Batch 13100\\ Train Loss:22.067\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:21] Epoch 0\\Batch 13150\\ Train Loss:22.050\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:26] Epoch 0\\Batch 13200\\ Train Loss:22.032\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:31] Epoch 0\\Batch 13250\\ Train Loss:22.015\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:36] Epoch 0\\Batch 13300\\ Train Loss:21.996\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:41] Epoch 0\\Batch 13350\\ Train Loss:21.978\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:44] Epoch 0\\Batch 13400\\ Train Loss:21.960\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:48] Epoch 0\\Batch 13450\\ Train Loss:21.943\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:51] Epoch 0\\Batch 13500\\ Train Loss:21.926\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:54] Epoch 0\\Batch 13550\\ Train Loss:21.909\\ Learning rate:0.00030\n",
      "[2019/03/17 16:53:57] Epoch 0\\Batch 13600\\ Train Loss:21.891\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:01] Epoch 0\\Batch 13650\\ Train Loss:21.873\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:04] Epoch 0\\Batch 13700\\ Train Loss:21.855\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:08] Epoch 0\\Batch 13750\\ Train Loss:21.838\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:12] Epoch 0\\Batch 13800\\ Train Loss:21.821\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:17] Epoch 0\\Batch 13850\\ Train Loss:21.804\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:22] Epoch 0\\Batch 13900\\ Train Loss:21.788\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:27] Epoch 0\\Batch 13950\\ Train Loss:21.772\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:32] Epoch 0\\Batch 14000\\ Train Loss:21.755\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:37] Epoch 0\\Batch 14050\\ Train Loss:21.738\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:42] Epoch 0\\Batch 14100\\ Train Loss:21.722\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:47] Epoch 0\\Batch 14150\\ Train Loss:21.704\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:52] Epoch 0\\Batch 14200\\ Train Loss:21.688\\ Learning rate:0.00030\n",
      "[2019/03/17 16:54:56] Epoch 0\\Batch 14250\\ Train Loss:21.671\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:01] Epoch 0\\Batch 14300\\ Train Loss:21.655\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:06] Epoch 0\\Batch 14350\\ Train Loss:21.638\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:11] Epoch 0\\Batch 14400\\ Train Loss:21.622\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:16] Epoch 0\\Batch 14450\\ Train Loss:21.606\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:21] Epoch 0\\Batch 14500\\ Train Loss:21.590\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:26] Epoch 0\\Batch 14550\\ Train Loss:21.574\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:31] Epoch 0\\Batch 14600\\ Train Loss:21.559\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:36] Epoch 0\\Batch 14650\\ Train Loss:21.543\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:41] Epoch 0\\Batch 14700\\ Train Loss:21.527\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:46] Epoch 0\\Batch 14750\\ Train Loss:21.511\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:51] Epoch 0\\Batch 14800\\ Train Loss:21.495\\ Learning rate:0.00030\n",
      "[2019/03/17 16:55:56] Epoch 0\\Batch 14850\\ Train Loss:21.479\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:00] Epoch 0\\Batch 14900\\ Train Loss:21.463\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:05] Epoch 0\\Batch 14950\\ Train Loss:21.448\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:10] Epoch 0\\Batch 15000\\ Train Loss:21.432\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [8]\n",
      "{'FN': 6869.000001, 'TP': 130.000001, 'FP': 2870.000001}\n",
      "[2019/03/17 16:56:33] Epoch 0/ Validation Loss:16.687/ F1_score:0.026/ Precision:0.043/ Recall:0.019\n",
      "[2019/03/17 16:56:38] Epoch 0\\Batch 15050\\ Train Loss:21.417\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:43] Epoch 0\\Batch 15100\\ Train Loss:21.401\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:48] Epoch 0\\Batch 15150\\ Train Loss:21.386\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:53] Epoch 0\\Batch 15200\\ Train Loss:21.370\\ Learning rate:0.00030\n",
      "[2019/03/17 16:56:58] Epoch 0\\Batch 15250\\ Train Loss:21.356\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:03] Epoch 0\\Batch 15300\\ Train Loss:21.341\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:08] Epoch 0\\Batch 15350\\ Train Loss:21.326\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:13] Epoch 0\\Batch 15400\\ Train Loss:21.312\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:17] Epoch 0\\Batch 15450\\ Train Loss:21.296\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:22] Epoch 0\\Batch 15500\\ Train Loss:21.281\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:27] Epoch 0\\Batch 15550\\ Train Loss:21.266\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 16:57:32] Epoch 0\\Batch 15600\\ Train Loss:21.251\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:37] Epoch 0\\Batch 15650\\ Train Loss:21.236\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:42] Epoch 0\\Batch 15700\\ Train Loss:21.222\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:47] Epoch 0\\Batch 15750\\ Train Loss:21.207\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:52] Epoch 0\\Batch 15800\\ Train Loss:21.192\\ Learning rate:0.00030\n",
      "[2019/03/17 16:57:57] Epoch 0\\Batch 15850\\ Train Loss:21.177\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:02] Epoch 0\\Batch 15900\\ Train Loss:21.161\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:07] Epoch 0\\Batch 15950\\ Train Loss:21.147\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:12] Epoch 0\\Batch 16000\\ Train Loss:21.132\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:17] Epoch 0\\Batch 16050\\ Train Loss:21.118\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:22] Epoch 0\\Batch 16100\\ Train Loss:21.103\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:27] Epoch 0\\Batch 16150\\ Train Loss:21.088\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:32] Epoch 0\\Batch 16200\\ Train Loss:21.073\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:37] Epoch 0\\Batch 16250\\ Train Loss:21.058\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:42] Epoch 0\\Batch 16300\\ Train Loss:21.043\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:46] Epoch 0\\Batch 16350\\ Train Loss:21.029\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:51] Epoch 0\\Batch 16400\\ Train Loss:21.014\\ Learning rate:0.00030\n",
      "[2019/03/17 16:58:56] Epoch 0\\Batch 16450\\ Train Loss:21.000\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:01] Epoch 0\\Batch 16500\\ Train Loss:20.985\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:06] Epoch 0\\Batch 16550\\ Train Loss:20.971\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:11] Epoch 0\\Batch 16600\\ Train Loss:20.956\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:16] Epoch 0\\Batch 16650\\ Train Loss:20.941\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:21] Epoch 0\\Batch 16700\\ Train Loss:20.926\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:26] Epoch 0\\Batch 16750\\ Train Loss:20.913\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:31] Epoch 0\\Batch 16800\\ Train Loss:20.899\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:36] Epoch 0\\Batch 16850\\ Train Loss:20.884\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:41] Epoch 0\\Batch 16900\\ Train Loss:20.870\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:46] Epoch 0\\Batch 16950\\ Train Loss:20.856\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:50] Epoch 0\\Batch 17000\\ Train Loss:20.842\\ Learning rate:0.00030\n",
      "[2019/03/17 16:59:56] Epoch 0\\Batch 17050\\ Train Loss:20.828\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:01] Epoch 0\\Batch 17100\\ Train Loss:20.814\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:05] Epoch 0\\Batch 17150\\ Train Loss:20.799\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:10] Epoch 0\\Batch 17200\\ Train Loss:20.785\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:15] Epoch 0\\Batch 17250\\ Train Loss:20.771\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:20] Epoch 0\\Batch 17300\\ Train Loss:20.757\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:25] Epoch 0\\Batch 17350\\ Train Loss:20.743\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:30] Epoch 0\\Batch 17400\\ Train Loss:20.729\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:35] Epoch 0\\Batch 17450\\ Train Loss:20.715\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:40] Epoch 0\\Batch 17500\\ Train Loss:20.700\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:45] Epoch 0\\Batch 17550\\ Train Loss:20.685\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:50] Epoch 0\\Batch 17600\\ Train Loss:20.671\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:53] Epoch 0\\Batch 17650\\ Train Loss:20.657\\ Learning rate:0.00030\n",
      "[2019/03/17 17:00:56] Epoch 0\\Batch 17700\\ Train Loss:20.643\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:00] Epoch 0\\Batch 17750\\ Train Loss:20.629\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:03] Epoch 0\\Batch 17800\\ Train Loss:20.615\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:06] Epoch 0\\Batch 17850\\ Train Loss:20.601\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:09] Epoch 0\\Batch 17900\\ Train Loss:20.587\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:13] Epoch 0\\Batch 17950\\ Train Loss:20.573\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:17] Epoch 0\\Batch 18000\\ Train Loss:20.559\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [28]\n",
      "{'FN': 6736.000001, 'TP': 263.000001, 'FP': 2737.000001}\n",
      "[2019/03/17 17:01:40] Epoch 0/ Validation Loss:15.417/ F1_score:0.053/ Precision:0.088/ Recall:0.038\n",
      "[2019/03/17 17:01:45] Epoch 0\\Batch 18050\\ Train Loss:20.544\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:50] Epoch 0\\Batch 18100\\ Train Loss:20.531\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:55] Epoch 0\\Batch 18150\\ Train Loss:20.516\\ Learning rate:0.00030\n",
      "[2019/03/17 17:01:59] Epoch 0\\Batch 18200\\ Train Loss:20.502\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:04] Epoch 0\\Batch 18250\\ Train Loss:20.489\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:10] Epoch 0\\Batch 18300\\ Train Loss:20.475\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:15] Epoch 0\\Batch 18350\\ Train Loss:20.461\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:20] Epoch 0\\Batch 18400\\ Train Loss:20.447\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:25] Epoch 0\\Batch 18450\\ Train Loss:20.433\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:30] Epoch 0\\Batch 18500\\ Train Loss:20.420\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:35] Epoch 0\\Batch 18550\\ Train Loss:20.407\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:40] Epoch 0\\Batch 18600\\ Train Loss:20.393\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:45] Epoch 0\\Batch 18650\\ Train Loss:20.380\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:50] Epoch 0\\Batch 18700\\ Train Loss:20.366\\ Learning rate:0.00030\n",
      "[2019/03/17 17:02:55] Epoch 0\\Batch 18750\\ Train Loss:20.352\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:00] Epoch 0\\Batch 18800\\ Train Loss:20.339\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:05] Epoch 0\\Batch 18850\\ Train Loss:20.325\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:10] Epoch 0\\Batch 18900\\ Train Loss:20.312\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:15] Epoch 0\\Batch 18950\\ Train Loss:20.298\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:19] Epoch 0\\Batch 19000\\ Train Loss:20.285\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:24] Epoch 0\\Batch 19050\\ Train Loss:20.271\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:29] Epoch 0\\Batch 19100\\ Train Loss:20.258\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:34] Epoch 0\\Batch 19150\\ Train Loss:20.244\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:39] Epoch 0\\Batch 19200\\ Train Loss:20.231\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:44] Epoch 0\\Batch 19250\\ Train Loss:20.217\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:49] Epoch 0\\Batch 19300\\ Train Loss:20.204\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:54] Epoch 0\\Batch 19350\\ Train Loss:20.190\\ Learning rate:0.00030\n",
      "[2019/03/17 17:03:59] Epoch 0\\Batch 19400\\ Train Loss:20.177\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:04] Epoch 0\\Batch 19450\\ Train Loss:20.164\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:09] Epoch 0\\Batch 19500\\ Train Loss:20.151\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:14] Epoch 0\\Batch 19550\\ Train Loss:20.137\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:19] Epoch 0\\Batch 19600\\ Train Loss:20.123\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:24] Epoch 0\\Batch 19650\\ Train Loss:20.110\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:29] Epoch 0\\Batch 19700\\ Train Loss:20.098\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:33] Epoch 0\\Batch 19750\\ Train Loss:20.084\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:38] Epoch 0\\Batch 19800\\ Train Loss:20.071\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:43] Epoch 0\\Batch 19850\\ Train Loss:20.057\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:48] Epoch 0\\Batch 19900\\ Train Loss:20.044\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:53] Epoch 0\\Batch 19950\\ Train Loss:20.031\\ Learning rate:0.00030\n",
      "[2019/03/17 17:04:58] Epoch 0\\Batch 20000\\ Train Loss:20.018\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:03] Epoch 0\\Batch 20050\\ Train Loss:20.005\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:08] Epoch 0\\Batch 20100\\ Train Loss:19.992\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:13] Epoch 0\\Batch 20150\\ Train Loss:19.978\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:18] Epoch 0\\Batch 20200\\ Train Loss:19.965\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:23] Epoch 0\\Batch 20250\\ Train Loss:19.952\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:28] Epoch 0\\Batch 20300\\ Train Loss:19.939\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:05:33] Epoch 0\\Batch 20350\\ Train Loss:19.925\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:38] Epoch 0\\Batch 20400\\ Train Loss:19.912\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:43] Epoch 0\\Batch 20450\\ Train Loss:19.899\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:48] Epoch 0\\Batch 20500\\ Train Loss:19.886\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:53] Epoch 0\\Batch 20550\\ Train Loss:19.874\\ Learning rate:0.00030\n",
      "[2019/03/17 17:05:58] Epoch 0\\Batch 20600\\ Train Loss:19.860\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:03] Epoch 0\\Batch 20650\\ Train Loss:19.847\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:08] Epoch 0\\Batch 20700\\ Train Loss:19.835\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:13] Epoch 0\\Batch 20750\\ Train Loss:19.822\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:18] Epoch 0\\Batch 20800\\ Train Loss:19.809\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:22] Epoch 0\\Batch 20850\\ Train Loss:19.796\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:27] Epoch 0\\Batch 20900\\ Train Loss:19.782\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:32] Epoch 0\\Batch 20950\\ Train Loss:19.769\\ Learning rate:0.00030\n",
      "[2019/03/17 17:06:37] Epoch 0\\Batch 21000\\ Train Loss:19.757\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [52]\n",
      "{'FN': 6558.000001, 'TP': 441.000001, 'FP': 2559.000001}\n",
      "[2019/03/17 17:07:00] Epoch 0/ Validation Loss:14.145/ F1_score:0.088/ Precision:0.147/ Recall:0.063\n",
      "[2019/03/17 17:07:05] Epoch 0\\Batch 21050\\ Train Loss:19.743\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:10] Epoch 0\\Batch 21100\\ Train Loss:19.730\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:15] Epoch 0\\Batch 21150\\ Train Loss:19.717\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:20] Epoch 0\\Batch 21200\\ Train Loss:19.705\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:25] Epoch 0\\Batch 21250\\ Train Loss:19.692\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:30] Epoch 0\\Batch 21300\\ Train Loss:19.679\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:35] Epoch 0\\Batch 21350\\ Train Loss:19.666\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:39] Epoch 0\\Batch 21400\\ Train Loss:19.653\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:44] Epoch 0\\Batch 21450\\ Train Loss:19.640\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:49] Epoch 0\\Batch 21500\\ Train Loss:19.627\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:53] Epoch 0\\Batch 21550\\ Train Loss:19.614\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:56] Epoch 0\\Batch 21600\\ Train Loss:19.601\\ Learning rate:0.00030\n",
      "[2019/03/17 17:07:59] Epoch 0\\Batch 21650\\ Train Loss:19.589\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:03] Epoch 0\\Batch 21700\\ Train Loss:19.576\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:06] Epoch 0\\Batch 21750\\ Train Loss:19.563\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:09] Epoch 0\\Batch 21800\\ Train Loss:19.551\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:12] Epoch 0\\Batch 21850\\ Train Loss:19.539\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:17] Epoch 0\\Batch 21900\\ Train Loss:19.526\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:21] Epoch 0\\Batch 21950\\ Train Loss:19.514\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:26] Epoch 0\\Batch 22000\\ Train Loss:19.501\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:31] Epoch 0\\Batch 22050\\ Train Loss:19.489\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:36] Epoch 0\\Batch 22100\\ Train Loss:19.476\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:41] Epoch 0\\Batch 22150\\ Train Loss:19.464\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:46] Epoch 0\\Batch 22200\\ Train Loss:19.452\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:51] Epoch 0\\Batch 22250\\ Train Loss:19.440\\ Learning rate:0.00030\n",
      "[2019/03/17 17:08:56] Epoch 0\\Batch 22300\\ Train Loss:19.428\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:01] Epoch 0\\Batch 22350\\ Train Loss:19.416\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:06] Epoch 0\\Batch 22400\\ Train Loss:19.404\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:11] Epoch 0\\Batch 22450\\ Train Loss:19.391\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:16] Epoch 0\\Batch 22500\\ Train Loss:19.380\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:21] Epoch 0\\Batch 22550\\ Train Loss:19.368\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:26] Epoch 0\\Batch 22600\\ Train Loss:19.356\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:30] Epoch 0\\Batch 22650\\ Train Loss:19.343\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:35] Epoch 0\\Batch 22700\\ Train Loss:19.331\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:40] Epoch 0\\Batch 22750\\ Train Loss:19.318\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:45] Epoch 0\\Batch 22800\\ Train Loss:19.306\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:50] Epoch 0\\Batch 22850\\ Train Loss:19.294\\ Learning rate:0.00030\n",
      "[2019/03/17 17:09:55] Epoch 0\\Batch 22900\\ Train Loss:19.282\\ Learning rate:0.00030\n",
      "[2019/03/17 17:10:00] Epoch 0\\Batch 22950\\ Train Loss:19.271\\ Learning rate:0.00030\n",
      "[2019/03/17 17:10:45] Epoch 0\\Batch 23400\\ Train Loss:19.163\\ Learning rate:0.00030\n",
      "[2019/03/17 17:10:50] Epoch 0\\Batch 23450\\ Train Loss:19.151\\ Learning rate:0.00030\n",
      "[2019/03/17 17:10:55] Epoch 0\\Batch 23500\\ Train Loss:19.139\\ Learning rate:0.00030\n",
      "[2019/03/17 17:11:19] Epoch 0\\Batch 23750\\ Train Loss:19.082\\ Learning rate:0.00030\n",
      "[2019/03/17 17:11:24] Epoch 0\\Batch 23800\\ Train Loss:19.070\\ Learning rate:0.00030\n",
      "[2019/03/17 17:11:29] Epoch 0\\Batch 23850\\ Train Loss:19.059\\ Learning rate:0.00030\n",
      "[2019/03/17 17:11:34] Epoch 0\\Batch 23900\\ Train Loss:19.048\\ Learning rate:0.00030\n",
      "[2019/03/17 17:11:39] Epoch 0\\Batch 23950\\ Train Loss:19.037\\ Learning rate:0.00030\n",
      "[2019/03/17 17:11:44] Epoch 0\\Batch 24000\\ Train Loss:19.025\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6460.000001, 'TP': 539.000001, 'FP': 2461.000001}\n",
      "[2019/03/17 17:12:06] Epoch 0/ Validation Loss:13.213/ F1_score:0.108/ Precision:0.180/ Recall:0.077\n",
      "[2019/03/17 17:12:12] Epoch 0\\Batch 24050\\ Train Loss:19.013\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:17] Epoch 0\\Batch 24100\\ Train Loss:19.001\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:21] Epoch 0\\Batch 24150\\ Train Loss:18.989\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:26] Epoch 0\\Batch 24200\\ Train Loss:18.978\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:31] Epoch 0\\Batch 24250\\ Train Loss:18.966\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:36] Epoch 0\\Batch 24300\\ Train Loss:18.955\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:41] Epoch 0\\Batch 24350\\ Train Loss:18.943\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:46] Epoch 0\\Batch 24400\\ Train Loss:18.932\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:51] Epoch 0\\Batch 24450\\ Train Loss:18.920\\ Learning rate:0.00030\n",
      "[2019/03/17 17:12:56] Epoch 0\\Batch 24500\\ Train Loss:18.908\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:01] Epoch 0\\Batch 24550\\ Train Loss:18.897\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:06] Epoch 0\\Batch 24600\\ Train Loss:18.886\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:11] Epoch 0\\Batch 24650\\ Train Loss:18.874\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:16] Epoch 0\\Batch 24700\\ Train Loss:18.863\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:21] Epoch 0\\Batch 24750\\ Train Loss:18.852\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:26] Epoch 0\\Batch 24800\\ Train Loss:18.841\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:31] Epoch 0\\Batch 24850\\ Train Loss:18.829\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:36] Epoch 0\\Batch 24900\\ Train Loss:18.818\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:41] Epoch 0\\Batch 24950\\ Train Loss:18.807\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:46] Epoch 0\\Batch 25000\\ Train Loss:18.796\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:51] Epoch 0\\Batch 25050\\ Train Loss:18.786\\ Learning rate:0.00030\n",
      "[2019/03/17 17:13:56] Epoch 0\\Batch 25100\\ Train Loss:18.774\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:01] Epoch 0\\Batch 25150\\ Train Loss:18.763\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:06] Epoch 0\\Batch 25200\\ Train Loss:18.752\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:11] Epoch 0\\Batch 25250\\ Train Loss:18.741\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:15] Epoch 0\\Batch 25300\\ Train Loss:18.730\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:20] Epoch 0\\Batch 25350\\ Train Loss:18.720\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:25] Epoch 0\\Batch 25400\\ Train Loss:18.709\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:30] Epoch 0\\Batch 25450\\ Train Loss:18.697\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:35] Epoch 0\\Batch 25500\\ Train Loss:18.686\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:40] Epoch 0\\Batch 25550\\ Train Loss:18.676\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:14:46] Epoch 0\\Batch 25600\\ Train Loss:18.665\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:50] Epoch 0\\Batch 25650\\ Train Loss:18.654\\ Learning rate:0.00030\n",
      "[2019/03/17 17:14:56] Epoch 0\\Batch 25700\\ Train Loss:18.643\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:00] Epoch 0\\Batch 25750\\ Train Loss:18.633\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:04] Epoch 0\\Batch 25800\\ Train Loss:18.622\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:07] Epoch 0\\Batch 25850\\ Train Loss:18.612\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:10] Epoch 0\\Batch 25900\\ Train Loss:18.601\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:14] Epoch 0\\Batch 25950\\ Train Loss:18.590\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:17] Epoch 0\\Batch 26000\\ Train Loss:18.579\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:20] Epoch 0\\Batch 26050\\ Train Loss:18.569\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:23] Epoch 0\\Batch 26100\\ Train Loss:18.558\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:28] Epoch 0\\Batch 26150\\ Train Loss:18.547\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:33] Epoch 0\\Batch 26200\\ Train Loss:18.536\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:37] Epoch 0\\Batch 26250\\ Train Loss:18.526\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:42] Epoch 0\\Batch 26300\\ Train Loss:18.515\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:47] Epoch 0\\Batch 26350\\ Train Loss:18.505\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:52] Epoch 0\\Batch 26400\\ Train Loss:18.495\\ Learning rate:0.00030\n",
      "[2019/03/17 17:15:57] Epoch 0\\Batch 26450\\ Train Loss:18.485\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:02] Epoch 0\\Batch 26500\\ Train Loss:18.475\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:07] Epoch 0\\Batch 26550\\ Train Loss:18.465\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:12] Epoch 0\\Batch 26600\\ Train Loss:18.454\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:17] Epoch 0\\Batch 26650\\ Train Loss:18.444\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:22] Epoch 0\\Batch 26700\\ Train Loss:18.434\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:27] Epoch 0\\Batch 26750\\ Train Loss:18.423\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:32] Epoch 0\\Batch 26800\\ Train Loss:18.413\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:37] Epoch 0\\Batch 26850\\ Train Loss:18.403\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:42] Epoch 0\\Batch 26900\\ Train Loss:18.393\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:47] Epoch 0\\Batch 26950\\ Train Loss:18.383\\ Learning rate:0.00030\n",
      "[2019/03/17 17:16:52] Epoch 0\\Batch 27000\\ Train Loss:18.372\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6373.000001, 'TP': 626.000001, 'FP': 2374.000001}\n",
      "[2019/03/17 17:17:14] Epoch 0/ Validation Loss:12.604/ F1_score:0.125/ Precision:0.209/ Recall:0.089\n",
      "[2019/03/17 17:17:19] Epoch 0\\Batch 27050\\ Train Loss:18.363\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:24] Epoch 0\\Batch 27100\\ Train Loss:18.353\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:29] Epoch 0\\Batch 27150\\ Train Loss:18.343\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:34] Epoch 0\\Batch 27200\\ Train Loss:18.333\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:39] Epoch 0\\Batch 27250\\ Train Loss:18.322\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:44] Epoch 0\\Batch 27300\\ Train Loss:18.313\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:49] Epoch 0\\Batch 27350\\ Train Loss:18.302\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:54] Epoch 0\\Batch 27400\\ Train Loss:18.293\\ Learning rate:0.00030\n",
      "[2019/03/17 17:17:59] Epoch 0\\Batch 27450\\ Train Loss:18.283\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:04] Epoch 0\\Batch 27500\\ Train Loss:18.273\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:09] Epoch 0\\Batch 27550\\ Train Loss:18.263\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:14] Epoch 0\\Batch 27600\\ Train Loss:18.253\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:19] Epoch 0\\Batch 27650\\ Train Loss:18.243\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:24] Epoch 0\\Batch 27700\\ Train Loss:18.234\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:28] Epoch 0\\Batch 27750\\ Train Loss:18.224\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:33] Epoch 0\\Batch 27800\\ Train Loss:18.215\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:38] Epoch 0\\Batch 27850\\ Train Loss:18.205\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:43] Epoch 0\\Batch 27900\\ Train Loss:18.195\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:48] Epoch 0\\Batch 27950\\ Train Loss:18.185\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:53] Epoch 0\\Batch 28000\\ Train Loss:18.176\\ Learning rate:0.00030\n",
      "[2019/03/17 17:18:58] Epoch 0\\Batch 28050\\ Train Loss:18.166\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:03] Epoch 0\\Batch 28100\\ Train Loss:18.157\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:08] Epoch 0\\Batch 28150\\ Train Loss:18.147\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:13] Epoch 0\\Batch 28200\\ Train Loss:18.138\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:19] Epoch 0\\Batch 28250\\ Train Loss:18.128\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:23] Epoch 0\\Batch 28300\\ Train Loss:18.119\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:28] Epoch 0\\Batch 28350\\ Train Loss:18.109\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:33] Epoch 0\\Batch 28400\\ Train Loss:18.099\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:38] Epoch 0\\Batch 28450\\ Train Loss:18.090\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:43] Epoch 0\\Batch 28500\\ Train Loss:18.080\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:48] Epoch 0\\Batch 28550\\ Train Loss:18.071\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:53] Epoch 0\\Batch 28600\\ Train Loss:18.061\\ Learning rate:0.00030\n",
      "[2019/03/17 17:19:58] Epoch 0\\Batch 28650\\ Train Loss:18.052\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:03] Epoch 0\\Batch 28700\\ Train Loss:18.043\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:08] Epoch 0\\Batch 28750\\ Train Loss:18.033\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:13] Epoch 0\\Batch 28800\\ Train Loss:18.024\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:18] Epoch 0\\Batch 28850\\ Train Loss:18.015\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:23] Epoch 0\\Batch 28900\\ Train Loss:18.006\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:28] Epoch 0\\Batch 28950\\ Train Loss:17.996\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:33] Epoch 0\\Batch 29000\\ Train Loss:17.987\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:38] Epoch 0\\Batch 29050\\ Train Loss:17.978\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:42] Epoch 0\\Batch 29100\\ Train Loss:17.968\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:47] Epoch 0\\Batch 29150\\ Train Loss:17.960\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:52] Epoch 0\\Batch 29200\\ Train Loss:17.950\\ Learning rate:0.00030\n",
      "[2019/03/17 17:20:57] Epoch 0\\Batch 29250\\ Train Loss:17.942\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:02] Epoch 0\\Batch 29300\\ Train Loss:17.933\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:07] Epoch 0\\Batch 29350\\ Train Loss:17.924\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:12] Epoch 0\\Batch 29400\\ Train Loss:17.914\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:17] Epoch 0\\Batch 29450\\ Train Loss:17.906\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:22] Epoch 0\\Batch 29500\\ Train Loss:17.897\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:27] Epoch 0\\Batch 29550\\ Train Loss:17.888\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:31] Epoch 0\\Batch 29600\\ Train Loss:17.879\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:36] Epoch 0\\Batch 29650\\ Train Loss:17.870\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:41] Epoch 0\\Batch 29700\\ Train Loss:17.861\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:46] Epoch 0\\Batch 29750\\ Train Loss:17.852\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:51] Epoch 0\\Batch 29800\\ Train Loss:17.843\\ Learning rate:0.00030\n",
      "[2019/03/17 17:21:56] Epoch 0\\Batch 29850\\ Train Loss:17.834\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:01] Epoch 0\\Batch 29900\\ Train Loss:17.826\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:06] Epoch 0\\Batch 29950\\ Train Loss:17.817\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:11] Epoch 0\\Batch 30000\\ Train Loss:17.808\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6283.000001, 'TP': 716.000001, 'FP': 2286.000001}\n",
      "[2019/03/17 17:22:28] Epoch 0/ Validation Loss:12.190/ F1_score:0.143/ Precision:0.239/ Recall:0.102\n",
      "[2019/03/17 17:22:33] Epoch 0\\Batch 30050\\ Train Loss:17.800\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:37] Epoch 0\\Batch 30100\\ Train Loss:17.791\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:42] Epoch 0\\Batch 30150\\ Train Loss:17.782\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:48] Epoch 0\\Batch 30200\\ Train Loss:17.773\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:22:52] Epoch 0\\Batch 30250\\ Train Loss:17.765\\ Learning rate:0.00030\n",
      "[2019/03/17 17:22:57] Epoch 0\\Batch 30300\\ Train Loss:17.756\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:02] Epoch 0\\Batch 30350\\ Train Loss:17.747\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:07] Epoch 0\\Batch 30400\\ Train Loss:17.738\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:12] Epoch 0\\Batch 30450\\ Train Loss:17.729\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:17] Epoch 0\\Batch 30500\\ Train Loss:17.721\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:22] Epoch 0\\Batch 30550\\ Train Loss:17.712\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:27] Epoch 0\\Batch 30600\\ Train Loss:17.704\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:32] Epoch 0\\Batch 30650\\ Train Loss:17.695\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:37] Epoch 0\\Batch 30700\\ Train Loss:17.687\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:42] Epoch 0\\Batch 30750\\ Train Loss:17.678\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:47] Epoch 0\\Batch 30800\\ Train Loss:17.670\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:52] Epoch 0\\Batch 30850\\ Train Loss:17.662\\ Learning rate:0.00030\n",
      "[2019/03/17 17:23:57] Epoch 0\\Batch 30900\\ Train Loss:17.654\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:02] Epoch 0\\Batch 30950\\ Train Loss:17.645\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:07] Epoch 0\\Batch 31000\\ Train Loss:17.637\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:12] Epoch 0\\Batch 31050\\ Train Loss:17.629\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:17] Epoch 0\\Batch 31100\\ Train Loss:17.620\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:22] Epoch 0\\Batch 31150\\ Train Loss:17.612\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:27] Epoch 0\\Batch 31200\\ Train Loss:17.604\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:32] Epoch 0\\Batch 31250\\ Train Loss:17.595\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:37] Epoch 0\\Batch 31300\\ Train Loss:17.587\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:42] Epoch 0\\Batch 31350\\ Train Loss:17.579\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:47] Epoch 0\\Batch 31400\\ Train Loss:17.570\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:52] Epoch 0\\Batch 31450\\ Train Loss:17.562\\ Learning rate:0.00030\n",
      "[2019/03/17 17:24:57] Epoch 0\\Batch 31500\\ Train Loss:17.554\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:02] Epoch 0\\Batch 31550\\ Train Loss:17.545\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:07] Epoch 0\\Batch 31600\\ Train Loss:17.537\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:12] Epoch 0\\Batch 31650\\ Train Loss:17.529\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:17] Epoch 0\\Batch 31700\\ Train Loss:17.521\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:21] Epoch 0\\Batch 31750\\ Train Loss:17.513\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:25] Epoch 0\\Batch 31800\\ Train Loss:17.505\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:28] Epoch 0\\Batch 31850\\ Train Loss:17.497\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:32] Epoch 0\\Batch 31900\\ Train Loss:17.488\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:35] Epoch 0\\Batch 31950\\ Train Loss:17.480\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:38] Epoch 0\\Batch 32000\\ Train Loss:17.472\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:41] Epoch 0\\Batch 32050\\ Train Loss:17.464\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:45] Epoch 0\\Batch 32100\\ Train Loss:17.456\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:48] Epoch 0\\Batch 32150\\ Train Loss:17.448\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:51] Epoch 0\\Batch 32200\\ Train Loss:17.440\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:55] Epoch 0\\Batch 32250\\ Train Loss:17.432\\ Learning rate:0.00030\n",
      "[2019/03/17 17:25:58] Epoch 0\\Batch 32300\\ Train Loss:17.424\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:01] Epoch 0\\Batch 32350\\ Train Loss:17.416\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:04] Epoch 0\\Batch 32400\\ Train Loss:17.408\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:08] Epoch 0\\Batch 32450\\ Train Loss:17.401\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:11] Epoch 0\\Batch 32500\\ Train Loss:17.393\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:16] Epoch 0\\Batch 32550\\ Train Loss:17.385\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:21] Epoch 0\\Batch 32600\\ Train Loss:17.377\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:26] Epoch 0\\Batch 32650\\ Train Loss:17.369\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:31] Epoch 0\\Batch 32700\\ Train Loss:17.362\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:35] Epoch 0\\Batch 32750\\ Train Loss:17.354\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:40] Epoch 0\\Batch 32800\\ Train Loss:17.346\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:46] Epoch 0\\Batch 32850\\ Train Loss:17.338\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:50] Epoch 0\\Batch 32900\\ Train Loss:17.330\\ Learning rate:0.00030\n",
      "[2019/03/17 17:26:55] Epoch 0\\Batch 32950\\ Train Loss:17.322\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:00] Epoch 0\\Batch 33000\\ Train Loss:17.315\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [21]\n",
      "{'FN': 6242.000001, 'TP': 757.000001, 'FP': 2249.000001}\n",
      "[2019/03/17 17:27:23] Epoch 0/ Validation Loss:11.860/ F1_score:0.151/ Precision:0.252/ Recall:0.108\n",
      "[2019/03/17 17:27:28] Epoch 0\\Batch 33050\\ Train Loss:17.307\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:33] Epoch 0\\Batch 33100\\ Train Loss:17.299\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:38] Epoch 0\\Batch 33150\\ Train Loss:17.291\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:43] Epoch 0\\Batch 33200\\ Train Loss:17.283\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:48] Epoch 0\\Batch 33250\\ Train Loss:17.276\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:53] Epoch 0\\Batch 33300\\ Train Loss:17.268\\ Learning rate:0.00030\n",
      "[2019/03/17 17:27:58] Epoch 0\\Batch 33350\\ Train Loss:17.260\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:03] Epoch 0\\Batch 33400\\ Train Loss:17.253\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:08] Epoch 0\\Batch 33450\\ Train Loss:17.245\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:13] Epoch 0\\Batch 33500\\ Train Loss:17.238\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:18] Epoch 0\\Batch 33550\\ Train Loss:17.231\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:23] Epoch 0\\Batch 33600\\ Train Loss:17.223\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:28] Epoch 0\\Batch 33650\\ Train Loss:17.216\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:32] Epoch 0\\Batch 33700\\ Train Loss:17.208\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:37] Epoch 0\\Batch 33750\\ Train Loss:17.200\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:42] Epoch 0\\Batch 33800\\ Train Loss:17.193\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:47] Epoch 0\\Batch 33850\\ Train Loss:17.185\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:52] Epoch 0\\Batch 33900\\ Train Loss:17.178\\ Learning rate:0.00030\n",
      "[2019/03/17 17:28:57] Epoch 0\\Batch 33950\\ Train Loss:17.171\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:02] Epoch 0\\Batch 34000\\ Train Loss:17.163\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:06] Epoch 0\\Batch 34050\\ Train Loss:17.156\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:11] Epoch 0\\Batch 34100\\ Train Loss:17.149\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:16] Epoch 0\\Batch 34150\\ Train Loss:17.141\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:21] Epoch 0\\Batch 34200\\ Train Loss:17.134\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:26] Epoch 0\\Batch 34250\\ Train Loss:17.127\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:31] Epoch 0\\Batch 34300\\ Train Loss:17.119\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:36] Epoch 0\\Batch 34350\\ Train Loss:17.112\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:41] Epoch 0\\Batch 34400\\ Train Loss:17.105\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:46] Epoch 0\\Batch 34450\\ Train Loss:17.098\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:51] Epoch 0\\Batch 34500\\ Train Loss:17.090\\ Learning rate:0.00030\n",
      "[2019/03/17 17:29:56] Epoch 0\\Batch 34550\\ Train Loss:17.083\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:01] Epoch 0\\Batch 34600\\ Train Loss:17.076\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:06] Epoch 0\\Batch 34650\\ Train Loss:17.069\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:11] Epoch 0\\Batch 34700\\ Train Loss:17.062\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:15] Epoch 0\\Batch 34750\\ Train Loss:17.055\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:20] Epoch 0\\Batch 34800\\ Train Loss:17.048\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:25] Epoch 0\\Batch 34850\\ Train Loss:17.041\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:30] Epoch 0\\Batch 34900\\ Train Loss:17.034\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:35] Epoch 0\\Batch 34950\\ Train Loss:17.027\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:30:40] Epoch 0\\Batch 35000\\ Train Loss:17.019\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:45] Epoch 0\\Batch 35050\\ Train Loss:17.012\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:50] Epoch 0\\Batch 35100\\ Train Loss:17.005\\ Learning rate:0.00030\n",
      "[2019/03/17 17:30:55] Epoch 0\\Batch 35150\\ Train Loss:16.998\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:00] Epoch 0\\Batch 35200\\ Train Loss:16.991\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:05] Epoch 0\\Batch 35250\\ Train Loss:16.984\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:10] Epoch 0\\Batch 35300\\ Train Loss:16.977\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:15] Epoch 0\\Batch 35350\\ Train Loss:16.970\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:20] Epoch 0\\Batch 35400\\ Train Loss:16.963\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:25] Epoch 0\\Batch 35450\\ Train Loss:16.956\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:30] Epoch 0\\Batch 35500\\ Train Loss:16.949\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:34] Epoch 0\\Batch 35550\\ Train Loss:16.942\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:39] Epoch 0\\Batch 35600\\ Train Loss:16.935\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:44] Epoch 0\\Batch 35650\\ Train Loss:16.928\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:49] Epoch 0\\Batch 35700\\ Train Loss:16.921\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:54] Epoch 0\\Batch 35750\\ Train Loss:16.914\\ Learning rate:0.00030\n",
      "[2019/03/17 17:31:59] Epoch 0\\Batch 35800\\ Train Loss:16.907\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:04] Epoch 0\\Batch 35850\\ Train Loss:16.900\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:09] Epoch 0\\Batch 35900\\ Train Loss:16.893\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:14] Epoch 0\\Batch 35950\\ Train Loss:16.886\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:19] Epoch 0\\Batch 36000\\ Train Loss:16.879\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [21]\n",
      "{'FN': 6130.000001, 'TP': 869.000001, 'FP': 2148.000001}\n",
      "[2019/03/17 17:32:42] Epoch 0/ Validation Loss:11.607/ F1_score:0.174/ Precision:0.288/ Recall:0.124\n",
      "[2019/03/17 17:32:46] Epoch 0\\Batch 36050\\ Train Loss:16.873\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:49] Epoch 0\\Batch 36100\\ Train Loss:16.866\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:53] Epoch 0\\Batch 36150\\ Train Loss:16.859\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:56] Epoch 0\\Batch 36200\\ Train Loss:16.852\\ Learning rate:0.00030\n",
      "[2019/03/17 17:32:59] Epoch 0\\Batch 36250\\ Train Loss:16.845\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:02] Epoch 0\\Batch 36300\\ Train Loss:16.839\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:05] Epoch 0\\Batch 36350\\ Train Loss:16.832\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:09] Epoch 0\\Batch 36400\\ Train Loss:16.825\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:13] Epoch 0\\Batch 36450\\ Train Loss:16.818\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:18] Epoch 0\\Batch 36500\\ Train Loss:16.811\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:23] Epoch 0\\Batch 36550\\ Train Loss:16.805\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:28] Epoch 0\\Batch 36600\\ Train Loss:16.798\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:33] Epoch 0\\Batch 36650\\ Train Loss:16.791\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:38] Epoch 0\\Batch 36700\\ Train Loss:16.784\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:43] Epoch 0\\Batch 36750\\ Train Loss:16.777\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:47] Epoch 0\\Batch 36800\\ Train Loss:16.771\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:52] Epoch 0\\Batch 36850\\ Train Loss:16.764\\ Learning rate:0.00030\n",
      "[2019/03/17 17:33:57] Epoch 0\\Batch 36900\\ Train Loss:16.758\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:02] Epoch 0\\Batch 36950\\ Train Loss:16.751\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:07] Epoch 0\\Batch 37000\\ Train Loss:16.745\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:12] Epoch 0\\Batch 37050\\ Train Loss:16.738\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:17] Epoch 0\\Batch 37100\\ Train Loss:16.732\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:22] Epoch 0\\Batch 37150\\ Train Loss:16.725\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:27] Epoch 0\\Batch 37200\\ Train Loss:16.719\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:32] Epoch 0\\Batch 37250\\ Train Loss:16.712\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:37] Epoch 0\\Batch 37300\\ Train Loss:16.706\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:42] Epoch 0\\Batch 37350\\ Train Loss:16.699\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:47] Epoch 0\\Batch 37400\\ Train Loss:16.693\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:51] Epoch 0\\Batch 37450\\ Train Loss:16.686\\ Learning rate:0.00030\n",
      "[2019/03/17 17:34:57] Epoch 0\\Batch 37500\\ Train Loss:16.680\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:01] Epoch 0\\Batch 37550\\ Train Loss:16.673\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:06] Epoch 0\\Batch 37600\\ Train Loss:16.667\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:11] Epoch 0\\Batch 37650\\ Train Loss:16.661\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:16] Epoch 0\\Batch 37700\\ Train Loss:16.654\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:21] Epoch 0\\Batch 37750\\ Train Loss:16.648\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:26] Epoch 0\\Batch 37800\\ Train Loss:16.641\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:31] Epoch 0\\Batch 37850\\ Train Loss:16.635\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:36] Epoch 0\\Batch 37900\\ Train Loss:16.629\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:41] Epoch 0\\Batch 37950\\ Train Loss:16.623\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:46] Epoch 0\\Batch 38000\\ Train Loss:16.616\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:51] Epoch 0\\Batch 38050\\ Train Loss:16.610\\ Learning rate:0.00030\n",
      "[2019/03/17 17:35:56] Epoch 0\\Batch 38100\\ Train Loss:16.604\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:01] Epoch 0\\Batch 38150\\ Train Loss:16.597\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:05] Epoch 0\\Batch 38200\\ Train Loss:16.591\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:10] Epoch 0\\Batch 38250\\ Train Loss:16.585\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:15] Epoch 0\\Batch 38300\\ Train Loss:16.579\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:20] Epoch 0\\Batch 38350\\ Train Loss:16.573\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:25] Epoch 0\\Batch 38400\\ Train Loss:16.567\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:30] Epoch 0\\Batch 38450\\ Train Loss:16.561\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:35] Epoch 0\\Batch 38500\\ Train Loss:16.555\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:40] Epoch 0\\Batch 38550\\ Train Loss:16.549\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:45] Epoch 0\\Batch 38600\\ Train Loss:16.542\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:50] Epoch 0\\Batch 38650\\ Train Loss:16.536\\ Learning rate:0.00030\n",
      "[2019/03/17 17:36:55] Epoch 0\\Batch 38700\\ Train Loss:16.530\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:00] Epoch 0\\Batch 38750\\ Train Loss:16.524\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:05] Epoch 0\\Batch 38800\\ Train Loss:16.518\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:10] Epoch 0\\Batch 38850\\ Train Loss:16.512\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:15] Epoch 0\\Batch 38900\\ Train Loss:16.505\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:20] Epoch 0\\Batch 38950\\ Train Loss:16.499\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:24] Epoch 0\\Batch 39000\\ Train Loss:16.493\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 6065.000001, 'TP': 934.000001, 'FP': 2093.000001}\n",
      "[2019/03/17 17:37:47] Epoch 0/ Validation Loss:11.385/ F1_score:0.186/ Precision:0.309/ Recall:0.133\n",
      "[2019/03/17 17:37:53] Epoch 0\\Batch 39050\\ Train Loss:16.487\\ Learning rate:0.00030\n",
      "[2019/03/17 17:37:58] Epoch 0\\Batch 39100\\ Train Loss:16.481\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:03] Epoch 0\\Batch 39150\\ Train Loss:16.475\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:08] Epoch 0\\Batch 39200\\ Train Loss:16.469\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:12] Epoch 0\\Batch 39250\\ Train Loss:16.463\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:17] Epoch 0\\Batch 39300\\ Train Loss:16.457\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:22] Epoch 0\\Batch 39350\\ Train Loss:16.451\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:28] Epoch 0\\Batch 39400\\ Train Loss:16.445\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:33] Epoch 0\\Batch 39450\\ Train Loss:16.439\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:38] Epoch 0\\Batch 39500\\ Train Loss:16.433\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:43] Epoch 0\\Batch 39550\\ Train Loss:16.427\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:48] Epoch 0\\Batch 39600\\ Train Loss:16.421\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:38:53] Epoch 0\\Batch 39650\\ Train Loss:16.415\\ Learning rate:0.00030\n",
      "[2019/03/17 17:38:58] Epoch 0\\Batch 39700\\ Train Loss:16.409\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:03] Epoch 0\\Batch 39750\\ Train Loss:16.403\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:08] Epoch 0\\Batch 39800\\ Train Loss:16.397\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:13] Epoch 0\\Batch 39850\\ Train Loss:16.392\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:18] Epoch 0\\Batch 39900\\ Train Loss:16.386\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:23] Epoch 0\\Batch 39950\\ Train Loss:16.380\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:28] Epoch 0\\Batch 40000\\ Train Loss:16.374\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:33] Epoch 0\\Batch 40050\\ Train Loss:16.368\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:38] Epoch 0\\Batch 40100\\ Train Loss:16.362\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:43] Epoch 0\\Batch 40150\\ Train Loss:16.356\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:48] Epoch 0\\Batch 40200\\ Train Loss:16.350\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:53] Epoch 0\\Batch 40250\\ Train Loss:16.345\\ Learning rate:0.00030\n",
      "[2019/03/17 17:39:57] Epoch 0\\Batch 40300\\ Train Loss:16.339\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:00] Epoch 0\\Batch 40350\\ Train Loss:16.333\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:04] Epoch 0\\Batch 40400\\ Train Loss:16.328\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:07] Epoch 0\\Batch 40450\\ Train Loss:16.322\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:10] Epoch 0\\Batch 40500\\ Train Loss:16.316\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:14] Epoch 0\\Batch 40550\\ Train Loss:16.310\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:17] Epoch 0\\Batch 40600\\ Train Loss:16.305\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:20] Epoch 0\\Batch 40650\\ Train Loss:16.299\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:24] Epoch 0\\Batch 40700\\ Train Loss:16.294\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:29] Epoch 0\\Batch 40750\\ Train Loss:16.288\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:34] Epoch 0\\Batch 40800\\ Train Loss:16.282\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:39] Epoch 0\\Batch 40850\\ Train Loss:16.276\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:44] Epoch 0\\Batch 40900\\ Train Loss:16.270\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:49] Epoch 0\\Batch 40950\\ Train Loss:16.265\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:54] Epoch 0\\Batch 41000\\ Train Loss:16.259\\ Learning rate:0.00030\n",
      "[2019/03/17 17:40:59] Epoch 0\\Batch 41050\\ Train Loss:16.254\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:04] Epoch 0\\Batch 41100\\ Train Loss:16.248\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:09] Epoch 0\\Batch 41150\\ Train Loss:16.243\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:14] Epoch 0\\Batch 41200\\ Train Loss:16.237\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:19] Epoch 0\\Batch 41250\\ Train Loss:16.232\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:24] Epoch 0\\Batch 41300\\ Train Loss:16.226\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:28] Epoch 0\\Batch 41350\\ Train Loss:16.220\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:33] Epoch 0\\Batch 41400\\ Train Loss:16.215\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:38] Epoch 0\\Batch 41450\\ Train Loss:16.209\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:43] Epoch 0\\Batch 41500\\ Train Loss:16.204\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:48] Epoch 0\\Batch 41550\\ Train Loss:16.198\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:53] Epoch 0\\Batch 41600\\ Train Loss:16.193\\ Learning rate:0.00030\n",
      "[2019/03/17 17:41:58] Epoch 0\\Batch 41650\\ Train Loss:16.187\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:03] Epoch 0\\Batch 41700\\ Train Loss:16.182\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:08] Epoch 0\\Batch 41750\\ Train Loss:16.176\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:14] Epoch 0\\Batch 41800\\ Train Loss:16.171\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:19] Epoch 0\\Batch 41850\\ Train Loss:16.166\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:24] Epoch 0\\Batch 41900\\ Train Loss:16.160\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:29] Epoch 0\\Batch 41950\\ Train Loss:16.155\\ Learning rate:0.00030\n",
      "[2019/03/17 17:42:34] Epoch 0\\Batch 42000\\ Train Loss:16.149\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [21]\n",
      "{'FN': 5985.000001, 'TP': 1014.000001, 'FP': 2016.0000009999999}\n",
      "[2019/03/17 17:42:57] Epoch 0/ Validation Loss:11.183/ F1_score:0.202/ Precision:0.335/ Recall:0.145\n",
      "[2019/03/17 17:43:02] Epoch 0\\Batch 42050\\ Train Loss:16.144\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:07] Epoch 0\\Batch 42100\\ Train Loss:16.138\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:13] Epoch 0\\Batch 42150\\ Train Loss:16.133\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:18] Epoch 0\\Batch 42200\\ Train Loss:16.128\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:23] Epoch 0\\Batch 42250\\ Train Loss:16.122\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:28] Epoch 0\\Batch 42300\\ Train Loss:16.117\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:33] Epoch 0\\Batch 42350\\ Train Loss:16.111\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:38] Epoch 0\\Batch 42400\\ Train Loss:16.106\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:43] Epoch 0\\Batch 42450\\ Train Loss:16.101\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:48] Epoch 0\\Batch 42500\\ Train Loss:16.095\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:53] Epoch 0\\Batch 42550\\ Train Loss:16.090\\ Learning rate:0.00030\n",
      "[2019/03/17 17:43:58] Epoch 0\\Batch 42600\\ Train Loss:16.085\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:03] Epoch 0\\Batch 42650\\ Train Loss:16.080\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:08] Epoch 0\\Batch 42700\\ Train Loss:16.074\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:13] Epoch 0\\Batch 42750\\ Train Loss:16.069\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:19] Epoch 0\\Batch 42800\\ Train Loss:16.064\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:24] Epoch 0\\Batch 42850\\ Train Loss:16.059\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:29] Epoch 0\\Batch 42900\\ Train Loss:16.053\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:34] Epoch 0\\Batch 42950\\ Train Loss:16.048\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:39] Epoch 0\\Batch 43000\\ Train Loss:16.043\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:44] Epoch 0\\Batch 43050\\ Train Loss:16.037\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:49] Epoch 0\\Batch 43100\\ Train Loss:16.032\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:54] Epoch 0\\Batch 43150\\ Train Loss:16.027\\ Learning rate:0.00030\n",
      "[2019/03/17 17:44:59] Epoch 0\\Batch 43200\\ Train Loss:16.021\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:04] Epoch 0\\Batch 43250\\ Train Loss:16.016\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:09] Epoch 0\\Batch 43300\\ Train Loss:16.011\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:14] Epoch 0\\Batch 43350\\ Train Loss:16.006\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:19] Epoch 0\\Batch 43400\\ Train Loss:16.001\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:25] Epoch 0\\Batch 43450\\ Train Loss:15.996\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:30] Epoch 0\\Batch 43500\\ Train Loss:15.990\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:35] Epoch 0\\Batch 43550\\ Train Loss:15.985\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:40] Epoch 0\\Batch 43600\\ Train Loss:15.980\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:45] Epoch 0\\Batch 43650\\ Train Loss:15.975\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:50] Epoch 0\\Batch 43700\\ Train Loss:15.970\\ Learning rate:0.00030\n",
      "[2019/03/17 17:45:55] Epoch 0\\Batch 43750\\ Train Loss:15.965\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:01] Epoch 0\\Batch 43800\\ Train Loss:15.960\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:06] Epoch 0\\Batch 43850\\ Train Loss:15.954\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:11] Epoch 0\\Batch 43900\\ Train Loss:15.949\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:16] Epoch 0\\Batch 43950\\ Train Loss:15.944\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:21] Epoch 0\\Batch 44000\\ Train Loss:15.939\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:26] Epoch 0\\Batch 44050\\ Train Loss:15.934\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:31] Epoch 0\\Batch 44100\\ Train Loss:15.929\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:36] Epoch 0\\Batch 44150\\ Train Loss:15.924\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:41] Epoch 0\\Batch 44200\\ Train Loss:15.919\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:46] Epoch 0\\Batch 44250\\ Train Loss:15.914\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:52] Epoch 0\\Batch 44300\\ Train Loss:15.909\\ Learning rate:0.00030\n",
      "[2019/03/17 17:46:57] Epoch 0\\Batch 44350\\ Train Loss:15.904\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:47:01] Epoch 0\\Batch 44400\\ Train Loss:15.899\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:07] Epoch 0\\Batch 44450\\ Train Loss:15.894\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:12] Epoch 0\\Batch 44500\\ Train Loss:15.889\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:16] Epoch 0\\Batch 44550\\ Train Loss:15.884\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:19] Epoch 0\\Batch 44600\\ Train Loss:15.879\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:23] Epoch 0\\Batch 44650\\ Train Loss:15.874\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:26] Epoch 0\\Batch 44700\\ Train Loss:15.869\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:29] Epoch 0\\Batch 44750\\ Train Loss:15.864\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:33] Epoch 0\\Batch 44800\\ Train Loss:15.859\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:36] Epoch 0\\Batch 44850\\ Train Loss:15.854\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:40] Epoch 0\\Batch 44900\\ Train Loss:15.850\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:45] Epoch 0\\Batch 44950\\ Train Loss:15.845\\ Learning rate:0.00030\n",
      "[2019/03/17 17:47:50] Epoch 0\\Batch 45000\\ Train Loss:15.840\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5949.000001, 'TP': 1050.0000009999999, 'FP': 1984.0000009999999}\n",
      "[2019/03/17 17:48:13] Epoch 0/ Validation Loss:11.071/ F1_score:0.209/ Precision:0.346/ Recall:0.150\n",
      "[2019/03/17 17:48:18] Epoch 0\\Batch 45050\\ Train Loss:15.835\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:23] Epoch 0\\Batch 45100\\ Train Loss:15.830\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:28] Epoch 0\\Batch 45150\\ Train Loss:15.825\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:33] Epoch 0\\Batch 45200\\ Train Loss:15.821\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:39] Epoch 0\\Batch 45250\\ Train Loss:15.816\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:44] Epoch 0\\Batch 45300\\ Train Loss:15.811\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:49] Epoch 0\\Batch 45350\\ Train Loss:15.806\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:54] Epoch 0\\Batch 45400\\ Train Loss:15.801\\ Learning rate:0.00030\n",
      "[2019/03/17 17:48:59] Epoch 0\\Batch 45450\\ Train Loss:15.796\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:04] Epoch 0\\Batch 45500\\ Train Loss:15.791\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:09] Epoch 0\\Batch 45550\\ Train Loss:15.786\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:14] Epoch 0\\Batch 45600\\ Train Loss:15.782\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:19] Epoch 0\\Batch 45650\\ Train Loss:15.777\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:24] Epoch 0\\Batch 45700\\ Train Loss:15.772\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:29] Epoch 0\\Batch 45750\\ Train Loss:15.768\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:34] Epoch 0\\Batch 45800\\ Train Loss:15.763\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:39] Epoch 0\\Batch 45850\\ Train Loss:15.758\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:44] Epoch 0\\Batch 45900\\ Train Loss:15.753\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:49] Epoch 0\\Batch 45950\\ Train Loss:15.749\\ Learning rate:0.00030\n",
      "[2019/03/17 17:49:54] Epoch 0\\Batch 46000\\ Train Loss:15.744\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:00] Epoch 0\\Batch 46050\\ Train Loss:15.740\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:05] Epoch 0\\Batch 46100\\ Train Loss:15.735\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:10] Epoch 0\\Batch 46150\\ Train Loss:15.731\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:15] Epoch 0\\Batch 46200\\ Train Loss:15.726\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:25] Epoch 1\\Batch 50\\ Train Loss:11.402\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:30] Epoch 1\\Batch 100\\ Train Loss:11.494\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:35] Epoch 1\\Batch 150\\ Train Loss:11.539\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:41] Epoch 1\\Batch 200\\ Train Loss:11.480\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:46] Epoch 1\\Batch 250\\ Train Loss:11.472\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:51] Epoch 1\\Batch 300\\ Train Loss:11.473\\ Learning rate:0.00030\n",
      "[2019/03/17 17:50:56] Epoch 1\\Batch 350\\ Train Loss:11.466\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:01] Epoch 1\\Batch 400\\ Train Loss:11.470\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:06] Epoch 1\\Batch 450\\ Train Loss:11.450\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:11] Epoch 1\\Batch 500\\ Train Loss:11.440\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:16] Epoch 1\\Batch 550\\ Train Loss:11.424\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:21] Epoch 1\\Batch 600\\ Train Loss:11.424\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:26] Epoch 1\\Batch 650\\ Train Loss:11.418\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:31] Epoch 1\\Batch 700\\ Train Loss:11.427\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:37] Epoch 1\\Batch 750\\ Train Loss:11.422\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:42] Epoch 1\\Batch 800\\ Train Loss:11.420\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:47] Epoch 1\\Batch 850\\ Train Loss:11.423\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:52] Epoch 1\\Batch 900\\ Train Loss:11.428\\ Learning rate:0.00030\n",
      "[2019/03/17 17:51:57] Epoch 1\\Batch 950\\ Train Loss:11.424\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:02] Epoch 1\\Batch 1000\\ Train Loss:11.414\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:08] Epoch 1\\Batch 1050\\ Train Loss:11.416\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:13] Epoch 1\\Batch 1100\\ Train Loss:11.410\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:18] Epoch 1\\Batch 1150\\ Train Loss:11.416\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:23] Epoch 1\\Batch 1200\\ Train Loss:11.411\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:28] Epoch 1\\Batch 1250\\ Train Loss:11.403\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:33] Epoch 1\\Batch 1300\\ Train Loss:11.401\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:38] Epoch 1\\Batch 1350\\ Train Loss:11.396\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:43] Epoch 1\\Batch 1400\\ Train Loss:11.395\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:48] Epoch 1\\Batch 1450\\ Train Loss:11.396\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:53] Epoch 1\\Batch 1500\\ Train Loss:11.389\\ Learning rate:0.00030\n",
      "[2019/03/17 17:52:58] Epoch 1\\Batch 1550\\ Train Loss:11.390\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:03] Epoch 1\\Batch 1600\\ Train Loss:11.384\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:08] Epoch 1\\Batch 1650\\ Train Loss:11.390\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:13] Epoch 1\\Batch 1700\\ Train Loss:11.388\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:19] Epoch 1\\Batch 1750\\ Train Loss:11.383\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:24] Epoch 1\\Batch 1800\\ Train Loss:11.386\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:29] Epoch 1\\Batch 1850\\ Train Loss:11.384\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:34] Epoch 1\\Batch 1900\\ Train Loss:11.385\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:39] Epoch 1\\Batch 1950\\ Train Loss:11.384\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:44] Epoch 1\\Batch 2000\\ Train Loss:11.383\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:49] Epoch 1\\Batch 2050\\ Train Loss:11.384\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:54] Epoch 1\\Batch 2100\\ Train Loss:11.383\\ Learning rate:0.00030\n",
      "[2019/03/17 17:53:59] Epoch 1\\Batch 2150\\ Train Loss:11.386\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:05] Epoch 1\\Batch 2200\\ Train Loss:11.388\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:10] Epoch 1\\Batch 2250\\ Train Loss:11.391\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:15] Epoch 1\\Batch 2300\\ Train Loss:11.392\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:20] Epoch 1\\Batch 2350\\ Train Loss:11.388\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:25] Epoch 1\\Batch 2400\\ Train Loss:11.387\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:30] Epoch 1\\Batch 2450\\ Train Loss:11.386\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:35] Epoch 1\\Batch 2500\\ Train Loss:11.388\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:39] Epoch 1\\Batch 2550\\ Train Loss:11.387\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:42] Epoch 1\\Batch 2600\\ Train Loss:11.383\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:46] Epoch 1\\Batch 2650\\ Train Loss:11.385\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:49] Epoch 1\\Batch 2700\\ Train Loss:11.383\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:52] Epoch 1\\Batch 2750\\ Train Loss:11.387\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:56] Epoch 1\\Batch 2800\\ Train Loss:11.384\\ Learning rate:0.00030\n",
      "[2019/03/17 17:54:59] Epoch 1\\Batch 2850\\ Train Loss:11.381\\ Learning rate:0.00030\n",
      "[2019/03/17 17:55:03] Epoch 1\\Batch 2900\\ Train Loss:11.378\\ Learning rate:0.00030\n",
      "[2019/03/17 17:55:08] Epoch 1\\Batch 2950\\ Train Loss:11.377\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 17:55:14] Epoch 1\\Batch 3000\\ Train Loss:11.375\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5898.000001, 'TP': 1101.0000009999999, 'FP': 1946.0000009999999}\n",
      "[2019/03/17 17:55:37] Epoch 1/ Validation Loss:10.915/ F1_score:0.219/ Precision:0.361/ Recall:0.157\n",
      "[2019/03/17 17:55:42] Epoch 1\\Batch 3050\\ Train Loss:11.374\\ Learning rate:0.00030\n",
      "[2019/03/17 17:55:47] Epoch 1\\Batch 3100\\ Train Loss:11.376\\ Learning rate:0.00030\n",
      "[2019/03/17 17:55:52] Epoch 1\\Batch 3150\\ Train Loss:11.375\\ Learning rate:0.00030\n",
      "[2019/03/17 17:55:57] Epoch 1\\Batch 3200\\ Train Loss:11.372\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:02] Epoch 1\\Batch 3250\\ Train Loss:11.369\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:07] Epoch 1\\Batch 3300\\ Train Loss:11.366\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:12] Epoch 1\\Batch 3350\\ Train Loss:11.363\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:17] Epoch 1\\Batch 3400\\ Train Loss:11.363\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:23] Epoch 1\\Batch 3450\\ Train Loss:11.363\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:27] Epoch 1\\Batch 3500\\ Train Loss:11.360\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:33] Epoch 1\\Batch 3550\\ Train Loss:11.357\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:38] Epoch 1\\Batch 3600\\ Train Loss:11.355\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:43] Epoch 1\\Batch 3650\\ Train Loss:11.353\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:48] Epoch 1\\Batch 3700\\ Train Loss:11.352\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:53] Epoch 1\\Batch 3750\\ Train Loss:11.354\\ Learning rate:0.00030\n",
      "[2019/03/17 17:56:58] Epoch 1\\Batch 3800\\ Train Loss:11.356\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:03] Epoch 1\\Batch 3850\\ Train Loss:11.354\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:08] Epoch 1\\Batch 3900\\ Train Loss:11.352\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:13] Epoch 1\\Batch 3950\\ Train Loss:11.350\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:19] Epoch 1\\Batch 4000\\ Train Loss:11.350\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:24] Epoch 1\\Batch 4050\\ Train Loss:11.350\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:29] Epoch 1\\Batch 4100\\ Train Loss:11.349\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:34] Epoch 1\\Batch 4150\\ Train Loss:11.348\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:39] Epoch 1\\Batch 4200\\ Train Loss:11.347\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:44] Epoch 1\\Batch 4250\\ Train Loss:11.346\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:49] Epoch 1\\Batch 4300\\ Train Loss:11.344\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:54] Epoch 1\\Batch 4350\\ Train Loss:11.345\\ Learning rate:0.00030\n",
      "[2019/03/17 17:57:59] Epoch 1\\Batch 4400\\ Train Loss:11.346\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:04] Epoch 1\\Batch 4450\\ Train Loss:11.343\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:10] Epoch 1\\Batch 4500\\ Train Loss:11.343\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:15] Epoch 1\\Batch 4550\\ Train Loss:11.341\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:20] Epoch 1\\Batch 4600\\ Train Loss:11.340\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:25] Epoch 1\\Batch 4650\\ Train Loss:11.340\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:30] Epoch 1\\Batch 4700\\ Train Loss:11.339\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:35] Epoch 1\\Batch 4750\\ Train Loss:11.338\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:41] Epoch 1\\Batch 4800\\ Train Loss:11.339\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:46] Epoch 1\\Batch 4850\\ Train Loss:11.339\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:51] Epoch 1\\Batch 4900\\ Train Loss:11.337\\ Learning rate:0.00030\n",
      "[2019/03/17 17:58:56] Epoch 1\\Batch 4950\\ Train Loss:11.336\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:01] Epoch 1\\Batch 5000\\ Train Loss:11.336\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:06] Epoch 1\\Batch 5050\\ Train Loss:11.333\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:11] Epoch 1\\Batch 5100\\ Train Loss:11.332\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:16] Epoch 1\\Batch 5150\\ Train Loss:11.331\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:21] Epoch 1\\Batch 5200\\ Train Loss:11.332\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:26] Epoch 1\\Batch 5250\\ Train Loss:11.332\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:31] Epoch 1\\Batch 5300\\ Train Loss:11.332\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:37] Epoch 1\\Batch 5350\\ Train Loss:11.332\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:42] Epoch 1\\Batch 5400\\ Train Loss:11.331\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:47] Epoch 1\\Batch 5450\\ Train Loss:11.331\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:52] Epoch 1\\Batch 5500\\ Train Loss:11.329\\ Learning rate:0.00030\n",
      "[2019/03/17 17:59:57] Epoch 1\\Batch 5550\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:02] Epoch 1\\Batch 5600\\ Train Loss:11.327\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:07] Epoch 1\\Batch 5650\\ Train Loss:11.326\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:12] Epoch 1\\Batch 5700\\ Train Loss:11.327\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:17] Epoch 1\\Batch 5750\\ Train Loss:11.327\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:22] Epoch 1\\Batch 5800\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:27] Epoch 1\\Batch 5850\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:32] Epoch 1\\Batch 5900\\ Train Loss:11.330\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:37] Epoch 1\\Batch 5950\\ Train Loss:11.329\\ Learning rate:0.00030\n",
      "[2019/03/17 18:00:43] Epoch 1\\Batch 6000\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5826.000001, 'TP': 1173.0000009999999, 'FP': 1886.0000009999999}\n",
      "[2019/03/17 18:01:06] Epoch 1/ Validation Loss:10.839/ F1_score:0.233/ Precision:0.383/ Recall:0.168\n",
      "[2019/03/17 18:01:11] Epoch 1\\Batch 6050\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:16] Epoch 1\\Batch 6100\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:21] Epoch 1\\Batch 6150\\ Train Loss:11.328\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:26] Epoch 1\\Batch 6200\\ Train Loss:11.327\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:31] Epoch 1\\Batch 6250\\ Train Loss:11.327\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:36] Epoch 1\\Batch 6300\\ Train Loss:11.326\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:42] Epoch 1\\Batch 6350\\ Train Loss:11.325\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:46] Epoch 1\\Batch 6400\\ Train Loss:11.324\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:50] Epoch 1\\Batch 6450\\ Train Loss:11.323\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:53] Epoch 1\\Batch 6500\\ Train Loss:11.322\\ Learning rate:0.00030\n",
      "[2019/03/17 18:01:56] Epoch 1\\Batch 6550\\ Train Loss:11.321\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:00] Epoch 1\\Batch 6600\\ Train Loss:11.321\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:03] Epoch 1\\Batch 6650\\ Train Loss:11.321\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:06] Epoch 1\\Batch 6700\\ Train Loss:11.320\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:10] Epoch 1\\Batch 6750\\ Train Loss:11.318\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:15] Epoch 1\\Batch 6800\\ Train Loss:11.317\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:20] Epoch 1\\Batch 6850\\ Train Loss:11.318\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:25] Epoch 1\\Batch 6900\\ Train Loss:11.317\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:30] Epoch 1\\Batch 6950\\ Train Loss:11.316\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:35] Epoch 1\\Batch 7000\\ Train Loss:11.315\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:40] Epoch 1\\Batch 7050\\ Train Loss:11.314\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:45] Epoch 1\\Batch 7100\\ Train Loss:11.314\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:50] Epoch 1\\Batch 7150\\ Train Loss:11.313\\ Learning rate:0.00030\n",
      "[2019/03/17 18:02:55] Epoch 1\\Batch 7200\\ Train Loss:11.313\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:00] Epoch 1\\Batch 7250\\ Train Loss:11.313\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:05] Epoch 1\\Batch 7300\\ Train Loss:11.312\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:10] Epoch 1\\Batch 7350\\ Train Loss:11.311\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:15] Epoch 1\\Batch 7400\\ Train Loss:11.311\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:20] Epoch 1\\Batch 7450\\ Train Loss:11.310\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:25] Epoch 1\\Batch 7500\\ Train Loss:11.310\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:30] Epoch 1\\Batch 7550\\ Train Loss:11.309\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:35] Epoch 1\\Batch 7600\\ Train Loss:11.308\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:03:40] Epoch 1\\Batch 7650\\ Train Loss:11.307\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:45] Epoch 1\\Batch 7700\\ Train Loss:11.307\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:50] Epoch 1\\Batch 7750\\ Train Loss:11.306\\ Learning rate:0.00030\n",
      "[2019/03/17 18:03:56] Epoch 1\\Batch 7800\\ Train Loss:11.305\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:01] Epoch 1\\Batch 7850\\ Train Loss:11.305\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:06] Epoch 1\\Batch 7900\\ Train Loss:11.303\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:11] Epoch 1\\Batch 7950\\ Train Loss:11.300\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:16] Epoch 1\\Batch 8000\\ Train Loss:11.300\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:21] Epoch 1\\Batch 8050\\ Train Loss:11.300\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:26] Epoch 1\\Batch 8100\\ Train Loss:11.299\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:31] Epoch 1\\Batch 8150\\ Train Loss:11.299\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:37] Epoch 1\\Batch 8200\\ Train Loss:11.298\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:42] Epoch 1\\Batch 8250\\ Train Loss:11.296\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:47] Epoch 1\\Batch 8300\\ Train Loss:11.296\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:52] Epoch 1\\Batch 8350\\ Train Loss:11.295\\ Learning rate:0.00030\n",
      "[2019/03/17 18:04:57] Epoch 1\\Batch 8400\\ Train Loss:11.294\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:02] Epoch 1\\Batch 8450\\ Train Loss:11.295\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:07] Epoch 1\\Batch 8500\\ Train Loss:11.294\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:13] Epoch 1\\Batch 8550\\ Train Loss:11.294\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:18] Epoch 1\\Batch 8600\\ Train Loss:11.294\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:23] Epoch 1\\Batch 8650\\ Train Loss:11.294\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:28] Epoch 1\\Batch 8700\\ Train Loss:11.293\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:33] Epoch 1\\Batch 8750\\ Train Loss:11.293\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:38] Epoch 1\\Batch 8800\\ Train Loss:11.292\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:43] Epoch 1\\Batch 8850\\ Train Loss:11.291\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:48] Epoch 1\\Batch 8900\\ Train Loss:11.291\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:53] Epoch 1\\Batch 8950\\ Train Loss:11.290\\ Learning rate:0.00030\n",
      "[2019/03/17 18:05:59] Epoch 1\\Batch 9000\\ Train Loss:11.290\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5836.000001, 'TP': 1163.0000009999999, 'FP': 1907.0000009999999}\n",
      "[2019/03/17 18:06:22] Epoch 1/ Validation Loss:10.757/ F1_score:0.231/ Precision:0.379/ Recall:0.166\n",
      "[2019/03/17 18:06:27] Epoch 1\\Batch 9050\\ Train Loss:11.289\\ Learning rate:0.00030\n",
      "[2019/03/17 18:06:32] Epoch 1\\Batch 9100\\ Train Loss:11.290\\ Learning rate:0.00030\n",
      "[2019/03/17 18:06:37] Epoch 1\\Batch 9150\\ Train Loss:11.289\\ Learning rate:0.00030\n",
      "[2019/03/17 18:06:43] Epoch 1\\Batch 9200\\ Train Loss:11.287\\ Learning rate:0.00030\n",
      "[2019/03/17 18:06:48] Epoch 1\\Batch 9250\\ Train Loss:11.287\\ Learning rate:0.00030\n",
      "[2019/03/17 18:06:53] Epoch 1\\Batch 9300\\ Train Loss:11.286\\ Learning rate:0.00030\n",
      "[2019/03/17 18:06:58] Epoch 1\\Batch 9350\\ Train Loss:11.286\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:03] Epoch 1\\Batch 9400\\ Train Loss:11.285\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:08] Epoch 1\\Batch 9450\\ Train Loss:11.285\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:13] Epoch 1\\Batch 9500\\ Train Loss:11.283\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:18] Epoch 1\\Batch 9550\\ Train Loss:11.283\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:24] Epoch 1\\Batch 9600\\ Train Loss:11.283\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:29] Epoch 1\\Batch 9650\\ Train Loss:11.282\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:34] Epoch 1\\Batch 9700\\ Train Loss:11.281\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:39] Epoch 1\\Batch 9750\\ Train Loss:11.281\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:44] Epoch 1\\Batch 9800\\ Train Loss:11.282\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:49] Epoch 1\\Batch 9850\\ Train Loss:11.281\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:54] Epoch 1\\Batch 9900\\ Train Loss:11.281\\ Learning rate:0.00030\n",
      "[2019/03/17 18:07:59] Epoch 1\\Batch 9950\\ Train Loss:11.280\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:04] Epoch 1\\Batch 10000\\ Train Loss:11.279\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:09] Epoch 1\\Batch 10050\\ Train Loss:11.278\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:14] Epoch 1\\Batch 10100\\ Train Loss:11.277\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:19] Epoch 1\\Batch 10150\\ Train Loss:11.277\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:24] Epoch 1\\Batch 10200\\ Train Loss:11.277\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:29] Epoch 1\\Batch 10250\\ Train Loss:11.276\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:34] Epoch 1\\Batch 10300\\ Train Loss:11.275\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:39] Epoch 1\\Batch 10350\\ Train Loss:11.274\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:44] Epoch 1\\Batch 10400\\ Train Loss:11.273\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:50] Epoch 1\\Batch 10450\\ Train Loss:11.273\\ Learning rate:0.00030\n",
      "[2019/03/17 18:08:55] Epoch 1\\Batch 10500\\ Train Loss:11.272\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:00] Epoch 1\\Batch 10550\\ Train Loss:11.271\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:05] Epoch 1\\Batch 10600\\ Train Loss:11.270\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:08] Epoch 1\\Batch 10650\\ Train Loss:11.269\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:11] Epoch 1\\Batch 10700\\ Train Loss:11.268\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:15] Epoch 1\\Batch 10750\\ Train Loss:11.268\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:18] Epoch 1\\Batch 10800\\ Train Loss:11.267\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:22] Epoch 1\\Batch 10850\\ Train Loss:11.266\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:25] Epoch 1\\Batch 10900\\ Train Loss:11.266\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:28] Epoch 1\\Batch 10950\\ Train Loss:11.265\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:33] Epoch 1\\Batch 11000\\ Train Loss:11.265\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:38] Epoch 1\\Batch 11050\\ Train Loss:11.264\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:43] Epoch 1\\Batch 11100\\ Train Loss:11.264\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:48] Epoch 1\\Batch 11150\\ Train Loss:11.263\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:53] Epoch 1\\Batch 11200\\ Train Loss:11.262\\ Learning rate:0.00030\n",
      "[2019/03/17 18:09:59] Epoch 1\\Batch 11250\\ Train Loss:11.261\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:04] Epoch 1\\Batch 11300\\ Train Loss:11.260\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:09] Epoch 1\\Batch 11350\\ Train Loss:11.260\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:14] Epoch 1\\Batch 11400\\ Train Loss:11.259\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:19] Epoch 1\\Batch 11450\\ Train Loss:11.258\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:24] Epoch 1\\Batch 11500\\ Train Loss:11.258\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:29] Epoch 1\\Batch 11550\\ Train Loss:11.257\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:34] Epoch 1\\Batch 11600\\ Train Loss:11.257\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:39] Epoch 1\\Batch 11650\\ Train Loss:11.256\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:45] Epoch 1\\Batch 11700\\ Train Loss:11.256\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:50] Epoch 1\\Batch 11750\\ Train Loss:11.256\\ Learning rate:0.00030\n",
      "[2019/03/17 18:10:55] Epoch 1\\Batch 11800\\ Train Loss:11.256\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:00] Epoch 1\\Batch 11850\\ Train Loss:11.255\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:05] Epoch 1\\Batch 11900\\ Train Loss:11.254\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:10] Epoch 1\\Batch 11950\\ Train Loss:11.253\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:16] Epoch 1\\Batch 12000\\ Train Loss:11.252\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [34]\n",
      "{'FN': 5793.000001, 'TP': 1206.0000009999999, 'FP': 1879.0000009999999}\n",
      "[2019/03/17 18:11:39] Epoch 1/ Validation Loss:10.685/ F1_score:0.239/ Precision:0.391/ Recall:0.172\n",
      "[2019/03/17 18:11:44] Epoch 1\\Batch 12050\\ Train Loss:11.252\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:49] Epoch 1\\Batch 12100\\ Train Loss:11.252\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:54] Epoch 1\\Batch 12150\\ Train Loss:11.252\\ Learning rate:0.00030\n",
      "[2019/03/17 18:11:59] Epoch 1\\Batch 12200\\ Train Loss:11.251\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:05] Epoch 1\\Batch 12250\\ Train Loss:11.250\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:12:10] Epoch 1\\Batch 12300\\ Train Loss:11.249\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:15] Epoch 1\\Batch 12350\\ Train Loss:11.248\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:20] Epoch 1\\Batch 12400\\ Train Loss:11.248\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:25] Epoch 1\\Batch 12450\\ Train Loss:11.247\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:30] Epoch 1\\Batch 12500\\ Train Loss:11.246\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:35] Epoch 1\\Batch 12550\\ Train Loss:11.245\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:40] Epoch 1\\Batch 12600\\ Train Loss:11.245\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:46] Epoch 1\\Batch 12650\\ Train Loss:11.244\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:51] Epoch 1\\Batch 12700\\ Train Loss:11.244\\ Learning rate:0.00030\n",
      "[2019/03/17 18:12:56] Epoch 1\\Batch 12750\\ Train Loss:11.243\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:01] Epoch 1\\Batch 12800\\ Train Loss:11.242\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:06] Epoch 1\\Batch 12850\\ Train Loss:11.241\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:11] Epoch 1\\Batch 12900\\ Train Loss:11.240\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:16] Epoch 1\\Batch 12950\\ Train Loss:11.239\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:21] Epoch 1\\Batch 13000\\ Train Loss:11.238\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:26] Epoch 1\\Batch 13050\\ Train Loss:11.238\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:32] Epoch 1\\Batch 13100\\ Train Loss:11.237\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:37] Epoch 1\\Batch 13150\\ Train Loss:11.237\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:42] Epoch 1\\Batch 13200\\ Train Loss:11.236\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:47] Epoch 1\\Batch 13250\\ Train Loss:11.236\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:52] Epoch 1\\Batch 13300\\ Train Loss:11.235\\ Learning rate:0.00030\n",
      "[2019/03/17 18:13:57] Epoch 1\\Batch 13350\\ Train Loss:11.234\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:02] Epoch 1\\Batch 13400\\ Train Loss:11.234\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:07] Epoch 1\\Batch 13450\\ Train Loss:11.233\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:12] Epoch 1\\Batch 13500\\ Train Loss:11.233\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:17] Epoch 1\\Batch 13550\\ Train Loss:11.232\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:21] Epoch 1\\Batch 13600\\ Train Loss:11.232\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:26] Epoch 1\\Batch 13650\\ Train Loss:11.231\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:31] Epoch 1\\Batch 13700\\ Train Loss:11.230\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:36] Epoch 1\\Batch 13750\\ Train Loss:11.230\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:41] Epoch 1\\Batch 13800\\ Train Loss:11.229\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:46] Epoch 1\\Batch 13850\\ Train Loss:11.229\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:51] Epoch 1\\Batch 13900\\ Train Loss:11.229\\ Learning rate:0.00030\n",
      "[2019/03/17 18:14:56] Epoch 1\\Batch 13950\\ Train Loss:11.229\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:01] Epoch 1\\Batch 14000\\ Train Loss:11.228\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:06] Epoch 1\\Batch 14050\\ Train Loss:11.228\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:11] Epoch 1\\Batch 14100\\ Train Loss:11.227\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:16] Epoch 1\\Batch 14150\\ Train Loss:11.226\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:21] Epoch 1\\Batch 14200\\ Train Loss:11.225\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:25] Epoch 1\\Batch 14250\\ Train Loss:11.225\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:30] Epoch 1\\Batch 14300\\ Train Loss:11.224\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:35] Epoch 1\\Batch 14350\\ Train Loss:11.223\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:40] Epoch 1\\Batch 14400\\ Train Loss:11.223\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:45] Epoch 1\\Batch 14450\\ Train Loss:11.223\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:50] Epoch 1\\Batch 14500\\ Train Loss:11.223\\ Learning rate:0.00030\n",
      "[2019/03/17 18:15:55] Epoch 1\\Batch 14550\\ Train Loss:11.222\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:00] Epoch 1\\Batch 14600\\ Train Loss:11.222\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:05] Epoch 1\\Batch 14650\\ Train Loss:11.221\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:10] Epoch 1\\Batch 14700\\ Train Loss:11.221\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:15] Epoch 1\\Batch 14750\\ Train Loss:11.220\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:20] Epoch 1\\Batch 14800\\ Train Loss:11.220\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:24] Epoch 1\\Batch 14850\\ Train Loss:11.219\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:27] Epoch 1\\Batch 14900\\ Train Loss:11.218\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:31] Epoch 1\\Batch 14950\\ Train Loss:11.218\\ Learning rate:0.00030\n",
      "[2019/03/17 18:16:34] Epoch 1\\Batch 15000\\ Train Loss:11.218\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5769.000001, 'TP': 1230.0000009999999, 'FP': 1857.0000009999999}\n",
      "[2019/03/17 18:16:54] Epoch 1/ Validation Loss:10.628/ F1_score:0.244/ Precision:0.398/ Recall:0.176\n",
      "[2019/03/17 18:16:59] Epoch 1\\Batch 15050\\ Train Loss:11.217\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:04] Epoch 1\\Batch 15100\\ Train Loss:11.217\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:09] Epoch 1\\Batch 15150\\ Train Loss:11.216\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:15] Epoch 1\\Batch 15200\\ Train Loss:11.216\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:20] Epoch 1\\Batch 15250\\ Train Loss:11.216\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:25] Epoch 1\\Batch 15300\\ Train Loss:11.215\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:30] Epoch 1\\Batch 15350\\ Train Loss:11.215\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:36] Epoch 1\\Batch 15400\\ Train Loss:11.215\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:41] Epoch 1\\Batch 15450\\ Train Loss:11.214\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:46] Epoch 1\\Batch 15500\\ Train Loss:11.213\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:51] Epoch 1\\Batch 15550\\ Train Loss:11.213\\ Learning rate:0.00030\n",
      "[2019/03/17 18:17:57] Epoch 1\\Batch 15600\\ Train Loss:11.212\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:02] Epoch 1\\Batch 15650\\ Train Loss:11.211\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:07] Epoch 1\\Batch 15700\\ Train Loss:11.211\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:12] Epoch 1\\Batch 15750\\ Train Loss:11.210\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:17] Epoch 1\\Batch 15800\\ Train Loss:11.210\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:22] Epoch 1\\Batch 15850\\ Train Loss:11.208\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:28] Epoch 1\\Batch 15900\\ Train Loss:11.207\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:33] Epoch 1\\Batch 15950\\ Train Loss:11.207\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:38] Epoch 1\\Batch 16000\\ Train Loss:11.206\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:43] Epoch 1\\Batch 16050\\ Train Loss:11.206\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:49] Epoch 1\\Batch 16100\\ Train Loss:11.205\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:54] Epoch 1\\Batch 16150\\ Train Loss:11.205\\ Learning rate:0.00030\n",
      "[2019/03/17 18:18:59] Epoch 1\\Batch 16200\\ Train Loss:11.204\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:04] Epoch 1\\Batch 16250\\ Train Loss:11.203\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:10] Epoch 1\\Batch 16300\\ Train Loss:11.203\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:15] Epoch 1\\Batch 16350\\ Train Loss:11.203\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:21] Epoch 1\\Batch 16400\\ Train Loss:11.202\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:26] Epoch 1\\Batch 16450\\ Train Loss:11.202\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:31] Epoch 1\\Batch 16500\\ Train Loss:11.202\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:36] Epoch 1\\Batch 16550\\ Train Loss:11.201\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:42] Epoch 1\\Batch 16600\\ Train Loss:11.200\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:47] Epoch 1\\Batch 16650\\ Train Loss:11.199\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:52] Epoch 1\\Batch 16700\\ Train Loss:11.198\\ Learning rate:0.00030\n",
      "[2019/03/17 18:19:57] Epoch 1\\Batch 16750\\ Train Loss:11.198\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:02] Epoch 1\\Batch 16800\\ Train Loss:11.198\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:07] Epoch 1\\Batch 16850\\ Train Loss:11.197\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:12] Epoch 1\\Batch 16900\\ Train Loss:11.197\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:17] Epoch 1\\Batch 16950\\ Train Loss:11.197\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:22] Epoch 1\\Batch 17000\\ Train Loss:11.197\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:20:27] Epoch 1\\Batch 17050\\ Train Loss:11.197\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:33] Epoch 1\\Batch 17100\\ Train Loss:11.196\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:38] Epoch 1\\Batch 17150\\ Train Loss:11.195\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:43] Epoch 1\\Batch 17200\\ Train Loss:11.194\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:48] Epoch 1\\Batch 17250\\ Train Loss:11.194\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:53] Epoch 1\\Batch 17300\\ Train Loss:11.194\\ Learning rate:0.00030\n",
      "[2019/03/17 18:20:58] Epoch 1\\Batch 17350\\ Train Loss:11.193\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:03] Epoch 1\\Batch 17400\\ Train Loss:11.193\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:08] Epoch 1\\Batch 17450\\ Train Loss:11.192\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:13] Epoch 1\\Batch 17500\\ Train Loss:11.191\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:18] Epoch 1\\Batch 17550\\ Train Loss:11.190\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:23] Epoch 1\\Batch 17600\\ Train Loss:11.189\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:28] Epoch 1\\Batch 17650\\ Train Loss:11.188\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:33] Epoch 1\\Batch 17700\\ Train Loss:11.188\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:39] Epoch 1\\Batch 17750\\ Train Loss:11.188\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:44] Epoch 1\\Batch 17800\\ Train Loss:11.187\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:49] Epoch 1\\Batch 17850\\ Train Loss:11.187\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:54] Epoch 1\\Batch 17900\\ Train Loss:11.186\\ Learning rate:0.00030\n",
      "[2019/03/17 18:21:59] Epoch 1\\Batch 17950\\ Train Loss:11.185\\ Learning rate:0.00030\n",
      "[2019/03/17 18:22:04] Epoch 1\\Batch 18000\\ Train Loss:11.184\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5738.000001, 'TP': 1261.0000009999999, 'FP': 1843.0000009999999}\n",
      "[2019/03/17 18:22:27] Epoch 1/ Validation Loss:10.570/ F1_score:0.250/ Precision:0.406/ Recall:0.180\n",
      "[2019/03/17 18:22:32] Epoch 1\\Batch 18050\\ Train Loss:11.183\\ Learning rate:0.00030\n",
      "[2019/03/17 18:22:37] Epoch 1\\Batch 18100\\ Train Loss:11.183\\ Learning rate:0.00030\n",
      "[2019/03/17 18:22:42] Epoch 1\\Batch 18150\\ Train Loss:11.182\\ Learning rate:0.00030\n",
      "[2019/03/17 18:22:47] Epoch 1\\Batch 18200\\ Train Loss:11.181\\ Learning rate:0.00030\n",
      "[2019/03/17 18:22:52] Epoch 1\\Batch 18250\\ Train Loss:11.181\\ Learning rate:0.00030\n",
      "[2019/03/17 18:22:57] Epoch 1\\Batch 18300\\ Train Loss:11.181\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:02] Epoch 1\\Batch 18350\\ Train Loss:11.180\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:07] Epoch 1\\Batch 18400\\ Train Loss:11.180\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:12] Epoch 1\\Batch 18450\\ Train Loss:11.179\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:17] Epoch 1\\Batch 18500\\ Train Loss:11.179\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:23] Epoch 1\\Batch 18550\\ Train Loss:11.179\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:28] Epoch 1\\Batch 18600\\ Train Loss:11.179\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:33] Epoch 1\\Batch 18650\\ Train Loss:11.178\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:36] Epoch 1\\Batch 18700\\ Train Loss:11.178\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:39] Epoch 1\\Batch 18750\\ Train Loss:11.177\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:43] Epoch 1\\Batch 18800\\ Train Loss:11.177\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:46] Epoch 1\\Batch 18850\\ Train Loss:11.177\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:50] Epoch 1\\Batch 18900\\ Train Loss:11.176\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:53] Epoch 1\\Batch 18950\\ Train Loss:11.176\\ Learning rate:0.00030\n",
      "[2019/03/17 18:23:57] Epoch 1\\Batch 19000\\ Train Loss:11.175\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:02] Epoch 1\\Batch 19050\\ Train Loss:11.175\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:07] Epoch 1\\Batch 19100\\ Train Loss:11.175\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:11] Epoch 1\\Batch 19150\\ Train Loss:11.174\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:16] Epoch 1\\Batch 19200\\ Train Loss:11.174\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:21] Epoch 1\\Batch 19250\\ Train Loss:11.173\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:26] Epoch 1\\Batch 19300\\ Train Loss:11.173\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:31] Epoch 1\\Batch 19350\\ Train Loss:11.172\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:36] Epoch 1\\Batch 19400\\ Train Loss:11.172\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:41] Epoch 1\\Batch 19450\\ Train Loss:11.172\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:46] Epoch 1\\Batch 19500\\ Train Loss:11.172\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:51] Epoch 1\\Batch 19550\\ Train Loss:11.171\\ Learning rate:0.00030\n",
      "[2019/03/17 18:24:56] Epoch 1\\Batch 19600\\ Train Loss:11.170\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:01] Epoch 1\\Batch 19650\\ Train Loss:11.170\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:06] Epoch 1\\Batch 19700\\ Train Loss:11.170\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:11] Epoch 1\\Batch 19750\\ Train Loss:11.170\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:16] Epoch 1\\Batch 19800\\ Train Loss:11.169\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:21] Epoch 1\\Batch 19850\\ Train Loss:11.169\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:26] Epoch 1\\Batch 19900\\ Train Loss:11.169\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:31] Epoch 1\\Batch 19950\\ Train Loss:11.169\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:36] Epoch 1\\Batch 20000\\ Train Loss:11.168\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:41] Epoch 1\\Batch 20050\\ Train Loss:11.168\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:46] Epoch 1\\Batch 20100\\ Train Loss:11.167\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:51] Epoch 1\\Batch 20150\\ Train Loss:11.167\\ Learning rate:0.00030\n",
      "[2019/03/17 18:25:56] Epoch 1\\Batch 20200\\ Train Loss:11.166\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:01] Epoch 1\\Batch 20250\\ Train Loss:11.166\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:06] Epoch 1\\Batch 20300\\ Train Loss:11.165\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:11] Epoch 1\\Batch 20350\\ Train Loss:11.164\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:16] Epoch 1\\Batch 20400\\ Train Loss:11.164\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:21] Epoch 1\\Batch 20450\\ Train Loss:11.164\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:26] Epoch 1\\Batch 20500\\ Train Loss:11.163\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:31] Epoch 1\\Batch 20550\\ Train Loss:11.163\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:36] Epoch 1\\Batch 20600\\ Train Loss:11.163\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:41] Epoch 1\\Batch 20650\\ Train Loss:11.162\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:46] Epoch 1\\Batch 20700\\ Train Loss:11.162\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:51] Epoch 1\\Batch 20750\\ Train Loss:11.161\\ Learning rate:0.00030\n",
      "[2019/03/17 18:26:56] Epoch 1\\Batch 20800\\ Train Loss:11.161\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:01] Epoch 1\\Batch 20850\\ Train Loss:11.160\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:06] Epoch 1\\Batch 20900\\ Train Loss:11.159\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:11] Epoch 1\\Batch 20950\\ Train Loss:11.159\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:16] Epoch 1\\Batch 21000\\ Train Loss:11.159\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5715.000001, 'TP': 1284.0000009999999, 'FP': 1808.0000009999999}\n",
      "[2019/03/17 18:27:39] Epoch 1/ Validation Loss:10.535/ F1_score:0.254/ Precision:0.415/ Recall:0.183\n",
      "[2019/03/17 18:27:44] Epoch 1\\Batch 21050\\ Train Loss:11.158\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:49] Epoch 1\\Batch 21100\\ Train Loss:11.157\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:54] Epoch 1\\Batch 21150\\ Train Loss:11.156\\ Learning rate:0.00030\n",
      "[2019/03/17 18:27:59] Epoch 1\\Batch 21200\\ Train Loss:11.156\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:04] Epoch 1\\Batch 21250\\ Train Loss:11.155\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:09] Epoch 1\\Batch 21300\\ Train Loss:11.155\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:14] Epoch 1\\Batch 21350\\ Train Loss:11.154\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:20] Epoch 1\\Batch 21400\\ Train Loss:11.153\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:25] Epoch 1\\Batch 21450\\ Train Loss:11.153\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:30] Epoch 1\\Batch 21500\\ Train Loss:11.152\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:35] Epoch 1\\Batch 21550\\ Train Loss:11.151\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:40] Epoch 1\\Batch 21600\\ Train Loss:11.151\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:28:45] Epoch 1\\Batch 21650\\ Train Loss:11.150\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:50] Epoch 1\\Batch 21700\\ Train Loss:11.150\\ Learning rate:0.00030\n",
      "[2019/03/17 18:28:55] Epoch 1\\Batch 21750\\ Train Loss:11.149\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:00] Epoch 1\\Batch 21800\\ Train Loss:11.149\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:05] Epoch 1\\Batch 21850\\ Train Loss:11.148\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:10] Epoch 1\\Batch 21900\\ Train Loss:11.148\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:15] Epoch 1\\Batch 21950\\ Train Loss:11.148\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:20] Epoch 1\\Batch 22000\\ Train Loss:11.147\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:25] Epoch 1\\Batch 22050\\ Train Loss:11.147\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:30] Epoch 1\\Batch 22100\\ Train Loss:11.146\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:35] Epoch 1\\Batch 22150\\ Train Loss:11.146\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:41] Epoch 1\\Batch 22200\\ Train Loss:11.145\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:46] Epoch 1\\Batch 22250\\ Train Loss:11.145\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:51] Epoch 1\\Batch 22300\\ Train Loss:11.145\\ Learning rate:0.00030\n",
      "[2019/03/17 18:29:56] Epoch 1\\Batch 22350\\ Train Loss:11.145\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:00] Epoch 1\\Batch 22400\\ Train Loss:11.144\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:05] Epoch 1\\Batch 22450\\ Train Loss:11.144\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:10] Epoch 1\\Batch 22500\\ Train Loss:11.143\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:15] Epoch 1\\Batch 22550\\ Train Loss:11.143\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:20] Epoch 1\\Batch 22600\\ Train Loss:11.143\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:26] Epoch 1\\Batch 22650\\ Train Loss:11.142\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:30] Epoch 1\\Batch 22700\\ Train Loss:11.141\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:36] Epoch 1\\Batch 22750\\ Train Loss:11.141\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:41] Epoch 1\\Batch 22800\\ Train Loss:11.140\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:45] Epoch 1\\Batch 22850\\ Train Loss:11.140\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:48] Epoch 1\\Batch 22900\\ Train Loss:11.139\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:52] Epoch 1\\Batch 22950\\ Train Loss:11.139\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:55] Epoch 1\\Batch 23000\\ Train Loss:11.139\\ Learning rate:0.00030\n",
      "[2019/03/17 18:30:59] Epoch 1\\Batch 23050\\ Train Loss:11.138\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:02] Epoch 1\\Batch 23100\\ Train Loss:11.138\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:06] Epoch 1\\Batch 23150\\ Train Loss:11.138\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:09] Epoch 1\\Batch 23200\\ Train Loss:11.137\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:14] Epoch 1\\Batch 23250\\ Train Loss:11.137\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:19] Epoch 1\\Batch 23300\\ Train Loss:11.136\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:24] Epoch 1\\Batch 23350\\ Train Loss:11.135\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:29] Epoch 1\\Batch 23400\\ Train Loss:11.135\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:34] Epoch 1\\Batch 23450\\ Train Loss:11.135\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:39] Epoch 1\\Batch 23500\\ Train Loss:11.134\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:44] Epoch 1\\Batch 23550\\ Train Loss:11.134\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:49] Epoch 1\\Batch 23600\\ Train Loss:11.133\\ Learning rate:0.00030\n",
      "[2019/03/17 18:31:54] Epoch 1\\Batch 23650\\ Train Loss:11.133\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:00] Epoch 1\\Batch 23700\\ Train Loss:11.132\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:05] Epoch 1\\Batch 23750\\ Train Loss:11.132\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:10] Epoch 1\\Batch 23800\\ Train Loss:11.132\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:15] Epoch 1\\Batch 23850\\ Train Loss:11.132\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:20] Epoch 1\\Batch 23900\\ Train Loss:11.131\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:25] Epoch 1\\Batch 23950\\ Train Loss:11.131\\ Learning rate:0.00030\n",
      "[2019/03/17 18:32:30] Epoch 1\\Batch 24000\\ Train Loss:11.131\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5725.000001, 'TP': 1274.0000009999999, 'FP': 1837.0000009999999}\n",
      "[2019/03/17 18:32:52] Epoch 1/ Validation Loss:10.497/ F1_score:0.252/ Precision:0.410/ Recall:0.182\n",
      "[2019/03/17 18:32:58] Epoch 1\\Batch 24050\\ Train Loss:11.130\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:03] Epoch 1\\Batch 24100\\ Train Loss:11.129\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:08] Epoch 1\\Batch 24150\\ Train Loss:11.129\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:13] Epoch 1\\Batch 24200\\ Train Loss:11.128\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:17] Epoch 1\\Batch 24250\\ Train Loss:11.127\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:23] Epoch 1\\Batch 24300\\ Train Loss:11.127\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:27] Epoch 1\\Batch 24350\\ Train Loss:11.126\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:32] Epoch 1\\Batch 24400\\ Train Loss:11.125\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:37] Epoch 1\\Batch 24450\\ Train Loss:11.125\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:42] Epoch 1\\Batch 24500\\ Train Loss:11.124\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:47] Epoch 1\\Batch 24550\\ Train Loss:11.123\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:52] Epoch 1\\Batch 24600\\ Train Loss:11.123\\ Learning rate:0.00030\n",
      "[2019/03/17 18:33:57] Epoch 1\\Batch 24650\\ Train Loss:11.122\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:02] Epoch 1\\Batch 24700\\ Train Loss:11.122\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:07] Epoch 1\\Batch 24750\\ Train Loss:11.121\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:12] Epoch 1\\Batch 24800\\ Train Loss:11.121\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:17] Epoch 1\\Batch 24850\\ Train Loss:11.120\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:22] Epoch 1\\Batch 24900\\ Train Loss:11.120\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:27] Epoch 1\\Batch 24950\\ Train Loss:11.119\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:32] Epoch 1\\Batch 25000\\ Train Loss:11.119\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:37] Epoch 1\\Batch 25050\\ Train Loss:11.118\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:42] Epoch 1\\Batch 25100\\ Train Loss:11.118\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:47] Epoch 1\\Batch 25150\\ Train Loss:11.117\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:52] Epoch 1\\Batch 25200\\ Train Loss:11.117\\ Learning rate:0.00030\n",
      "[2019/03/17 18:34:56] Epoch 1\\Batch 25250\\ Train Loss:11.116\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:01] Epoch 1\\Batch 25300\\ Train Loss:11.116\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:06] Epoch 1\\Batch 25350\\ Train Loss:11.116\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:11] Epoch 1\\Batch 25400\\ Train Loss:11.115\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:16] Epoch 1\\Batch 25450\\ Train Loss:11.114\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:21] Epoch 1\\Batch 25500\\ Train Loss:11.114\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:26] Epoch 1\\Batch 25550\\ Train Loss:11.113\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:31] Epoch 1\\Batch 25600\\ Train Loss:11.113\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:36] Epoch 1\\Batch 25650\\ Train Loss:11.112\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:41] Epoch 1\\Batch 25700\\ Train Loss:11.112\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:46] Epoch 1\\Batch 25750\\ Train Loss:11.111\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:51] Epoch 1\\Batch 25800\\ Train Loss:11.111\\ Learning rate:0.00030\n",
      "[2019/03/17 18:35:56] Epoch 1\\Batch 25850\\ Train Loss:11.111\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:01] Epoch 1\\Batch 25900\\ Train Loss:11.110\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:06] Epoch 1\\Batch 25950\\ Train Loss:11.109\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:11] Epoch 1\\Batch 26000\\ Train Loss:11.109\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:15] Epoch 1\\Batch 26050\\ Train Loss:11.108\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:20] Epoch 1\\Batch 26100\\ Train Loss:11.108\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:25] Epoch 1\\Batch 26150\\ Train Loss:11.107\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:30] Epoch 1\\Batch 26200\\ Train Loss:11.106\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:35] Epoch 1\\Batch 26250\\ Train Loss:11.106\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:40] Epoch 1\\Batch 26300\\ Train Loss:11.105\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:45] Epoch 1\\Batch 26350\\ Train Loss:11.105\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:36:50] Epoch 1\\Batch 26400\\ Train Loss:11.105\\ Learning rate:0.00030\n",
      "[2019/03/17 18:36:55] Epoch 1\\Batch 26450\\ Train Loss:11.105\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:00] Epoch 1\\Batch 26500\\ Train Loss:11.105\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:05] Epoch 1\\Batch 26550\\ Train Loss:11.104\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:10] Epoch 1\\Batch 26600\\ Train Loss:11.104\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:14] Epoch 1\\Batch 26650\\ Train Loss:11.104\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:19] Epoch 1\\Batch 26700\\ Train Loss:11.103\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:24] Epoch 1\\Batch 26750\\ Train Loss:11.102\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:29] Epoch 1\\Batch 26800\\ Train Loss:11.102\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:34] Epoch 1\\Batch 26850\\ Train Loss:11.102\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:39] Epoch 1\\Batch 26900\\ Train Loss:11.101\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:45] Epoch 1\\Batch 26950\\ Train Loss:11.101\\ Learning rate:0.00030\n",
      "[2019/03/17 18:37:49] Epoch 1\\Batch 27000\\ Train Loss:11.100\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5672.000001, 'TP': 1327.0000009999999, 'FP': 1786.0000009999999}\n",
      "[2019/03/17 18:38:07] Epoch 1/ Validation Loss:10.447/ F1_score:0.262/ Precision:0.426/ Recall:0.190\n",
      "[2019/03/17 18:38:11] Epoch 1\\Batch 27050\\ Train Loss:11.100\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:15] Epoch 1\\Batch 27100\\ Train Loss:11.100\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:20] Epoch 1\\Batch 27150\\ Train Loss:11.099\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:25] Epoch 1\\Batch 27200\\ Train Loss:11.099\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:30] Epoch 1\\Batch 27250\\ Train Loss:11.098\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:35] Epoch 1\\Batch 27300\\ Train Loss:11.098\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:40] Epoch 1\\Batch 27350\\ Train Loss:11.097\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:45] Epoch 1\\Batch 27400\\ Train Loss:11.097\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:50] Epoch 1\\Batch 27450\\ Train Loss:11.096\\ Learning rate:0.00030\n",
      "[2019/03/17 18:38:55] Epoch 1\\Batch 27500\\ Train Loss:11.096\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:00] Epoch 1\\Batch 27550\\ Train Loss:11.095\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:05] Epoch 1\\Batch 27600\\ Train Loss:11.095\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:09] Epoch 1\\Batch 27650\\ Train Loss:11.095\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:14] Epoch 1\\Batch 27700\\ Train Loss:11.095\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:19] Epoch 1\\Batch 27750\\ Train Loss:11.095\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:24] Epoch 1\\Batch 27800\\ Train Loss:11.094\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:29] Epoch 1\\Batch 27850\\ Train Loss:11.094\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:34] Epoch 1\\Batch 27900\\ Train Loss:11.093\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:39] Epoch 1\\Batch 27950\\ Train Loss:11.093\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:44] Epoch 1\\Batch 28000\\ Train Loss:11.092\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:50] Epoch 1\\Batch 28050\\ Train Loss:11.092\\ Learning rate:0.00030\n",
      "[2019/03/17 18:39:55] Epoch 1\\Batch 28100\\ Train Loss:11.092\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:00] Epoch 1\\Batch 28150\\ Train Loss:11.091\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:05] Epoch 1\\Batch 28200\\ Train Loss:11.091\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:10] Epoch 1\\Batch 28250\\ Train Loss:11.090\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:15] Epoch 1\\Batch 28300\\ Train Loss:11.090\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:20] Epoch 1\\Batch 28350\\ Train Loss:11.090\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:25] Epoch 1\\Batch 28400\\ Train Loss:11.089\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:30] Epoch 1\\Batch 28450\\ Train Loss:11.088\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:35] Epoch 1\\Batch 28500\\ Train Loss:11.088\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:40] Epoch 1\\Batch 28550\\ Train Loss:11.087\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:45] Epoch 1\\Batch 28600\\ Train Loss:11.087\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:50] Epoch 1\\Batch 28650\\ Train Loss:11.086\\ Learning rate:0.00030\n",
      "[2019/03/17 18:40:55] Epoch 1\\Batch 28700\\ Train Loss:11.086\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:00] Epoch 1\\Batch 28750\\ Train Loss:11.086\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:06] Epoch 1\\Batch 28800\\ Train Loss:11.085\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:11] Epoch 1\\Batch 28850\\ Train Loss:11.085\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:16] Epoch 1\\Batch 28900\\ Train Loss:11.085\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:21] Epoch 1\\Batch 28950\\ Train Loss:11.084\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:26] Epoch 1\\Batch 29000\\ Train Loss:11.084\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:31] Epoch 1\\Batch 29050\\ Train Loss:11.084\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:36] Epoch 1\\Batch 29100\\ Train Loss:11.083\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:41] Epoch 1\\Batch 29150\\ Train Loss:11.083\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:46] Epoch 1\\Batch 29200\\ Train Loss:11.082\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:52] Epoch 1\\Batch 29250\\ Train Loss:11.082\\ Learning rate:0.00030\n",
      "[2019/03/17 18:41:57] Epoch 1\\Batch 29300\\ Train Loss:11.082\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:02] Epoch 1\\Batch 29350\\ Train Loss:11.081\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:07] Epoch 1\\Batch 29400\\ Train Loss:11.081\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:12] Epoch 1\\Batch 29450\\ Train Loss:11.081\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:17] Epoch 1\\Batch 29500\\ Train Loss:11.080\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:22] Epoch 1\\Batch 29550\\ Train Loss:11.080\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:27] Epoch 1\\Batch 29600\\ Train Loss:11.079\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:32] Epoch 1\\Batch 29650\\ Train Loss:11.079\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:36] Epoch 1\\Batch 29700\\ Train Loss:11.079\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:41] Epoch 1\\Batch 29750\\ Train Loss:11.078\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:46] Epoch 1\\Batch 29800\\ Train Loss:11.078\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:51] Epoch 1\\Batch 29850\\ Train Loss:11.078\\ Learning rate:0.00030\n",
      "[2019/03/17 18:42:56] Epoch 1\\Batch 29900\\ Train Loss:11.078\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:01] Epoch 1\\Batch 29950\\ Train Loss:11.077\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:06] Epoch 1\\Batch 30000\\ Train Loss:11.077\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5651.000001, 'TP': 1348.0000009999999, 'FP': 1774.0000009999999}\n",
      "[2019/03/17 18:43:29] Epoch 1/ Validation Loss:10.401/ F1_score:0.266/ Precision:0.432/ Recall:0.193\n",
      "[2019/03/17 18:43:34] Epoch 1\\Batch 30050\\ Train Loss:11.077\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:39] Epoch 1\\Batch 30100\\ Train Loss:11.076\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:44] Epoch 1\\Batch 30150\\ Train Loss:11.076\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:49] Epoch 1\\Batch 30200\\ Train Loss:11.076\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:54] Epoch 1\\Batch 30250\\ Train Loss:11.075\\ Learning rate:0.00030\n",
      "[2019/03/17 18:43:59] Epoch 1\\Batch 30300\\ Train Loss:11.075\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:04] Epoch 1\\Batch 30350\\ Train Loss:11.075\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:09] Epoch 1\\Batch 30400\\ Train Loss:11.074\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:13] Epoch 1\\Batch 30450\\ Train Loss:11.073\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:18] Epoch 1\\Batch 30500\\ Train Loss:11.073\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:23] Epoch 1\\Batch 30550\\ Train Loss:11.073\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:28] Epoch 1\\Batch 30600\\ Train Loss:11.072\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:33] Epoch 1\\Batch 30650\\ Train Loss:11.072\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:38] Epoch 1\\Batch 30700\\ Train Loss:11.071\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:43] Epoch 1\\Batch 30750\\ Train Loss:11.071\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:48] Epoch 1\\Batch 30800\\ Train Loss:11.071\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:53] Epoch 1\\Batch 30850\\ Train Loss:11.071\\ Learning rate:0.00030\n",
      "[2019/03/17 18:44:58] Epoch 1\\Batch 30900\\ Train Loss:11.071\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:01] Epoch 1\\Batch 30950\\ Train Loss:11.070\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:45:05] Epoch 1\\Batch 31000\\ Train Loss:11.070\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:08] Epoch 1\\Batch 31050\\ Train Loss:11.070\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:11] Epoch 1\\Batch 31100\\ Train Loss:11.070\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:15] Epoch 1\\Batch 31150\\ Train Loss:11.069\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:18] Epoch 1\\Batch 31200\\ Train Loss:11.069\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:21] Epoch 1\\Batch 31250\\ Train Loss:11.069\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:25] Epoch 1\\Batch 31300\\ Train Loss:11.068\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:30] Epoch 1\\Batch 31350\\ Train Loss:11.068\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:35] Epoch 1\\Batch 31400\\ Train Loss:11.068\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:40] Epoch 1\\Batch 31450\\ Train Loss:11.067\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:45] Epoch 1\\Batch 31500\\ Train Loss:11.067\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:50] Epoch 1\\Batch 31550\\ Train Loss:11.066\\ Learning rate:0.00030\n",
      "[2019/03/17 18:45:55] Epoch 1\\Batch 31600\\ Train Loss:11.066\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:00] Epoch 1\\Batch 31650\\ Train Loss:11.065\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:05] Epoch 1\\Batch 31700\\ Train Loss:11.065\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:10] Epoch 1\\Batch 31750\\ Train Loss:11.065\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:15] Epoch 1\\Batch 31800\\ Train Loss:11.065\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:19] Epoch 1\\Batch 31850\\ Train Loss:11.064\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:24] Epoch 1\\Batch 31900\\ Train Loss:11.064\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:29] Epoch 1\\Batch 31950\\ Train Loss:11.064\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:34] Epoch 1\\Batch 32000\\ Train Loss:11.063\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:39] Epoch 1\\Batch 32050\\ Train Loss:11.063\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:44] Epoch 1\\Batch 32100\\ Train Loss:11.063\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:49] Epoch 1\\Batch 32150\\ Train Loss:11.062\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:54] Epoch 1\\Batch 32200\\ Train Loss:11.062\\ Learning rate:0.00030\n",
      "[2019/03/17 18:46:59] Epoch 1\\Batch 32250\\ Train Loss:11.061\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:04] Epoch 1\\Batch 32300\\ Train Loss:11.061\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:09] Epoch 1\\Batch 32350\\ Train Loss:11.061\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:14] Epoch 1\\Batch 32400\\ Train Loss:11.060\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:19] Epoch 1\\Batch 32450\\ Train Loss:11.060\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:24] Epoch 1\\Batch 32500\\ Train Loss:11.060\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:29] Epoch 1\\Batch 32550\\ Train Loss:11.060\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:34] Epoch 1\\Batch 32600\\ Train Loss:11.059\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:39] Epoch 1\\Batch 32650\\ Train Loss:11.059\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:44] Epoch 1\\Batch 32700\\ Train Loss:11.059\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:49] Epoch 1\\Batch 32750\\ Train Loss:11.058\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:54] Epoch 1\\Batch 32800\\ Train Loss:11.058\\ Learning rate:0.00030\n",
      "[2019/03/17 18:47:59] Epoch 1\\Batch 32850\\ Train Loss:11.057\\ Learning rate:0.00030\n",
      "[2019/03/17 18:48:04] Epoch 1\\Batch 32900\\ Train Loss:11.057\\ Learning rate:0.00030\n",
      "[2019/03/17 18:48:09] Epoch 1\\Batch 32950\\ Train Loss:11.056\\ Learning rate:0.00030\n",
      "[2019/03/17 18:48:14] Epoch 1\\Batch 33000\\ Train Loss:11.056\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5670.000001, 'TP': 1329.0000009999999, 'FP': 1794.0000009999999}\n",
      "[2019/03/17 18:48:36] Epoch 1/ Validation Loss:10.389/ F1_score:0.263/ Precision:0.426/ Recall:0.190\n",
      "[2019/03/17 18:48:41] Epoch 1\\Batch 33050\\ Train Loss:11.056\\ Learning rate:0.00030\n",
      "[2019/03/17 18:48:46] Epoch 1\\Batch 33100\\ Train Loss:11.055\\ Learning rate:0.00030\n",
      "[2019/03/17 18:48:51] Epoch 1\\Batch 33150\\ Train Loss:11.055\\ Learning rate:0.00030\n",
      "[2019/03/17 18:48:56] Epoch 1\\Batch 33200\\ Train Loss:11.054\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:01] Epoch 1\\Batch 33250\\ Train Loss:11.054\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:06] Epoch 1\\Batch 33300\\ Train Loss:11.054\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:11] Epoch 1\\Batch 33350\\ Train Loss:11.053\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:16] Epoch 1\\Batch 33400\\ Train Loss:11.053\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:21] Epoch 1\\Batch 33450\\ Train Loss:11.052\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:26] Epoch 1\\Batch 33500\\ Train Loss:11.052\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:31] Epoch 1\\Batch 33550\\ Train Loss:11.052\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:36] Epoch 1\\Batch 33600\\ Train Loss:11.052\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:41] Epoch 1\\Batch 33650\\ Train Loss:11.052\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:46] Epoch 1\\Batch 33700\\ Train Loss:11.051\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:51] Epoch 1\\Batch 33750\\ Train Loss:11.051\\ Learning rate:0.00030\n",
      "[2019/03/17 18:49:56] Epoch 1\\Batch 33800\\ Train Loss:11.051\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:01] Epoch 1\\Batch 33850\\ Train Loss:11.050\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:06] Epoch 1\\Batch 33900\\ Train Loss:11.050\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:11] Epoch 1\\Batch 33950\\ Train Loss:11.050\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:15] Epoch 1\\Batch 34000\\ Train Loss:11.049\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:20] Epoch 1\\Batch 34050\\ Train Loss:11.049\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:25] Epoch 1\\Batch 34100\\ Train Loss:11.049\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:30] Epoch 1\\Batch 34150\\ Train Loss:11.048\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:35] Epoch 1\\Batch 34200\\ Train Loss:11.048\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:40] Epoch 1\\Batch 34250\\ Train Loss:11.047\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:45] Epoch 1\\Batch 34300\\ Train Loss:11.047\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:50] Epoch 1\\Batch 34350\\ Train Loss:11.047\\ Learning rate:0.00030\n",
      "[2019/03/17 18:50:55] Epoch 1\\Batch 34400\\ Train Loss:11.046\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:00] Epoch 1\\Batch 34450\\ Train Loss:11.046\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:05] Epoch 1\\Batch 34500\\ Train Loss:11.046\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:10] Epoch 1\\Batch 34550\\ Train Loss:11.046\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:15] Epoch 1\\Batch 34600\\ Train Loss:11.046\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:19] Epoch 1\\Batch 34650\\ Train Loss:11.045\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:24] Epoch 1\\Batch 34700\\ Train Loss:11.045\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:29] Epoch 1\\Batch 34750\\ Train Loss:11.045\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:34] Epoch 1\\Batch 34800\\ Train Loss:11.045\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:39] Epoch 1\\Batch 34850\\ Train Loss:11.044\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:44] Epoch 1\\Batch 34900\\ Train Loss:11.044\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:49] Epoch 1\\Batch 34950\\ Train Loss:11.044\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:54] Epoch 1\\Batch 35000\\ Train Loss:11.044\\ Learning rate:0.00030\n",
      "[2019/03/17 18:51:59] Epoch 1\\Batch 35050\\ Train Loss:11.043\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:04] Epoch 1\\Batch 35100\\ Train Loss:11.043\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:08] Epoch 1\\Batch 35150\\ Train Loss:11.042\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:11] Epoch 1\\Batch 35200\\ Train Loss:11.042\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:14] Epoch 1\\Batch 35250\\ Train Loss:11.042\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:18] Epoch 1\\Batch 35300\\ Train Loss:11.041\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:21] Epoch 1\\Batch 35350\\ Train Loss:11.041\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:24] Epoch 1\\Batch 35400\\ Train Loss:11.041\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:28] Epoch 1\\Batch 35450\\ Train Loss:11.041\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:31] Epoch 1\\Batch 35500\\ Train Loss:11.041\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:36] Epoch 1\\Batch 35550\\ Train Loss:11.040\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:41] Epoch 1\\Batch 35600\\ Train Loss:11.040\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:46] Epoch 1\\Batch 35650\\ Train Loss:11.039\\ Learning rate:0.00030\n",
      "[2019/03/17 18:52:51] Epoch 1\\Batch 35700\\ Train Loss:11.039\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 18:52:56] Epoch 1\\Batch 35750\\ Train Loss:11.039\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:01] Epoch 1\\Batch 35800\\ Train Loss:11.038\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:06] Epoch 1\\Batch 35850\\ Train Loss:11.038\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:11] Epoch 1\\Batch 35900\\ Train Loss:11.037\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:16] Epoch 1\\Batch 35950\\ Train Loss:11.037\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:21] Epoch 1\\Batch 36000\\ Train Loss:11.036\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5667.000001, 'TP': 1332.0000009999999, 'FP': 1796.0000009999999}\n",
      "[2019/03/17 18:53:44] Epoch 1/ Validation Loss:10.373/ F1_score:0.263/ Precision:0.426/ Recall:0.190\n",
      "[2019/03/17 18:53:49] Epoch 1\\Batch 36050\\ Train Loss:11.036\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:54] Epoch 1\\Batch 36100\\ Train Loss:11.036\\ Learning rate:0.00030\n",
      "[2019/03/17 18:53:59] Epoch 1\\Batch 36150\\ Train Loss:11.035\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:04] Epoch 1\\Batch 36200\\ Train Loss:11.035\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:09] Epoch 1\\Batch 36250\\ Train Loss:11.035\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:14] Epoch 1\\Batch 36300\\ Train Loss:11.034\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:19] Epoch 1\\Batch 36350\\ Train Loss:11.034\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:24] Epoch 1\\Batch 36400\\ Train Loss:11.033\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:29] Epoch 1\\Batch 36450\\ Train Loss:11.033\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:34] Epoch 1\\Batch 36500\\ Train Loss:11.032\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:39] Epoch 1\\Batch 36550\\ Train Loss:11.032\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:44] Epoch 1\\Batch 36600\\ Train Loss:11.031\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:49] Epoch 1\\Batch 36650\\ Train Loss:11.031\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:54] Epoch 1\\Batch 36700\\ Train Loss:11.031\\ Learning rate:0.00030\n",
      "[2019/03/17 18:54:59] Epoch 1\\Batch 36750\\ Train Loss:11.030\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:04] Epoch 1\\Batch 36800\\ Train Loss:11.030\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:09] Epoch 1\\Batch 36850\\ Train Loss:11.030\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:14] Epoch 1\\Batch 36900\\ Train Loss:11.029\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:19] Epoch 1\\Batch 36950\\ Train Loss:11.029\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:24] Epoch 1\\Batch 37000\\ Train Loss:11.029\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:29] Epoch 1\\Batch 37050\\ Train Loss:11.028\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:34] Epoch 1\\Batch 37100\\ Train Loss:11.028\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:39] Epoch 1\\Batch 37150\\ Train Loss:11.028\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:44] Epoch 1\\Batch 37200\\ Train Loss:11.027\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:49] Epoch 1\\Batch 37250\\ Train Loss:11.027\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:54] Epoch 1\\Batch 37300\\ Train Loss:11.027\\ Learning rate:0.00030\n",
      "[2019/03/17 18:55:59] Epoch 1\\Batch 37350\\ Train Loss:11.027\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:04] Epoch 1\\Batch 37400\\ Train Loss:11.026\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:09] Epoch 1\\Batch 37450\\ Train Loss:11.026\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:14] Epoch 1\\Batch 37500\\ Train Loss:11.026\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:19] Epoch 1\\Batch 37550\\ Train Loss:11.025\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:24] Epoch 1\\Batch 37600\\ Train Loss:11.025\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:29] Epoch 1\\Batch 37650\\ Train Loss:11.025\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:34] Epoch 1\\Batch 37700\\ Train Loss:11.025\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:39] Epoch 1\\Batch 37750\\ Train Loss:11.024\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:44] Epoch 1\\Batch 37800\\ Train Loss:11.024\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:49] Epoch 1\\Batch 37850\\ Train Loss:11.024\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:54] Epoch 1\\Batch 37900\\ Train Loss:11.023\\ Learning rate:0.00030\n",
      "[2019/03/17 18:56:59] Epoch 1\\Batch 37950\\ Train Loss:11.023\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:04] Epoch 1\\Batch 38000\\ Train Loss:11.023\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:09] Epoch 1\\Batch 38050\\ Train Loss:11.023\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:14] Epoch 1\\Batch 38100\\ Train Loss:11.022\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:19] Epoch 1\\Batch 38150\\ Train Loss:11.022\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:24] Epoch 1\\Batch 38200\\ Train Loss:11.021\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:29] Epoch 1\\Batch 38250\\ Train Loss:11.021\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:33] Epoch 1\\Batch 38300\\ Train Loss:11.021\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:38] Epoch 1\\Batch 38350\\ Train Loss:11.021\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:43] Epoch 1\\Batch 38400\\ Train Loss:11.021\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:48] Epoch 1\\Batch 38450\\ Train Loss:11.020\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:53] Epoch 1\\Batch 38500\\ Train Loss:11.020\\ Learning rate:0.00030\n",
      "[2019/03/17 18:57:58] Epoch 1\\Batch 38550\\ Train Loss:11.020\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:03] Epoch 1\\Batch 38600\\ Train Loss:11.020\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:09] Epoch 1\\Batch 38650\\ Train Loss:11.019\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:14] Epoch 1\\Batch 38700\\ Train Loss:11.019\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:19] Epoch 1\\Batch 38750\\ Train Loss:11.019\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:24] Epoch 1\\Batch 38800\\ Train Loss:11.018\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:29] Epoch 1\\Batch 38850\\ Train Loss:11.018\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:34] Epoch 1\\Batch 38900\\ Train Loss:11.018\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:39] Epoch 1\\Batch 38950\\ Train Loss:11.017\\ Learning rate:0.00030\n",
      "[2019/03/17 18:58:45] Epoch 1\\Batch 39000\\ Train Loss:11.017\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5619.000001, 'TP': 1380.0000009999999, 'FP': 1765.0000009999999}\n",
      "[2019/03/17 18:59:07] Epoch 1/ Validation Loss:10.347/ F1_score:0.272/ Precision:0.439/ Recall:0.197\n",
      "[2019/03/17 18:59:10] Epoch 1\\Batch 39050\\ Train Loss:11.017\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:14] Epoch 1\\Batch 39100\\ Train Loss:11.016\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:17] Epoch 1\\Batch 39150\\ Train Loss:11.016\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:20] Epoch 1\\Batch 39200\\ Train Loss:11.015\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:24] Epoch 1\\Batch 39250\\ Train Loss:11.015\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:27] Epoch 1\\Batch 39300\\ Train Loss:11.015\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:32] Epoch 1\\Batch 39350\\ Train Loss:11.015\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:37] Epoch 1\\Batch 39400\\ Train Loss:11.014\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:42] Epoch 1\\Batch 39450\\ Train Loss:11.014\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:47] Epoch 1\\Batch 39500\\ Train Loss:11.014\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:52] Epoch 1\\Batch 39550\\ Train Loss:11.013\\ Learning rate:0.00030\n",
      "[2019/03/17 18:59:56] Epoch 1\\Batch 39600\\ Train Loss:11.013\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:02] Epoch 1\\Batch 39650\\ Train Loss:11.013\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:06] Epoch 1\\Batch 39700\\ Train Loss:11.012\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:12] Epoch 1\\Batch 39750\\ Train Loss:11.012\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:17] Epoch 1\\Batch 39800\\ Train Loss:11.012\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:22] Epoch 1\\Batch 39850\\ Train Loss:11.012\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:26] Epoch 1\\Batch 39900\\ Train Loss:11.011\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:32] Epoch 1\\Batch 39950\\ Train Loss:11.011\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:36] Epoch 1\\Batch 40000\\ Train Loss:11.011\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:41] Epoch 1\\Batch 40050\\ Train Loss:11.010\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:46] Epoch 1\\Batch 40100\\ Train Loss:11.010\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:51] Epoch 1\\Batch 40150\\ Train Loss:11.010\\ Learning rate:0.00030\n",
      "[2019/03/17 19:00:56] Epoch 1\\Batch 40200\\ Train Loss:11.009\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:01] Epoch 1\\Batch 40250\\ Train Loss:11.009\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:06] Epoch 1\\Batch 40300\\ Train Loss:11.009\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 19:01:11] Epoch 1\\Batch 40350\\ Train Loss:11.008\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:16] Epoch 1\\Batch 40400\\ Train Loss:11.008\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:21] Epoch 1\\Batch 40450\\ Train Loss:11.008\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:26] Epoch 1\\Batch 40500\\ Train Loss:11.008\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:31] Epoch 1\\Batch 40550\\ Train Loss:11.007\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:36] Epoch 1\\Batch 40600\\ Train Loss:11.007\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:41] Epoch 1\\Batch 40650\\ Train Loss:11.007\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:46] Epoch 1\\Batch 40700\\ Train Loss:11.006\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:51] Epoch 1\\Batch 40750\\ Train Loss:11.006\\ Learning rate:0.00030\n",
      "[2019/03/17 19:01:56] Epoch 1\\Batch 40800\\ Train Loss:11.006\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:01] Epoch 1\\Batch 40850\\ Train Loss:11.005\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:06] Epoch 1\\Batch 40900\\ Train Loss:11.005\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:11] Epoch 1\\Batch 40950\\ Train Loss:11.004\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:15] Epoch 1\\Batch 41000\\ Train Loss:11.004\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:20] Epoch 1\\Batch 41050\\ Train Loss:11.004\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:25] Epoch 1\\Batch 41100\\ Train Loss:11.004\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:30] Epoch 1\\Batch 41150\\ Train Loss:11.004\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:35] Epoch 1\\Batch 41200\\ Train Loss:11.003\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:40] Epoch 1\\Batch 41250\\ Train Loss:11.003\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:45] Epoch 1\\Batch 41300\\ Train Loss:11.003\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:51] Epoch 1\\Batch 41350\\ Train Loss:11.002\\ Learning rate:0.00030\n",
      "[2019/03/17 19:02:56] Epoch 1\\Batch 41400\\ Train Loss:11.002\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:01] Epoch 1\\Batch 41450\\ Train Loss:11.002\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:06] Epoch 1\\Batch 41500\\ Train Loss:11.001\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:11] Epoch 1\\Batch 41550\\ Train Loss:11.001\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:16] Epoch 1\\Batch 41600\\ Train Loss:11.001\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:21] Epoch 1\\Batch 41650\\ Train Loss:11.000\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:26] Epoch 1\\Batch 41700\\ Train Loss:11.000\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:31] Epoch 1\\Batch 41750\\ Train Loss:11.000\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:36] Epoch 1\\Batch 41800\\ Train Loss:11.000\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:41] Epoch 1\\Batch 41850\\ Train Loss:11.000\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:46] Epoch 1\\Batch 41900\\ Train Loss:10.999\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:51] Epoch 1\\Batch 41950\\ Train Loss:10.999\\ Learning rate:0.00030\n",
      "[2019/03/17 19:03:56] Epoch 1\\Batch 42000\\ Train Loss:10.998\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5623.000001, 'TP': 1376.0000009999999, 'FP': 1755.0000009999999}\n",
      "[2019/03/17 19:04:19] Epoch 1/ Validation Loss:10.309/ F1_score:0.272/ Precision:0.439/ Recall:0.197\n",
      "[2019/03/17 19:04:24] Epoch 1\\Batch 42050\\ Train Loss:10.998\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:29] Epoch 1\\Batch 42100\\ Train Loss:10.998\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:34] Epoch 1\\Batch 42150\\ Train Loss:10.997\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:39] Epoch 1\\Batch 42200\\ Train Loss:10.997\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:44] Epoch 1\\Batch 42250\\ Train Loss:10.997\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:49] Epoch 1\\Batch 42300\\ Train Loss:10.997\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:54] Epoch 1\\Batch 42350\\ Train Loss:10.996\\ Learning rate:0.00030\n",
      "[2019/03/17 19:04:59] Epoch 1\\Batch 42400\\ Train Loss:10.996\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:04] Epoch 1\\Batch 42450\\ Train Loss:10.996\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:09] Epoch 1\\Batch 42500\\ Train Loss:10.995\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:13] Epoch 1\\Batch 42550\\ Train Loss:10.995\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:18] Epoch 1\\Batch 42600\\ Train Loss:10.995\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:23] Epoch 1\\Batch 42650\\ Train Loss:10.995\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:28] Epoch 1\\Batch 42700\\ Train Loss:10.994\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:33] Epoch 1\\Batch 42750\\ Train Loss:10.994\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:38] Epoch 1\\Batch 42800\\ Train Loss:10.994\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:43] Epoch 1\\Batch 42850\\ Train Loss:10.993\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:48] Epoch 1\\Batch 42900\\ Train Loss:10.993\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:53] Epoch 1\\Batch 42950\\ Train Loss:10.993\\ Learning rate:0.00030\n",
      "[2019/03/17 19:05:58] Epoch 1\\Batch 43000\\ Train Loss:10.992\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:03] Epoch 1\\Batch 43050\\ Train Loss:10.992\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:08] Epoch 1\\Batch 43100\\ Train Loss:10.992\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:13] Epoch 1\\Batch 43150\\ Train Loss:10.991\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:16] Epoch 1\\Batch 43200\\ Train Loss:10.991\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:20] Epoch 1\\Batch 43250\\ Train Loss:10.991\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:23] Epoch 1\\Batch 43300\\ Train Loss:10.990\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:26] Epoch 1\\Batch 43350\\ Train Loss:10.990\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:30] Epoch 1\\Batch 43400\\ Train Loss:10.990\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:33] Epoch 1\\Batch 43450\\ Train Loss:10.989\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:36] Epoch 1\\Batch 43500\\ Train Loss:10.989\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:41] Epoch 1\\Batch 43550\\ Train Loss:10.988\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:46] Epoch 1\\Batch 43600\\ Train Loss:10.988\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:51] Epoch 1\\Batch 43650\\ Train Loss:10.988\\ Learning rate:0.00030\n",
      "[2019/03/17 19:06:55] Epoch 1\\Batch 43700\\ Train Loss:10.988\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:00] Epoch 1\\Batch 43750\\ Train Loss:10.987\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:05] Epoch 1\\Batch 43800\\ Train Loss:10.987\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:10] Epoch 1\\Batch 43850\\ Train Loss:10.987\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:16] Epoch 1\\Batch 43900\\ Train Loss:10.986\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:21] Epoch 1\\Batch 43950\\ Train Loss:10.986\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:26] Epoch 1\\Batch 44000\\ Train Loss:10.986\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:31] Epoch 1\\Batch 44050\\ Train Loss:10.985\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:36] Epoch 1\\Batch 44100\\ Train Loss:10.985\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:41] Epoch 1\\Batch 44150\\ Train Loss:10.985\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:46] Epoch 1\\Batch 44200\\ Train Loss:10.984\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:51] Epoch 1\\Batch 44250\\ Train Loss:10.984\\ Learning rate:0.00030\n",
      "[2019/03/17 19:07:56] Epoch 1\\Batch 44300\\ Train Loss:10.984\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:01] Epoch 1\\Batch 44350\\ Train Loss:10.984\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:06] Epoch 1\\Batch 44400\\ Train Loss:10.983\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:11] Epoch 1\\Batch 44450\\ Train Loss:10.983\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:16] Epoch 1\\Batch 44500\\ Train Loss:10.982\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:21] Epoch 1\\Batch 44550\\ Train Loss:10.982\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:26] Epoch 1\\Batch 44600\\ Train Loss:10.982\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:31] Epoch 1\\Batch 44650\\ Train Loss:10.981\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:36] Epoch 1\\Batch 44700\\ Train Loss:10.981\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:41] Epoch 1\\Batch 44750\\ Train Loss:10.981\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:46] Epoch 1\\Batch 44800\\ Train Loss:10.981\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:51] Epoch 1\\Batch 44850\\ Train Loss:10.980\\ Learning rate:0.00030\n",
      "[2019/03/17 19:08:56] Epoch 1\\Batch 44900\\ Train Loss:10.980\\ Learning rate:0.00030\n",
      "[2019/03/17 19:09:01] Epoch 1\\Batch 44950\\ Train Loss:10.980\\ Learning rate:0.00030\n",
      "[2019/03/17 19:09:06] Epoch 1\\Batch 45000\\ Train Loss:10.980\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5606.000001, 'TP': 1393.0000009999999, 'FP': 1742.0000009999999}\n",
      "[2019/03/17 19:09:29] Epoch 1/ Validation Loss:10.304/ F1_score:0.275/ Precision:0.444/ Recall:0.199\n",
      "[2019/03/17 19:09:34] Epoch 1\\Batch 45050\\ Train Loss:10.979\\ Learning rate:0.00030\n",
      "[2019/03/17 19:09:39] Epoch 1\\Batch 45100\\ Train Loss:10.979\\ Learning rate:0.00030\n",
      "[2019/03/17 19:09:44] Epoch 1\\Batch 45150\\ Train Loss:10.979\\ Learning rate:0.00030\n",
      "[2019/03/17 19:09:49] Epoch 1\\Batch 45200\\ Train Loss:10.978\\ Learning rate:0.00030\n",
      "[2019/03/17 19:09:55] Epoch 1\\Batch 45250\\ Train Loss:10.978\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:00] Epoch 1\\Batch 45300\\ Train Loss:10.978\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:05] Epoch 1\\Batch 45350\\ Train Loss:10.977\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:10] Epoch 1\\Batch 45400\\ Train Loss:10.977\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:15] Epoch 1\\Batch 45450\\ Train Loss:10.977\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:19] Epoch 1\\Batch 45500\\ Train Loss:10.976\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:24] Epoch 1\\Batch 45550\\ Train Loss:10.976\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:29] Epoch 1\\Batch 45600\\ Train Loss:10.976\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:34] Epoch 1\\Batch 45650\\ Train Loss:10.976\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:39] Epoch 1\\Batch 45700\\ Train Loss:10.975\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:44] Epoch 1\\Batch 45750\\ Train Loss:10.975\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:49] Epoch 1\\Batch 45800\\ Train Loss:10.975\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:54] Epoch 1\\Batch 45850\\ Train Loss:10.975\\ Learning rate:0.00030\n",
      "[2019/03/17 19:10:59] Epoch 1\\Batch 45900\\ Train Loss:10.974\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:04] Epoch 1\\Batch 45950\\ Train Loss:10.974\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:08] Epoch 1\\Batch 46000\\ Train Loss:10.974\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:14] Epoch 1\\Batch 46050\\ Train Loss:10.974\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:19] Epoch 1\\Batch 46100\\ Train Loss:10.973\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:23] Epoch 1\\Batch 46150\\ Train Loss:10.973\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:29] Epoch 1\\Batch 46200\\ Train Loss:10.973\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:38] Epoch 2\\Batch 50\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:43] Epoch 2\\Batch 100\\ Train Loss:10.774\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:48] Epoch 2\\Batch 150\\ Train Loss:10.806\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:53] Epoch 2\\Batch 200\\ Train Loss:10.764\\ Learning rate:0.00030\n",
      "[2019/03/17 19:11:58] Epoch 2\\Batch 250\\ Train Loss:10.752\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:03] Epoch 2\\Batch 300\\ Train Loss:10.763\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:08] Epoch 2\\Batch 350\\ Train Loss:10.769\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:13] Epoch 2\\Batch 400\\ Train Loss:10.769\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:18] Epoch 2\\Batch 450\\ Train Loss:10.748\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:23] Epoch 2\\Batch 500\\ Train Loss:10.737\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:28] Epoch 2\\Batch 550\\ Train Loss:10.720\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:33] Epoch 2\\Batch 600\\ Train Loss:10.728\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:38] Epoch 2\\Batch 650\\ Train Loss:10.721\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:43] Epoch 2\\Batch 700\\ Train Loss:10.726\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:48] Epoch 2\\Batch 750\\ Train Loss:10.726\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:53] Epoch 2\\Batch 800\\ Train Loss:10.725\\ Learning rate:0.00030\n",
      "[2019/03/17 19:12:58] Epoch 2\\Batch 850\\ Train Loss:10.731\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:03] Epoch 2\\Batch 900\\ Train Loss:10.738\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:08] Epoch 2\\Batch 950\\ Train Loss:10.730\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:13] Epoch 2\\Batch 1000\\ Train Loss:10.723\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:18] Epoch 2\\Batch 1050\\ Train Loss:10.721\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:23] Epoch 2\\Batch 1100\\ Train Loss:10.717\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:27] Epoch 2\\Batch 1150\\ Train Loss:10.724\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:30] Epoch 2\\Batch 1200\\ Train Loss:10.720\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:33] Epoch 2\\Batch 1250\\ Train Loss:10.715\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:37] Epoch 2\\Batch 1300\\ Train Loss:10.715\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:40] Epoch 2\\Batch 1350\\ Train Loss:10.712\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:44] Epoch 2\\Batch 1400\\ Train Loss:10.713\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:47] Epoch 2\\Batch 1450\\ Train Loss:10.713\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:51] Epoch 2\\Batch 1500\\ Train Loss:10.708\\ Learning rate:0.00030\n",
      "[2019/03/17 19:13:56] Epoch 2\\Batch 1550\\ Train Loss:10.709\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:01] Epoch 2\\Batch 1600\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:06] Epoch 2\\Batch 1650\\ Train Loss:10.713\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:11] Epoch 2\\Batch 1700\\ Train Loss:10.712\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:16] Epoch 2\\Batch 1750\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:21] Epoch 2\\Batch 1800\\ Train Loss:10.710\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:26] Epoch 2\\Batch 1850\\ Train Loss:10.709\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:31] Epoch 2\\Batch 1900\\ Train Loss:10.711\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:36] Epoch 2\\Batch 1950\\ Train Loss:10.713\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:41] Epoch 2\\Batch 2000\\ Train Loss:10.712\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:46] Epoch 2\\Batch 2050\\ Train Loss:10.712\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:51] Epoch 2\\Batch 2100\\ Train Loss:10.711\\ Learning rate:0.00030\n",
      "[2019/03/17 19:14:56] Epoch 2\\Batch 2150\\ Train Loss:10.715\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:01] Epoch 2\\Batch 2200\\ Train Loss:10.716\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:06] Epoch 2\\Batch 2250\\ Train Loss:10.720\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:11] Epoch 2\\Batch 2300\\ Train Loss:10.721\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:16] Epoch 2\\Batch 2350\\ Train Loss:10.719\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:21] Epoch 2\\Batch 2400\\ Train Loss:10.719\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:26] Epoch 2\\Batch 2450\\ Train Loss:10.718\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:31] Epoch 2\\Batch 2500\\ Train Loss:10.722\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:36] Epoch 2\\Batch 2550\\ Train Loss:10.721\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:41] Epoch 2\\Batch 2600\\ Train Loss:10.720\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:46] Epoch 2\\Batch 2650\\ Train Loss:10.722\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:52] Epoch 2\\Batch 2700\\ Train Loss:10.720\\ Learning rate:0.00030\n",
      "[2019/03/17 19:15:57] Epoch 2\\Batch 2750\\ Train Loss:10.725\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:02] Epoch 2\\Batch 2800\\ Train Loss:10.724\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:06] Epoch 2\\Batch 2850\\ Train Loss:10.723\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:11] Epoch 2\\Batch 2900\\ Train Loss:10.720\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:16] Epoch 2\\Batch 2950\\ Train Loss:10.721\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:21] Epoch 2\\Batch 3000\\ Train Loss:10.719\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5624.000001, 'TP': 1375.0000009999999, 'FP': 1753.0000009999999}\n",
      "[2019/03/17 19:16:42] Epoch 2/ Validation Loss:10.265/ F1_score:0.272/ Precision:0.440/ Recall:0.196\n",
      "[2019/03/17 19:16:45] Epoch 2\\Batch 3050\\ Train Loss:10.719\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:49] Epoch 2\\Batch 3100\\ Train Loss:10.721\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:52] Epoch 2\\Batch 3150\\ Train Loss:10.719\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:55] Epoch 2\\Batch 3200\\ Train Loss:10.717\\ Learning rate:0.00030\n",
      "[2019/03/17 19:16:59] Epoch 2\\Batch 3250\\ Train Loss:10.716\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:02] Epoch 2\\Batch 3300\\ Train Loss:10.713\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:06] Epoch 2\\Batch 3350\\ Train Loss:10.710\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:09] Epoch 2\\Batch 3400\\ Train Loss:10.711\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:12] Epoch 2\\Batch 3450\\ Train Loss:10.711\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:16] Epoch 2\\Batch 3500\\ Train Loss:10.708\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 19:17:19] Epoch 2\\Batch 3550\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:23] Epoch 2\\Batch 3600\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:28] Epoch 2\\Batch 3650\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:33] Epoch 2\\Batch 3700\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:38] Epoch 2\\Batch 3750\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:42] Epoch 2\\Batch 3800\\ Train Loss:10.708\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:47] Epoch 2\\Batch 3850\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:52] Epoch 2\\Batch 3900\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:17:57] Epoch 2\\Batch 3950\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:03] Epoch 2\\Batch 4000\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:07] Epoch 2\\Batch 4050\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:12] Epoch 2\\Batch 4100\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:17] Epoch 2\\Batch 4150\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:22] Epoch 2\\Batch 4200\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:27] Epoch 2\\Batch 4250\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:32] Epoch 2\\Batch 4300\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:37] Epoch 2\\Batch 4350\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:42] Epoch 2\\Batch 4400\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:47] Epoch 2\\Batch 4450\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:52] Epoch 2\\Batch 4500\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:18:57] Epoch 2\\Batch 4550\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:02] Epoch 2\\Batch 4600\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:07] Epoch 2\\Batch 4650\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:12] Epoch 2\\Batch 4700\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:17] Epoch 2\\Batch 4750\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:22] Epoch 2\\Batch 4800\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:27] Epoch 2\\Batch 4850\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:32] Epoch 2\\Batch 4900\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:37] Epoch 2\\Batch 4950\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:42] Epoch 2\\Batch 5000\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:46] Epoch 2\\Batch 5050\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:51] Epoch 2\\Batch 5100\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:19:56] Epoch 2\\Batch 5150\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:01] Epoch 2\\Batch 5200\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:06] Epoch 2\\Batch 5250\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:11] Epoch 2\\Batch 5300\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:16] Epoch 2\\Batch 5350\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:21] Epoch 2\\Batch 5400\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:26] Epoch 2\\Batch 5450\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:31] Epoch 2\\Batch 5500\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:36] Epoch 2\\Batch 5550\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:41] Epoch 2\\Batch 5600\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:46] Epoch 2\\Batch 5650\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:50] Epoch 2\\Batch 5700\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:20:55] Epoch 2\\Batch 5750\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:00] Epoch 2\\Batch 5800\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:05] Epoch 2\\Batch 5850\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:10] Epoch 2\\Batch 5900\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:15] Epoch 2\\Batch 5950\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:20] Epoch 2\\Batch 6000\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5588.000001, 'TP': 1411.0000009999999, 'FP': 1739.0000009999999}\n",
      "[2019/03/17 19:21:43] Epoch 2/ Validation Loss:10.246/ F1_score:0.278/ Precision:0.448/ Recall:0.202\n",
      "[2019/03/17 19:21:48] Epoch 2\\Batch 6050\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:53] Epoch 2\\Batch 6100\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:21:58] Epoch 2\\Batch 6150\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:03] Epoch 2\\Batch 6200\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:08] Epoch 2\\Batch 6250\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:13] Epoch 2\\Batch 6300\\ Train Loss:10.707\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:18] Epoch 2\\Batch 6350\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:22] Epoch 2\\Batch 6400\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:27] Epoch 2\\Batch 6450\\ Train Loss:10.706\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:32] Epoch 2\\Batch 6500\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:37] Epoch 2\\Batch 6550\\ Train Loss:10.705\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:42] Epoch 2\\Batch 6600\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:47] Epoch 2\\Batch 6650\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:52] Epoch 2\\Batch 6700\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:22:57] Epoch 2\\Batch 6750\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:02] Epoch 2\\Batch 6800\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:07] Epoch 2\\Batch 6850\\ Train Loss:10.704\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:12] Epoch 2\\Batch 6900\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:17] Epoch 2\\Batch 6950\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:21] Epoch 2\\Batch 7000\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:27] Epoch 2\\Batch 7050\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:32] Epoch 2\\Batch 7100\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:37] Epoch 2\\Batch 7150\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:41] Epoch 2\\Batch 7200\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:46] Epoch 2\\Batch 7250\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:51] Epoch 2\\Batch 7300\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:23:56] Epoch 2\\Batch 7350\\ Train Loss:10.703\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:01] Epoch 2\\Batch 7400\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:06] Epoch 2\\Batch 7450\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:10] Epoch 2\\Batch 7500\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:13] Epoch 2\\Batch 7550\\ Train Loss:10.702\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:16] Epoch 2\\Batch 7600\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:20] Epoch 2\\Batch 7650\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:23] Epoch 2\\Batch 7700\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:26] Epoch 2\\Batch 7750\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:29] Epoch 2\\Batch 7800\\ Train Loss:10.700\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:33] Epoch 2\\Batch 7850\\ Train Loss:10.701\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:38] Epoch 2\\Batch 7900\\ Train Loss:10.700\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:43] Epoch 2\\Batch 7950\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:48] Epoch 2\\Batch 8000\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:53] Epoch 2\\Batch 8050\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:24:58] Epoch 2\\Batch 8100\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:03] Epoch 2\\Batch 8150\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:08] Epoch 2\\Batch 8200\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:13] Epoch 2\\Batch 8250\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:18] Epoch 2\\Batch 8300\\ Train Loss:10.696\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 19:25:23] Epoch 2\\Batch 8350\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:28] Epoch 2\\Batch 8400\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:33] Epoch 2\\Batch 8450\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:38] Epoch 2\\Batch 8500\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:43] Epoch 2\\Batch 8550\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:48] Epoch 2\\Batch 8600\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:53] Epoch 2\\Batch 8650\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:25:58] Epoch 2\\Batch 8700\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:26:02] Epoch 2\\Batch 8750\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:26:07] Epoch 2\\Batch 8800\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:26:12] Epoch 2\\Batch 8850\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:26:17] Epoch 2\\Batch 8900\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:26:22] Epoch 2\\Batch 8950\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:26:27] Epoch 2\\Batch 9000\\ Train Loss:10.699\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5624.000001, 'TP': 1375.0000009999999, 'FP': 1759.0000009999999}\n",
      "[2019/03/17 19:26:50] Epoch 2/ Validation Loss:10.227/ F1_score:0.271/ Precision:0.439/ Recall:0.196\n",
      "[2019/03/17 19:26:55] Epoch 2\\Batch 9050\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:00] Epoch 2\\Batch 9100\\ Train Loss:10.699\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:05] Epoch 2\\Batch 9150\\ Train Loss:10.699\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:10] Epoch 2\\Batch 9200\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:15] Epoch 2\\Batch 9250\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:20] Epoch 2\\Batch 9300\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:25] Epoch 2\\Batch 9350\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:30] Epoch 2\\Batch 9400\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:35] Epoch 2\\Batch 9450\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:40] Epoch 2\\Batch 9500\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:44] Epoch 2\\Batch 9550\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:49] Epoch 2\\Batch 9600\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:54] Epoch 2\\Batch 9650\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:27:59] Epoch 2\\Batch 9700\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:04] Epoch 2\\Batch 9750\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:09] Epoch 2\\Batch 9800\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:14] Epoch 2\\Batch 9850\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:19] Epoch 2\\Batch 9900\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:24] Epoch 2\\Batch 9950\\ Train Loss:10.698\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:29] Epoch 2\\Batch 10000\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:34] Epoch 2\\Batch 10050\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:39] Epoch 2\\Batch 10100\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:44] Epoch 2\\Batch 10150\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:49] Epoch 2\\Batch 10200\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:54] Epoch 2\\Batch 10250\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:28:59] Epoch 2\\Batch 10300\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:04] Epoch 2\\Batch 10350\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:09] Epoch 2\\Batch 10400\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:14] Epoch 2\\Batch 10450\\ Train Loss:10.697\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:19] Epoch 2\\Batch 10500\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:24] Epoch 2\\Batch 10550\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:29] Epoch 2\\Batch 10600\\ Train Loss:10.695\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:34] Epoch 2\\Batch 10650\\ Train Loss:10.696\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:39] Epoch 2\\Batch 10700\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:44] Epoch 2\\Batch 10750\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:48] Epoch 2\\Batch 10800\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:53] Epoch 2\\Batch 10850\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:29:58] Epoch 2\\Batch 10900\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:03] Epoch 2\\Batch 10950\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:08] Epoch 2\\Batch 11000\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:13] Epoch 2\\Batch 11050\\ Train Loss:10.693\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:18] Epoch 2\\Batch 11100\\ Train Loss:10.694\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:23] Epoch 2\\Batch 11150\\ Train Loss:10.693\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:28] Epoch 2\\Batch 11200\\ Train Loss:10.692\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:33] Epoch 2\\Batch 11250\\ Train Loss:10.692\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:38] Epoch 2\\Batch 11300\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:42] Epoch 2\\Batch 11350\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:47] Epoch 2\\Batch 11400\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:52] Epoch 2\\Batch 11450\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:30:57] Epoch 2\\Batch 11500\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:02] Epoch 2\\Batch 11550\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:07] Epoch 2\\Batch 11600\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:12] Epoch 2\\Batch 11650\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:16] Epoch 2\\Batch 11700\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:20] Epoch 2\\Batch 11750\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:23] Epoch 2\\Batch 11800\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:26] Epoch 2\\Batch 11850\\ Train Loss:10.691\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:29] Epoch 2\\Batch 11900\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:33] Epoch 2\\Batch 11950\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:31:36] Epoch 2\\Batch 12000\\ Train Loss:10.689\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5586.000001, 'TP': 1413.0000009999999, 'FP': 1751.0000009999999}\n",
      "[2019/03/17 19:31:57] Epoch 2/ Validation Loss:10.201/ F1_score:0.278/ Precision:0.447/ Recall:0.202\n",
      "[2019/03/17 19:32:03] Epoch 2\\Batch 12050\\ Train Loss:10.689\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:08] Epoch 2\\Batch 12100\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:13] Epoch 2\\Batch 12150\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:18] Epoch 2\\Batch 12200\\ Train Loss:10.690\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:23] Epoch 2\\Batch 12250\\ Train Loss:10.688\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:28] Epoch 2\\Batch 12300\\ Train Loss:10.688\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:33] Epoch 2\\Batch 12350\\ Train Loss:10.688\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:38] Epoch 2\\Batch 12400\\ Train Loss:10.689\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:42] Epoch 2\\Batch 12450\\ Train Loss:10.688\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:47] Epoch 2\\Batch 12500\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:52] Epoch 2\\Batch 12550\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:32:57] Epoch 2\\Batch 12600\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:02] Epoch 2\\Batch 12650\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:07] Epoch 2\\Batch 12700\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:12] Epoch 2\\Batch 12750\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:17] Epoch 2\\Batch 12800\\ Train Loss:10.687\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:22] Epoch 2\\Batch 12850\\ Train Loss:10.686\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:27] Epoch 2\\Batch 12900\\ Train Loss:10.685\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:32] Epoch 2\\Batch 12950\\ Train Loss:10.685\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 19:33:37] Epoch 2\\Batch 13000\\ Train Loss:10.685\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:42] Epoch 2\\Batch 13050\\ Train Loss:10.685\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:47] Epoch 2\\Batch 13100\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:51] Epoch 2\\Batch 13150\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:33:56] Epoch 2\\Batch 13200\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:01] Epoch 2\\Batch 13250\\ Train Loss:10.685\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:06] Epoch 2\\Batch 13300\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:11] Epoch 2\\Batch 13350\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:16] Epoch 2\\Batch 13400\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:21] Epoch 2\\Batch 13450\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:26] Epoch 2\\Batch 13500\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:31] Epoch 2\\Batch 13550\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:36] Epoch 2\\Batch 13600\\ Train Loss:10.684\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:41] Epoch 2\\Batch 13650\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:46] Epoch 2\\Batch 13700\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:51] Epoch 2\\Batch 13750\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:34:56] Epoch 2\\Batch 13800\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:01] Epoch 2\\Batch 13850\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:06] Epoch 2\\Batch 13900\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:10] Epoch 2\\Batch 13950\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:15] Epoch 2\\Batch 14000\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:21] Epoch 2\\Batch 14050\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:25] Epoch 2\\Batch 14100\\ Train Loss:10.683\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:30] Epoch 2\\Batch 14150\\ Train Loss:10.682\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:35] Epoch 2\\Batch 14200\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:40] Epoch 2\\Batch 14250\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:45] Epoch 2\\Batch 14300\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:50] Epoch 2\\Batch 14350\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:35:55] Epoch 2\\Batch 14400\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:00] Epoch 2\\Batch 14450\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:05] Epoch 2\\Batch 14500\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:10] Epoch 2\\Batch 14550\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:15] Epoch 2\\Batch 14600\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:20] Epoch 2\\Batch 14650\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:25] Epoch 2\\Batch 14700\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:30] Epoch 2\\Batch 14750\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:35] Epoch 2\\Batch 14800\\ Train Loss:10.681\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:40] Epoch 2\\Batch 14850\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:45] Epoch 2\\Batch 14900\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:50] Epoch 2\\Batch 14950\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:36:55] Epoch 2\\Batch 15000\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5579.000001, 'TP': 1420.0000009999999, 'FP': 1718.0000009999999}\n",
      "[2019/03/17 19:37:18] Epoch 2/ Validation Loss:10.165/ F1_score:0.280/ Precision:0.453/ Recall:0.203\n",
      "[2019/03/17 19:37:23] Epoch 2\\Batch 15050\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:28] Epoch 2\\Batch 15100\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:33] Epoch 2\\Batch 15150\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:39] Epoch 2\\Batch 15200\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:43] Epoch 2\\Batch 15250\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:48] Epoch 2\\Batch 15300\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:53] Epoch 2\\Batch 15350\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:37:58] Epoch 2\\Batch 15400\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:03] Epoch 2\\Batch 15450\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:08] Epoch 2\\Batch 15500\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:13] Epoch 2\\Batch 15550\\ Train Loss:10.680\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:16] Epoch 2\\Batch 15600\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:20] Epoch 2\\Batch 15650\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:23] Epoch 2\\Batch 15700\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:27] Epoch 2\\Batch 15750\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:30] Epoch 2\\Batch 15800\\ Train Loss:10.679\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:33] Epoch 2\\Batch 15850\\ Train Loss:10.678\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:37] Epoch 2\\Batch 15900\\ Train Loss:10.677\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:41] Epoch 2\\Batch 15950\\ Train Loss:10.677\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:46] Epoch 2\\Batch 16000\\ Train Loss:10.677\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:51] Epoch 2\\Batch 16050\\ Train Loss:10.677\\ Learning rate:0.00030\n",
      "[2019/03/17 19:38:56] Epoch 2\\Batch 16100\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:01] Epoch 2\\Batch 16150\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:06] Epoch 2\\Batch 16200\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:11] Epoch 2\\Batch 16250\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:17] Epoch 2\\Batch 16300\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:22] Epoch 2\\Batch 16350\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:27] Epoch 2\\Batch 16400\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:32] Epoch 2\\Batch 16450\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:37] Epoch 2\\Batch 16500\\ Train Loss:10.676\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:42] Epoch 2\\Batch 16550\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:47] Epoch 2\\Batch 16600\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:52] Epoch 2\\Batch 16650\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:39:57] Epoch 2\\Batch 16700\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:02] Epoch 2\\Batch 16750\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:07] Epoch 2\\Batch 16800\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:11] Epoch 2\\Batch 16850\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:16] Epoch 2\\Batch 16900\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:21] Epoch 2\\Batch 16950\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:26] Epoch 2\\Batch 17000\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:31] Epoch 2\\Batch 17050\\ Train Loss:10.675\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:36] Epoch 2\\Batch 17100\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:41] Epoch 2\\Batch 17150\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:46] Epoch 2\\Batch 17200\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:51] Epoch 2\\Batch 17250\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:40:56] Epoch 2\\Batch 17300\\ Train Loss:10.674\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:01] Epoch 2\\Batch 17350\\ Train Loss:10.673\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:06] Epoch 2\\Batch 17400\\ Train Loss:10.673\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:11] Epoch 2\\Batch 17450\\ Train Loss:10.673\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:17] Epoch 2\\Batch 17500\\ Train Loss:10.673\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:22] Epoch 2\\Batch 17550\\ Train Loss:10.672\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:27] Epoch 2\\Batch 17600\\ Train Loss:10.671\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:32] Epoch 2\\Batch 17650\\ Train Loss:10.671\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:37] Epoch 2\\Batch 17700\\ Train Loss:10.671\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 19:41:42] Epoch 2\\Batch 17750\\ Train Loss:10.671\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:47] Epoch 2\\Batch 17800\\ Train Loss:10.671\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:53] Epoch 2\\Batch 17850\\ Train Loss:10.671\\ Learning rate:0.00030\n",
      "[2019/03/17 19:41:58] Epoch 2\\Batch 17900\\ Train Loss:10.670\\ Learning rate:0.00030\n",
      "[2019/03/17 19:42:03] Epoch 2\\Batch 17950\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "[2019/03/17 19:42:08] Epoch 2\\Batch 18000\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5573.000001, 'TP': 1426.0000009999999, 'FP': 1728.0000009999999}\n",
      "[2019/03/17 19:42:31] Epoch 2/ Validation Loss:10.174/ F1_score:0.281/ Precision:0.452/ Recall:0.204\n",
      "[2019/03/17 19:42:36] Epoch 2\\Batch 18050\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "[2019/03/17 19:42:42] Epoch 2\\Batch 18100\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "[2019/03/17 19:42:47] Epoch 2\\Batch 18150\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:42:52] Epoch 2\\Batch 18200\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:42:57] Epoch 2\\Batch 18250\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:02] Epoch 2\\Batch 18300\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:08] Epoch 2\\Batch 18350\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:13] Epoch 2\\Batch 18400\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:18] Epoch 2\\Batch 18450\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:23] Epoch 2\\Batch 18500\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:29] Epoch 2\\Batch 18550\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:34] Epoch 2\\Batch 18600\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:39] Epoch 2\\Batch 18650\\ Train Loss:10.669\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:44] Epoch 2\\Batch 18700\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:50] Epoch 2\\Batch 18750\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:43:55] Epoch 2\\Batch 18800\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:00] Epoch 2\\Batch 18850\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:06] Epoch 2\\Batch 18900\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:11] Epoch 2\\Batch 18950\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:17] Epoch 2\\Batch 19000\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:22] Epoch 2\\Batch 19050\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:27] Epoch 2\\Batch 19100\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:33] Epoch 2\\Batch 19150\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:38] Epoch 2\\Batch 19200\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:43] Epoch 2\\Batch 19250\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:49] Epoch 2\\Batch 19300\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:54] Epoch 2\\Batch 19350\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:44:59] Epoch 2\\Batch 19400\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:05] Epoch 2\\Batch 19450\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:10] Epoch 2\\Batch 19500\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:16] Epoch 2\\Batch 19550\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:21] Epoch 2\\Batch 19600\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:27] Epoch 2\\Batch 19650\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:32] Epoch 2\\Batch 19700\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:37] Epoch 2\\Batch 19750\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:42] Epoch 2\\Batch 19800\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:45] Epoch 2\\Batch 19850\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:49] Epoch 2\\Batch 19900\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:53] Epoch 2\\Batch 19950\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:45:56] Epoch 2\\Batch 20000\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:00] Epoch 2\\Batch 20050\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:04] Epoch 2\\Batch 20100\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:09] Epoch 2\\Batch 20150\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:14] Epoch 2\\Batch 20200\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:19] Epoch 2\\Batch 20250\\ Train Loss:10.668\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:25] Epoch 2\\Batch 20300\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:30] Epoch 2\\Batch 20350\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:36] Epoch 2\\Batch 20400\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:41] Epoch 2\\Batch 20450\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:46] Epoch 2\\Batch 20500\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:51] Epoch 2\\Batch 20550\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:46:57] Epoch 2\\Batch 20600\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:02] Epoch 2\\Batch 20650\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:07] Epoch 2\\Batch 20700\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:13] Epoch 2\\Batch 20750\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:18] Epoch 2\\Batch 20800\\ Train Loss:10.667\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:23] Epoch 2\\Batch 20850\\ Train Loss:10.666\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:28] Epoch 2\\Batch 20900\\ Train Loss:10.665\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:33] Epoch 2\\Batch 20950\\ Train Loss:10.665\\ Learning rate:0.00030\n",
      "[2019/03/17 19:47:38] Epoch 2\\Batch 21000\\ Train Loss:10.665\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5549.000001, 'TP': 1450.0000009999999, 'FP': 1702.0000009999999}\n",
      "[2019/03/17 19:48:01] Epoch 2/ Validation Loss:10.153/ F1_score:0.286/ Precision:0.460/ Recall:0.207\n",
      "[2019/03/17 19:48:07] Epoch 2\\Batch 21050\\ Train Loss:10.665\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:12] Epoch 2\\Batch 21100\\ Train Loss:10.664\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:17] Epoch 2\\Batch 21150\\ Train Loss:10.664\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:23] Epoch 2\\Batch 21200\\ Train Loss:10.664\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:28] Epoch 2\\Batch 21250\\ Train Loss:10.664\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:33] Epoch 2\\Batch 21300\\ Train Loss:10.664\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:38] Epoch 2\\Batch 21350\\ Train Loss:10.664\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:44] Epoch 2\\Batch 21400\\ Train Loss:10.663\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:49] Epoch 2\\Batch 21450\\ Train Loss:10.663\\ Learning rate:0.00030\n",
      "[2019/03/17 19:48:54] Epoch 2\\Batch 21500\\ Train Loss:10.662\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:00] Epoch 2\\Batch 21550\\ Train Loss:10.662\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:05] Epoch 2\\Batch 21600\\ Train Loss:10.662\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:10] Epoch 2\\Batch 21650\\ Train Loss:10.662\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:15] Epoch 2\\Batch 21700\\ Train Loss:10.661\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:21] Epoch 2\\Batch 21750\\ Train Loss:10.661\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:26] Epoch 2\\Batch 21800\\ Train Loss:10.661\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:31] Epoch 2\\Batch 21850\\ Train Loss:10.661\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:37] Epoch 2\\Batch 21900\\ Train Loss:10.661\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:42] Epoch 2\\Batch 21950\\ Train Loss:10.661\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:47] Epoch 2\\Batch 22000\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:53] Epoch 2\\Batch 22050\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:49:58] Epoch 2\\Batch 22100\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:03] Epoch 2\\Batch 22150\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:08] Epoch 2\\Batch 22200\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:13] Epoch 2\\Batch 22250\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:19] Epoch 2\\Batch 22300\\ Train Loss:10.660\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 19:50:24] Epoch 2\\Batch 22350\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:29] Epoch 2\\Batch 22400\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:34] Epoch 2\\Batch 22450\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:39] Epoch 2\\Batch 22500\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:44] Epoch 2\\Batch 22550\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:49] Epoch 2\\Batch 22600\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:50:55] Epoch 2\\Batch 22650\\ Train Loss:10.660\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:00] Epoch 2\\Batch 22700\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:05] Epoch 2\\Batch 22750\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:10] Epoch 2\\Batch 22800\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:15] Epoch 2\\Batch 22850\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:21] Epoch 2\\Batch 22900\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:26] Epoch 2\\Batch 22950\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:31] Epoch 2\\Batch 23000\\ Train Loss:10.658\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:36] Epoch 2\\Batch 23050\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:41] Epoch 2\\Batch 23100\\ Train Loss:10.659\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:46] Epoch 2\\Batch 23150\\ Train Loss:10.658\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:51] Epoch 2\\Batch 23200\\ Train Loss:10.658\\ Learning rate:0.00030\n",
      "[2019/03/17 19:51:56] Epoch 2\\Batch 23250\\ Train Loss:10.658\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:01] Epoch 2\\Batch 23300\\ Train Loss:10.658\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:06] Epoch 2\\Batch 23350\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:11] Epoch 2\\Batch 23400\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:16] Epoch 2\\Batch 23450\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:21] Epoch 2\\Batch 23500\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:26] Epoch 2\\Batch 23550\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:31] Epoch 2\\Batch 23600\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:36] Epoch 2\\Batch 23650\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:41] Epoch 2\\Batch 23700\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:47] Epoch 2\\Batch 23750\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:52] Epoch 2\\Batch 23800\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:52:57] Epoch 2\\Batch 23850\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:01] Epoch 2\\Batch 23900\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:04] Epoch 2\\Batch 23950\\ Train Loss:10.658\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:08] Epoch 2\\Batch 24000\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5575.000001, 'TP': 1424.0000009999999, 'FP': 1731.0000009999999}\n",
      "[2019/03/17 19:53:26] Epoch 2/ Validation Loss:10.135/ F1_score:0.280/ Precision:0.451/ Recall:0.203\n",
      "[2019/03/17 19:53:32] Epoch 2\\Batch 24050\\ Train Loss:10.657\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:36] Epoch 2\\Batch 24100\\ Train Loss:10.656\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:41] Epoch 2\\Batch 24150\\ Train Loss:10.656\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:47] Epoch 2\\Batch 24200\\ Train Loss:10.656\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:52] Epoch 2\\Batch 24250\\ Train Loss:10.655\\ Learning rate:0.00030\n",
      "[2019/03/17 19:53:57] Epoch 2\\Batch 24300\\ Train Loss:10.655\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:02] Epoch 2\\Batch 24350\\ Train Loss:10.655\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:07] Epoch 2\\Batch 24400\\ Train Loss:10.654\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:12] Epoch 2\\Batch 24450\\ Train Loss:10.654\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:17] Epoch 2\\Batch 24500\\ Train Loss:10.654\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:22] Epoch 2\\Batch 24550\\ Train Loss:10.653\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:27] Epoch 2\\Batch 24600\\ Train Loss:10.653\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:32] Epoch 2\\Batch 24650\\ Train Loss:10.653\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:37] Epoch 2\\Batch 24700\\ Train Loss:10.652\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:43] Epoch 2\\Batch 24750\\ Train Loss:10.652\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:48] Epoch 2\\Batch 24800\\ Train Loss:10.652\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:53] Epoch 2\\Batch 24850\\ Train Loss:10.652\\ Learning rate:0.00030\n",
      "[2019/03/17 19:54:58] Epoch 2\\Batch 24900\\ Train Loss:10.652\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:03] Epoch 2\\Batch 24950\\ Train Loss:10.651\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:08] Epoch 2\\Batch 25000\\ Train Loss:10.651\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:13] Epoch 2\\Batch 25050\\ Train Loss:10.651\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:18] Epoch 2\\Batch 25100\\ Train Loss:10.651\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:23] Epoch 2\\Batch 25150\\ Train Loss:10.651\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:28] Epoch 2\\Batch 25200\\ Train Loss:10.650\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:33] Epoch 2\\Batch 25250\\ Train Loss:10.650\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:38] Epoch 2\\Batch 25300\\ Train Loss:10.650\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:43] Epoch 2\\Batch 25350\\ Train Loss:10.650\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:48] Epoch 2\\Batch 25400\\ Train Loss:10.650\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:53] Epoch 2\\Batch 25450\\ Train Loss:10.649\\ Learning rate:0.00030\n",
      "[2019/03/17 19:55:58] Epoch 2\\Batch 25500\\ Train Loss:10.649\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:03] Epoch 2\\Batch 25550\\ Train Loss:10.649\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:09] Epoch 2\\Batch 25600\\ Train Loss:10.649\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:14] Epoch 2\\Batch 25650\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:19] Epoch 2\\Batch 25700\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:24] Epoch 2\\Batch 25750\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:29] Epoch 2\\Batch 25800\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:34] Epoch 2\\Batch 25850\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:39] Epoch 2\\Batch 25900\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:44] Epoch 2\\Batch 25950\\ Train Loss:10.648\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:49] Epoch 2\\Batch 26000\\ Train Loss:10.647\\ Learning rate:0.00030\n",
      "[2019/03/17 19:56:55] Epoch 2\\Batch 26050\\ Train Loss:10.647\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:00] Epoch 2\\Batch 26100\\ Train Loss:10.647\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:05] Epoch 2\\Batch 26150\\ Train Loss:10.647\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:10] Epoch 2\\Batch 26200\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:15] Epoch 2\\Batch 26250\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:20] Epoch 2\\Batch 26300\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:25] Epoch 2\\Batch 26350\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:30] Epoch 2\\Batch 26400\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:35] Epoch 2\\Batch 26450\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:40] Epoch 2\\Batch 26500\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:45] Epoch 2\\Batch 26550\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:50] Epoch 2\\Batch 26600\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:57:55] Epoch 2\\Batch 26650\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:00] Epoch 2\\Batch 26700\\ Train Loss:10.646\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:05] Epoch 2\\Batch 26750\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:10] Epoch 2\\Batch 26800\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:15] Epoch 2\\Batch 26850\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:20] Epoch 2\\Batch 26900\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:25] Epoch 2\\Batch 26950\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:58:31] Epoch 2\\Batch 27000\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5558.000001, 'TP': 1441.0000009999999, 'FP': 1717.0000009999999}\n",
      "[2019/03/17 19:58:53] Epoch 2/ Validation Loss:10.133/ F1_score:0.284/ Precision:0.456/ Recall:0.206\n",
      "[2019/03/17 19:58:59] Epoch 2\\Batch 27050\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:04] Epoch 2\\Batch 27100\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:09] Epoch 2\\Batch 27150\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:14] Epoch 2\\Batch 27200\\ Train Loss:10.645\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:19] Epoch 2\\Batch 27250\\ Train Loss:10.644\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:24] Epoch 2\\Batch 27300\\ Train Loss:10.644\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:29] Epoch 2\\Batch 27350\\ Train Loss:10.644\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:34] Epoch 2\\Batch 27400\\ Train Loss:10.644\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:39] Epoch 2\\Batch 27450\\ Train Loss:10.644\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:44] Epoch 2\\Batch 27500\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:49] Epoch 2\\Batch 27550\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:54] Epoch 2\\Batch 27600\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 19:59:59] Epoch 2\\Batch 27650\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:04] Epoch 2\\Batch 27700\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:07] Epoch 2\\Batch 27750\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:10] Epoch 2\\Batch 27800\\ Train Loss:10.644\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:14] Epoch 2\\Batch 27850\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:17] Epoch 2\\Batch 27900\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:21] Epoch 2\\Batch 27950\\ Train Loss:10.643\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:24] Epoch 2\\Batch 28000\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:28] Epoch 2\\Batch 28050\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:32] Epoch 2\\Batch 28100\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:37] Epoch 2\\Batch 28150\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:42] Epoch 2\\Batch 28200\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:47] Epoch 2\\Batch 28250\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:52] Epoch 2\\Batch 28300\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:00:57] Epoch 2\\Batch 28350\\ Train Loss:10.642\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:02] Epoch 2\\Batch 28400\\ Train Loss:10.641\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:08] Epoch 2\\Batch 28450\\ Train Loss:10.641\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:12] Epoch 2\\Batch 28500\\ Train Loss:10.641\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:18] Epoch 2\\Batch 28550\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:23] Epoch 2\\Batch 28600\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:28] Epoch 2\\Batch 28650\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:33] Epoch 2\\Batch 28700\\ Train Loss:10.641\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:38] Epoch 2\\Batch 28750\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:43] Epoch 2\\Batch 28800\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:48] Epoch 2\\Batch 28850\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:53] Epoch 2\\Batch 28900\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:01:58] Epoch 2\\Batch 28950\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:03] Epoch 2\\Batch 29000\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:08] Epoch 2\\Batch 29050\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:13] Epoch 2\\Batch 29100\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:19] Epoch 2\\Batch 29150\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:24] Epoch 2\\Batch 29200\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:29] Epoch 2\\Batch 29250\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:34] Epoch 2\\Batch 29300\\ Train Loss:10.640\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:39] Epoch 2\\Batch 29350\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:44] Epoch 2\\Batch 29400\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:49] Epoch 2\\Batch 29450\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:54] Epoch 2\\Batch 29500\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:02:59] Epoch 2\\Batch 29550\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:04] Epoch 2\\Batch 29600\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:09] Epoch 2\\Batch 29650\\ Train Loss:10.639\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:14] Epoch 2\\Batch 29700\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:19] Epoch 2\\Batch 29750\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:24] Epoch 2\\Batch 29800\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:29] Epoch 2\\Batch 29850\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:34] Epoch 2\\Batch 29900\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:40] Epoch 2\\Batch 29950\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:03:45] Epoch 2\\Batch 30000\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5550.000001, 'TP': 1449.0000009999999, 'FP': 1712.0000009999999}\n",
      "[2019/03/17 20:04:07] Epoch 2/ Validation Loss:10.125/ F1_score:0.285/ Precision:0.458/ Recall:0.207\n",
      "[2019/03/17 20:04:12] Epoch 2\\Batch 30050\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:18] Epoch 2\\Batch 30100\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:22] Epoch 2\\Batch 30150\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:27] Epoch 2\\Batch 30200\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:33] Epoch 2\\Batch 30250\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:37] Epoch 2\\Batch 30300\\ Train Loss:10.638\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:43] Epoch 2\\Batch 30350\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:48] Epoch 2\\Batch 30400\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:53] Epoch 2\\Batch 30450\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:04:58] Epoch 2\\Batch 30500\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:03] Epoch 2\\Batch 30550\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:08] Epoch 2\\Batch 30600\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:13] Epoch 2\\Batch 30650\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:19] Epoch 2\\Batch 30700\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:24] Epoch 2\\Batch 30750\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:29] Epoch 2\\Batch 30800\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:34] Epoch 2\\Batch 30850\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:39] Epoch 2\\Batch 30900\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:44] Epoch 2\\Batch 30950\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:49] Epoch 2\\Batch 31000\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:54] Epoch 2\\Batch 31050\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:05:59] Epoch 2\\Batch 31100\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:04] Epoch 2\\Batch 31150\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:09] Epoch 2\\Batch 31200\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:14] Epoch 2\\Batch 31250\\ Train Loss:10.637\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:19] Epoch 2\\Batch 31300\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:24] Epoch 2\\Batch 31350\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:29] Epoch 2\\Batch 31400\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:34] Epoch 2\\Batch 31450\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:39] Epoch 2\\Batch 31500\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:44] Epoch 2\\Batch 31550\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:49] Epoch 2\\Batch 31600\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:06:54] Epoch 2\\Batch 31650\\ Train Loss:10.635\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 20:06:59] Epoch 2\\Batch 31700\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:04] Epoch 2\\Batch 31750\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:09] Epoch 2\\Batch 31800\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:14] Epoch 2\\Batch 31850\\ Train Loss:10.636\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:17] Epoch 2\\Batch 31900\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:21] Epoch 2\\Batch 31950\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:24] Epoch 2\\Batch 32000\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:28] Epoch 2\\Batch 32050\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:31] Epoch 2\\Batch 32100\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:35] Epoch 2\\Batch 32150\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:38] Epoch 2\\Batch 32200\\ Train Loss:10.635\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:42] Epoch 2\\Batch 32250\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:47] Epoch 2\\Batch 32300\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:52] Epoch 2\\Batch 32350\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:07:57] Epoch 2\\Batch 32400\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:02] Epoch 2\\Batch 32450\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:07] Epoch 2\\Batch 32500\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:13] Epoch 2\\Batch 32550\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:18] Epoch 2\\Batch 32600\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:23] Epoch 2\\Batch 32650\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:28] Epoch 2\\Batch 32700\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:33] Epoch 2\\Batch 32750\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:38] Epoch 2\\Batch 32800\\ Train Loss:10.634\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:43] Epoch 2\\Batch 32850\\ Train Loss:10.633\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:48] Epoch 2\\Batch 32900\\ Train Loss:10.633\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:53] Epoch 2\\Batch 32950\\ Train Loss:10.633\\ Learning rate:0.00030\n",
      "[2019/03/17 20:08:58] Epoch 2\\Batch 33000\\ Train Loss:10.633\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5544.000001, 'TP': 1455.0000009999999, 'FP': 1714.0000009999999}\n",
      "[2019/03/17 20:09:20] Epoch 2/ Validation Loss:10.113/ F1_score:0.286/ Precision:0.459/ Recall:0.208\n",
      "[2019/03/17 20:09:26] Epoch 2\\Batch 33050\\ Train Loss:10.633\\ Learning rate:0.00030\n",
      "[2019/03/17 20:09:31] Epoch 2\\Batch 33100\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:09:36] Epoch 2\\Batch 33150\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:09:40] Epoch 2\\Batch 33200\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:09:46] Epoch 2\\Batch 33250\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:09:50] Epoch 2\\Batch 33300\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:09:55] Epoch 2\\Batch 33350\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:00] Epoch 2\\Batch 33400\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:05] Epoch 2\\Batch 33450\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:10] Epoch 2\\Batch 33500\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:15] Epoch 2\\Batch 33550\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:20] Epoch 2\\Batch 33600\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:25] Epoch 2\\Batch 33650\\ Train Loss:10.632\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:30] Epoch 2\\Batch 33700\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:35] Epoch 2\\Batch 33750\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:40] Epoch 2\\Batch 33800\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:46] Epoch 2\\Batch 33850\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:50] Epoch 2\\Batch 33900\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:10:55] Epoch 2\\Batch 33950\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:01] Epoch 2\\Batch 34000\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:06] Epoch 2\\Batch 34050\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:11] Epoch 2\\Batch 34100\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:16] Epoch 2\\Batch 34150\\ Train Loss:10.631\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:21] Epoch 2\\Batch 34200\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:26] Epoch 2\\Batch 34250\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:32] Epoch 2\\Batch 34300\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:37] Epoch 2\\Batch 34350\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:42] Epoch 2\\Batch 34400\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:47] Epoch 2\\Batch 34450\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:52] Epoch 2\\Batch 34500\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:11:57] Epoch 2\\Batch 34550\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:02] Epoch 2\\Batch 34600\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:07] Epoch 2\\Batch 34650\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:12] Epoch 2\\Batch 34700\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:17] Epoch 2\\Batch 34750\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:22] Epoch 2\\Batch 34800\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:27] Epoch 2\\Batch 34850\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:32] Epoch 2\\Batch 34900\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:37] Epoch 2\\Batch 34950\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:42] Epoch 2\\Batch 35000\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:47] Epoch 2\\Batch 35050\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:52] Epoch 2\\Batch 35100\\ Train Loss:10.630\\ Learning rate:0.00030\n",
      "[2019/03/17 20:12:57] Epoch 2\\Batch 35150\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:02] Epoch 2\\Batch 35200\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:07] Epoch 2\\Batch 35250\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:13] Epoch 2\\Batch 35300\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:18] Epoch 2\\Batch 35350\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:23] Epoch 2\\Batch 35400\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:28] Epoch 2\\Batch 35450\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:33] Epoch 2\\Batch 35500\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:38] Epoch 2\\Batch 35550\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:43] Epoch 2\\Batch 35600\\ Train Loss:10.629\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:48] Epoch 2\\Batch 35650\\ Train Loss:10.628\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:53] Epoch 2\\Batch 35700\\ Train Loss:10.628\\ Learning rate:0.00030\n",
      "[2019/03/17 20:13:58] Epoch 2\\Batch 35750\\ Train Loss:10.628\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:03] Epoch 2\\Batch 35800\\ Train Loss:10.628\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:08] Epoch 2\\Batch 35850\\ Train Loss:10.628\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:13] Epoch 2\\Batch 35900\\ Train Loss:10.628\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:18] Epoch 2\\Batch 35950\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:23] Epoch 2\\Batch 36000\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5547.000001, 'TP': 1452.0000009999999, 'FP': 1706.0000009999999}\n",
      "[2019/03/17 20:14:40] Epoch 2/ Validation Loss:10.111/ F1_score:0.286/ Precision:0.460/ Recall:0.207\n",
      "[2019/03/17 20:14:44] Epoch 2\\Batch 36050\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:49] Epoch 2\\Batch 36100\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:54] Epoch 2\\Batch 36150\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "[2019/03/17 20:14:59] Epoch 2\\Batch 36200\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:04] Epoch 2\\Batch 36250\\ Train Loss:10.627\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 20:15:09] Epoch 2\\Batch 36300\\ Train Loss:10.627\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:14] Epoch 2\\Batch 36350\\ Train Loss:10.626\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:19] Epoch 2\\Batch 36400\\ Train Loss:10.626\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:24] Epoch 2\\Batch 36450\\ Train Loss:10.626\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:29] Epoch 2\\Batch 36500\\ Train Loss:10.626\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:34] Epoch 2\\Batch 36550\\ Train Loss:10.626\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:39] Epoch 2\\Batch 36600\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:44] Epoch 2\\Batch 36650\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:49] Epoch 2\\Batch 36700\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:54] Epoch 2\\Batch 36750\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:15:59] Epoch 2\\Batch 36800\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:04] Epoch 2\\Batch 36850\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:09] Epoch 2\\Batch 36900\\ Train Loss:10.625\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:14] Epoch 2\\Batch 36950\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:19] Epoch 2\\Batch 37000\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:24] Epoch 2\\Batch 37050\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:29] Epoch 2\\Batch 37100\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:34] Epoch 2\\Batch 37150\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:39] Epoch 2\\Batch 37200\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:44] Epoch 2\\Batch 37250\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:49] Epoch 2\\Batch 37300\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:54] Epoch 2\\Batch 37350\\ Train Loss:10.624\\ Learning rate:0.00030\n",
      "[2019/03/17 20:16:59] Epoch 2\\Batch 37400\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:04] Epoch 2\\Batch 37450\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:09] Epoch 2\\Batch 37500\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:14] Epoch 2\\Batch 37550\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:19] Epoch 2\\Batch 37600\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:24] Epoch 2\\Batch 37650\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:29] Epoch 2\\Batch 37700\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:33] Epoch 2\\Batch 37750\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:38] Epoch 2\\Batch 37800\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:44] Epoch 2\\Batch 37850\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:48] Epoch 2\\Batch 37900\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:53] Epoch 2\\Batch 37950\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:17:58] Epoch 2\\Batch 38000\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:03] Epoch 2\\Batch 38050\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:08] Epoch 2\\Batch 38100\\ Train Loss:10.623\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:13] Epoch 2\\Batch 38150\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:18] Epoch 2\\Batch 38200\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:23] Epoch 2\\Batch 38250\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:28] Epoch 2\\Batch 38300\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:33] Epoch 2\\Batch 38350\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:38] Epoch 2\\Batch 38400\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:43] Epoch 2\\Batch 38450\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:48] Epoch 2\\Batch 38500\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:53] Epoch 2\\Batch 38550\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:18:58] Epoch 2\\Batch 38600\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:03] Epoch 2\\Batch 38650\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:08] Epoch 2\\Batch 38700\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:13] Epoch 2\\Batch 38750\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:18] Epoch 2\\Batch 38800\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:23] Epoch 2\\Batch 38850\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:29] Epoch 2\\Batch 38900\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:34] Epoch 2\\Batch 38950\\ Train Loss:10.622\\ Learning rate:0.00030\n",
      "[2019/03/17 20:19:39] Epoch 2\\Batch 39000\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5532.000001, 'TP': 1467.0000009999999, 'FP': 1726.0000009999999}\n",
      "[2019/03/17 20:20:01] Epoch 2/ Validation Loss:10.133/ F1_score:0.288/ Precision:0.459/ Recall:0.210\n",
      "[2019/03/17 20:20:07] Epoch 2\\Batch 39050\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:12] Epoch 2\\Batch 39100\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:17] Epoch 2\\Batch 39150\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:22] Epoch 2\\Batch 39200\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:27] Epoch 2\\Batch 39250\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:32] Epoch 2\\Batch 39300\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:37] Epoch 2\\Batch 39350\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:42] Epoch 2\\Batch 39400\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:47] Epoch 2\\Batch 39450\\ Train Loss:10.621\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:52] Epoch 2\\Batch 39500\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:20:57] Epoch 2\\Batch 39550\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:03] Epoch 2\\Batch 39600\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:08] Epoch 2\\Batch 39650\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:13] Epoch 2\\Batch 39700\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:18] Epoch 2\\Batch 39750\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:23] Epoch 2\\Batch 39800\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:28] Epoch 2\\Batch 39850\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:32] Epoch 2\\Batch 39900\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:35] Epoch 2\\Batch 39950\\ Train Loss:10.620\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:39] Epoch 2\\Batch 40000\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:42] Epoch 2\\Batch 40050\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:46] Epoch 2\\Batch 40100\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:49] Epoch 2\\Batch 40150\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:53] Epoch 2\\Batch 40200\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:21:57] Epoch 2\\Batch 40250\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:02] Epoch 2\\Batch 40300\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:07] Epoch 2\\Batch 40350\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:12] Epoch 2\\Batch 40400\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:18] Epoch 2\\Batch 40450\\ Train Loss:10.619\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:22] Epoch 2\\Batch 40500\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:28] Epoch 2\\Batch 40550\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:33] Epoch 2\\Batch 40600\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:38] Epoch 2\\Batch 40650\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:43] Epoch 2\\Batch 40700\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:48] Epoch 2\\Batch 40750\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:53] Epoch 2\\Batch 40800\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:22:58] Epoch 2\\Batch 40850\\ Train Loss:10.618\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:03] Epoch 2\\Batch 40900\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:08] Epoch 2\\Batch 40950\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:13] Epoch 2\\Batch 41000\\ Train Loss:10.617\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 20:23:18] Epoch 2\\Batch 41050\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:23] Epoch 2\\Batch 41100\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:27] Epoch 2\\Batch 41150\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:33] Epoch 2\\Batch 41200\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:38] Epoch 2\\Batch 41250\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:42] Epoch 2\\Batch 41300\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:47] Epoch 2\\Batch 41350\\ Train Loss:10.617\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:52] Epoch 2\\Batch 41400\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:23:57] Epoch 2\\Batch 41450\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:02] Epoch 2\\Batch 41500\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:08] Epoch 2\\Batch 41550\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:13] Epoch 2\\Batch 41600\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:18] Epoch 2\\Batch 41650\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:23] Epoch 2\\Batch 41700\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:27] Epoch 2\\Batch 41750\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:32] Epoch 2\\Batch 41800\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:37] Epoch 2\\Batch 41850\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:43] Epoch 2\\Batch 41900\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:48] Epoch 2\\Batch 41950\\ Train Loss:10.616\\ Learning rate:0.00030\n",
      "[2019/03/17 20:24:53] Epoch 2\\Batch 42000\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5547.000001, 'TP': 1452.0000009999999, 'FP': 1697.0000009999999}\n",
      "[2019/03/17 20:25:15] Epoch 2/ Validation Loss:10.103/ F1_score:0.286/ Precision:0.461/ Recall:0.207\n",
      "[2019/03/17 20:25:20] Epoch 2\\Batch 42050\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:25] Epoch 2\\Batch 42100\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:30] Epoch 2\\Batch 42150\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:35] Epoch 2\\Batch 42200\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:41] Epoch 2\\Batch 42250\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:46] Epoch 2\\Batch 42300\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:51] Epoch 2\\Batch 42350\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:25:56] Epoch 2\\Batch 42400\\ Train Loss:10.615\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:01] Epoch 2\\Batch 42450\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:06] Epoch 2\\Batch 42500\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:11] Epoch 2\\Batch 42550\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:17] Epoch 2\\Batch 42600\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:22] Epoch 2\\Batch 42650\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:27] Epoch 2\\Batch 42700\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:33] Epoch 2\\Batch 42750\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:38] Epoch 2\\Batch 42800\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:43] Epoch 2\\Batch 42850\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:49] Epoch 2\\Batch 42900\\ Train Loss:10.614\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:54] Epoch 2\\Batch 42950\\ Train Loss:10.613\\ Learning rate:0.00030\n",
      "[2019/03/17 20:26:59] Epoch 2\\Batch 43000\\ Train Loss:10.613\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:05] Epoch 2\\Batch 43050\\ Train Loss:10.613\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:09] Epoch 2\\Batch 43100\\ Train Loss:10.613\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:15] Epoch 2\\Batch 43150\\ Train Loss:10.613\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:20] Epoch 2\\Batch 43200\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:25] Epoch 2\\Batch 43250\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:30] Epoch 2\\Batch 43300\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:36] Epoch 2\\Batch 43350\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:41] Epoch 2\\Batch 43400\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:46] Epoch 2\\Batch 43450\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:51] Epoch 2\\Batch 43500\\ Train Loss:10.612\\ Learning rate:0.00030\n",
      "[2019/03/17 20:27:56] Epoch 2\\Batch 43550\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:02] Epoch 2\\Batch 43600\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:07] Epoch 2\\Batch 43650\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:12] Epoch 2\\Batch 43700\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:17] Epoch 2\\Batch 43750\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:22] Epoch 2\\Batch 43800\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:27] Epoch 2\\Batch 43850\\ Train Loss:10.611\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:33] Epoch 2\\Batch 43900\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:38] Epoch 2\\Batch 43950\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:43] Epoch 2\\Batch 44000\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:48] Epoch 2\\Batch 44050\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:51] Epoch 2\\Batch 44100\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:55] Epoch 2\\Batch 44150\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:28:58] Epoch 2\\Batch 44200\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:02] Epoch 2\\Batch 44250\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:05] Epoch 2\\Batch 44300\\ Train Loss:10.610\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:09] Epoch 2\\Batch 44350\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:12] Epoch 2\\Batch 44400\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:17] Epoch 2\\Batch 44450\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:23] Epoch 2\\Batch 44500\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:28] Epoch 2\\Batch 44550\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:33] Epoch 2\\Batch 44600\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:38] Epoch 2\\Batch 44650\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:44] Epoch 2\\Batch 44700\\ Train Loss:10.609\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:49] Epoch 2\\Batch 44750\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:54] Epoch 2\\Batch 44800\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:29:59] Epoch 2\\Batch 44850\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:30:05] Epoch 2\\Batch 44900\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:30:10] Epoch 2\\Batch 44950\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:30:15] Epoch 2\\Batch 45000\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5548.000001, 'TP': 1451.0000009999999, 'FP': 1715.0000009999999}\n",
      "[2019/03/17 20:30:38] Epoch 2/ Validation Loss:10.104/ F1_score:0.285/ Precision:0.458/ Recall:0.207\n",
      "[2019/03/17 20:30:43] Epoch 2\\Batch 45050\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:30:48] Epoch 2\\Batch 45100\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:30:53] Epoch 2\\Batch 45150\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:30:58] Epoch 2\\Batch 45200\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:03] Epoch 2\\Batch 45250\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:09] Epoch 2\\Batch 45300\\ Train Loss:10.607\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:14] Epoch 2\\Batch 45350\\ Train Loss:10.607\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:19] Epoch 2\\Batch 45400\\ Train Loss:10.607\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:24] Epoch 2\\Batch 45450\\ Train Loss:10.607\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:29] Epoch 2\\Batch 45500\\ Train Loss:10.607\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:34] Epoch 2\\Batch 45550\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:39] Epoch 2\\Batch 45600\\ Train Loss:10.606\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 20:31:44] Epoch 2\\Batch 45650\\ Train Loss:10.607\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:49] Epoch 2\\Batch 45700\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:31:54] Epoch 2\\Batch 45750\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:00] Epoch 2\\Batch 45800\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:05] Epoch 2\\Batch 45850\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:10] Epoch 2\\Batch 45900\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:15] Epoch 2\\Batch 45950\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:20] Epoch 2\\Batch 46000\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:25] Epoch 2\\Batch 46050\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:30] Epoch 2\\Batch 46100\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:35] Epoch 2\\Batch 46150\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:40] Epoch 2\\Batch 46200\\ Train Loss:10.606\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:51] Epoch 3\\Batch 50\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 20:32:56] Epoch 3\\Batch 100\\ Train Loss:10.576\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:01] Epoch 3\\Batch 150\\ Train Loss:10.608\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:06] Epoch 3\\Batch 200\\ Train Loss:10.575\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:11] Epoch 3\\Batch 250\\ Train Loss:10.570\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:16] Epoch 3\\Batch 300\\ Train Loss:10.578\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:21] Epoch 3\\Batch 350\\ Train Loss:10.577\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:26] Epoch 3\\Batch 400\\ Train Loss:10.577\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:32] Epoch 3\\Batch 450\\ Train Loss:10.565\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:37] Epoch 3\\Batch 500\\ Train Loss:10.553\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:42] Epoch 3\\Batch 550\\ Train Loss:10.537\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:47] Epoch 3\\Batch 600\\ Train Loss:10.537\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:52] Epoch 3\\Batch 650\\ Train Loss:10.533\\ Learning rate:0.00030\n",
      "[2019/03/17 20:33:57] Epoch 3\\Batch 700\\ Train Loss:10.538\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:03] Epoch 3\\Batch 750\\ Train Loss:10.539\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:08] Epoch 3\\Batch 800\\ Train Loss:10.533\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:13] Epoch 3\\Batch 850\\ Train Loss:10.540\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:18] Epoch 3\\Batch 900\\ Train Loss:10.547\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:23] Epoch 3\\Batch 950\\ Train Loss:10.538\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:29] Epoch 3\\Batch 1000\\ Train Loss:10.530\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:34] Epoch 3\\Batch 1050\\ Train Loss:10.530\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:39] Epoch 3\\Batch 1100\\ Train Loss:10.525\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:44] Epoch 3\\Batch 1150\\ Train Loss:10.531\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:50] Epoch 3\\Batch 1200\\ Train Loss:10.526\\ Learning rate:0.00030\n",
      "[2019/03/17 20:34:55] Epoch 3\\Batch 1250\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:01] Epoch 3\\Batch 1300\\ Train Loss:10.523\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:06] Epoch 3\\Batch 1350\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:11] Epoch 3\\Batch 1400\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:17] Epoch 3\\Batch 1450\\ Train Loss:10.522\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:22] Epoch 3\\Batch 1500\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:27] Epoch 3\\Batch 1550\\ Train Loss:10.519\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:33] Epoch 3\\Batch 1600\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:38] Epoch 3\\Batch 1650\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:43] Epoch 3\\Batch 1700\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:48] Epoch 3\\Batch 1750\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:54] Epoch 3\\Batch 1800\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:35:59] Epoch 3\\Batch 1850\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:05] Epoch 3\\Batch 1900\\ Train Loss:10.519\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:10] Epoch 3\\Batch 1950\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:15] Epoch 3\\Batch 2000\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:19] Epoch 3\\Batch 2050\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:22] Epoch 3\\Batch 2100\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:26] Epoch 3\\Batch 2150\\ Train Loss:10.523\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:29] Epoch 3\\Batch 2200\\ Train Loss:10.523\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:32] Epoch 3\\Batch 2250\\ Train Loss:10.527\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:36] Epoch 3\\Batch 2300\\ Train Loss:10.529\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:39] Epoch 3\\Batch 2350\\ Train Loss:10.526\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:43] Epoch 3\\Batch 2400\\ Train Loss:10.527\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:48] Epoch 3\\Batch 2450\\ Train Loss:10.526\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:53] Epoch 3\\Batch 2500\\ Train Loss:10.529\\ Learning rate:0.00030\n",
      "[2019/03/17 20:36:58] Epoch 3\\Batch 2550\\ Train Loss:10.528\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:04] Epoch 3\\Batch 2600\\ Train Loss:10.526\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:09] Epoch 3\\Batch 2650\\ Train Loss:10.527\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:14] Epoch 3\\Batch 2700\\ Train Loss:10.526\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:19] Epoch 3\\Batch 2750\\ Train Loss:10.530\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:24] Epoch 3\\Batch 2800\\ Train Loss:10.528\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:30] Epoch 3\\Batch 2850\\ Train Loss:10.528\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:35] Epoch 3\\Batch 2900\\ Train Loss:10.524\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:40] Epoch 3\\Batch 2950\\ Train Loss:10.525\\ Learning rate:0.00030\n",
      "[2019/03/17 20:37:44] Epoch 3\\Batch 3000\\ Train Loss:10.523\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5550.000001, 'TP': 1449.0000009999999, 'FP': 1705.0000009999999}\n",
      "[2019/03/17 20:38:07] Epoch 3/ Validation Loss:10.088/ F1_score:0.285/ Precision:0.459/ Recall:0.207\n",
      "[2019/03/17 20:38:12] Epoch 3\\Batch 3050\\ Train Loss:10.524\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:17] Epoch 3\\Batch 3100\\ Train Loss:10.527\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:22] Epoch 3\\Batch 3150\\ Train Loss:10.525\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:27] Epoch 3\\Batch 3200\\ Train Loss:10.524\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:32] Epoch 3\\Batch 3250\\ Train Loss:10.522\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:37] Epoch 3\\Batch 3300\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:42] Epoch 3\\Batch 3350\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:47] Epoch 3\\Batch 3400\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:52] Epoch 3\\Batch 3450\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:38:57] Epoch 3\\Batch 3500\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:02] Epoch 3\\Batch 3550\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:07] Epoch 3\\Batch 3600\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:12] Epoch 3\\Batch 3650\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:17] Epoch 3\\Batch 3700\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:22] Epoch 3\\Batch 3750\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:27] Epoch 3\\Batch 3800\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:32] Epoch 3\\Batch 3850\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:37] Epoch 3\\Batch 3900\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:42] Epoch 3\\Batch 3950\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:47] Epoch 3\\Batch 4000\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:52] Epoch 3\\Batch 4050\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:39:57] Epoch 3\\Batch 4100\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:02] Epoch 3\\Batch 4150\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:07] Epoch 3\\Batch 4200\\ Train Loss:10.514\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 20:40:12] Epoch 3\\Batch 4250\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:17] Epoch 3\\Batch 4300\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:22] Epoch 3\\Batch 4350\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:27] Epoch 3\\Batch 4400\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:32] Epoch 3\\Batch 4450\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:37] Epoch 3\\Batch 4500\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:42] Epoch 3\\Batch 4550\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:47] Epoch 3\\Batch 4600\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:52] Epoch 3\\Batch 4650\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:40:57] Epoch 3\\Batch 4700\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:02] Epoch 3\\Batch 4750\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:06] Epoch 3\\Batch 4800\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:11] Epoch 3\\Batch 4850\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:16] Epoch 3\\Batch 4900\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:21] Epoch 3\\Batch 4950\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:26] Epoch 3\\Batch 5000\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:31] Epoch 3\\Batch 5050\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:36] Epoch 3\\Batch 5100\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:41] Epoch 3\\Batch 5150\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:46] Epoch 3\\Batch 5200\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:51] Epoch 3\\Batch 5250\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:41:56] Epoch 3\\Batch 5300\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:01] Epoch 3\\Batch 5350\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:06] Epoch 3\\Batch 5400\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:11] Epoch 3\\Batch 5450\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:16] Epoch 3\\Batch 5500\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:21] Epoch 3\\Batch 5550\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:25] Epoch 3\\Batch 5600\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:30] Epoch 3\\Batch 5650\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:36] Epoch 3\\Batch 5700\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:41] Epoch 3\\Batch 5750\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:46] Epoch 3\\Batch 5800\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:51] Epoch 3\\Batch 5850\\ Train Loss:10.519\\ Learning rate:0.00030\n",
      "[2019/03/17 20:42:56] Epoch 3\\Batch 5900\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:00] Epoch 3\\Batch 5950\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:05] Epoch 3\\Batch 6000\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5545.000001, 'TP': 1454.0000009999999, 'FP': 1718.0000009999999}\n",
      "[2019/03/17 20:43:27] Epoch 3/ Validation Loss:10.076/ F1_score:0.286/ Precision:0.458/ Recall:0.208\n",
      "[2019/03/17 20:43:30] Epoch 3\\Batch 6050\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:33] Epoch 3\\Batch 6100\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:36] Epoch 3\\Batch 6150\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:40] Epoch 3\\Batch 6200\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:43] Epoch 3\\Batch 6250\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:47] Epoch 3\\Batch 6300\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:52] Epoch 3\\Batch 6350\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:43:57] Epoch 3\\Batch 6400\\ Train Loss:10.521\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:02] Epoch 3\\Batch 6450\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:07] Epoch 3\\Batch 6500\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:12] Epoch 3\\Batch 6550\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:17] Epoch 3\\Batch 6600\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:22] Epoch 3\\Batch 6650\\ Train Loss:10.520\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:27] Epoch 3\\Batch 6700\\ Train Loss:10.519\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:32] Epoch 3\\Batch 6750\\ Train Loss:10.519\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:37] Epoch 3\\Batch 6800\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:42] Epoch 3\\Batch 6850\\ Train Loss:10.519\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:47] Epoch 3\\Batch 6900\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:52] Epoch 3\\Batch 6950\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:44:57] Epoch 3\\Batch 7000\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:02] Epoch 3\\Batch 7050\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:07] Epoch 3\\Batch 7100\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:12] Epoch 3\\Batch 7150\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:17] Epoch 3\\Batch 7200\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:22] Epoch 3\\Batch 7250\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:27] Epoch 3\\Batch 7300\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:31] Epoch 3\\Batch 7350\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:37] Epoch 3\\Batch 7400\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:41] Epoch 3\\Batch 7450\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:46] Epoch 3\\Batch 7500\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:52] Epoch 3\\Batch 7550\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:45:56] Epoch 3\\Batch 7600\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:02] Epoch 3\\Batch 7650\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:07] Epoch 3\\Batch 7700\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:12] Epoch 3\\Batch 7750\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:17] Epoch 3\\Batch 7800\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:21] Epoch 3\\Batch 7850\\ Train Loss:10.518\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:26] Epoch 3\\Batch 7900\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:32] Epoch 3\\Batch 7950\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:37] Epoch 3\\Batch 8000\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:42] Epoch 3\\Batch 8050\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:47] Epoch 3\\Batch 8100\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:51] Epoch 3\\Batch 8150\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:46:56] Epoch 3\\Batch 8200\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:01] Epoch 3\\Batch 8250\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:06] Epoch 3\\Batch 8300\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:11] Epoch 3\\Batch 8350\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:16] Epoch 3\\Batch 8400\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:21] Epoch 3\\Batch 8450\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:26] Epoch 3\\Batch 8500\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:31] Epoch 3\\Batch 8550\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:36] Epoch 3\\Batch 8600\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:41] Epoch 3\\Batch 8650\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:46] Epoch 3\\Batch 8700\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:51] Epoch 3\\Batch 8750\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:47:55] Epoch 3\\Batch 8800\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:00] Epoch 3\\Batch 8850\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:05] Epoch 3\\Batch 8900\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:10] Epoch 3\\Batch 8950\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:15] Epoch 3\\Batch 9000\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5542.000001, 'TP': 1457.0000009999999, 'FP': 1711.0000009999999}\n",
      "[2019/03/17 20:48:38] Epoch 3/ Validation Loss:10.071/ F1_score:0.287/ Precision:0.460/ Recall:0.208\n",
      "[2019/03/17 20:48:43] Epoch 3\\Batch 9050\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:47] Epoch 3\\Batch 9100\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:52] Epoch 3\\Batch 9150\\ Train Loss:10.517\\ Learning rate:0.00030\n",
      "[2019/03/17 20:48:57] Epoch 3\\Batch 9200\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:02] Epoch 3\\Batch 9250\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:07] Epoch 3\\Batch 9300\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:12] Epoch 3\\Batch 9350\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:17] Epoch 3\\Batch 9400\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:22] Epoch 3\\Batch 9450\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:27] Epoch 3\\Batch 9500\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:32] Epoch 3\\Batch 9550\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:37] Epoch 3\\Batch 9600\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:41] Epoch 3\\Batch 9650\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:46] Epoch 3\\Batch 9700\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:51] Epoch 3\\Batch 9750\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:49:56] Epoch 3\\Batch 9800\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:01] Epoch 3\\Batch 9850\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:06] Epoch 3\\Batch 9900\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:11] Epoch 3\\Batch 9950\\ Train Loss:10.516\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:16] Epoch 3\\Batch 10000\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:21] Epoch 3\\Batch 10050\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:26] Epoch 3\\Batch 10100\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:30] Epoch 3\\Batch 10150\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:33] Epoch 3\\Batch 10200\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:37] Epoch 3\\Batch 10250\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:40] Epoch 3\\Batch 10300\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:43] Epoch 3\\Batch 10350\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:47] Epoch 3\\Batch 10400\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:50] Epoch 3\\Batch 10450\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:54] Epoch 3\\Batch 10500\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:50:59] Epoch 3\\Batch 10550\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:04] Epoch 3\\Batch 10600\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:09] Epoch 3\\Batch 10650\\ Train Loss:10.515\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:13] Epoch 3\\Batch 10700\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:18] Epoch 3\\Batch 10750\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:23] Epoch 3\\Batch 10800\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:28] Epoch 3\\Batch 10850\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:33] Epoch 3\\Batch 10900\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:38] Epoch 3\\Batch 10950\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:43] Epoch 3\\Batch 11000\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:48] Epoch 3\\Batch 11050\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:53] Epoch 3\\Batch 11100\\ Train Loss:10.514\\ Learning rate:0.00030\n",
      "[2019/03/17 20:51:58] Epoch 3\\Batch 11150\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:03] Epoch 3\\Batch 11200\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:08] Epoch 3\\Batch 11250\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:13] Epoch 3\\Batch 11300\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:18] Epoch 3\\Batch 11350\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:23] Epoch 3\\Batch 11400\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:28] Epoch 3\\Batch 11450\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:33] Epoch 3\\Batch 11500\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:38] Epoch 3\\Batch 11550\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:43] Epoch 3\\Batch 11600\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:48] Epoch 3\\Batch 11650\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:53] Epoch 3\\Batch 11700\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:52:58] Epoch 3\\Batch 11750\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:53:03] Epoch 3\\Batch 11800\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 20:53:08] Epoch 3\\Batch 11850\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:53:13] Epoch 3\\Batch 11900\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:53:18] Epoch 3\\Batch 11950\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:53:23] Epoch 3\\Batch 12000\\ Train Loss:10.511\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5535.000001, 'TP': 1464.0000009999999, 'FP': 1727.0000009999999}\n",
      "[2019/03/17 20:53:46] Epoch 3/ Validation Loss:10.043/ F1_score:0.287/ Precision:0.459/ Recall:0.209\n",
      "[2019/03/17 20:53:51] Epoch 3\\Batch 12050\\ Train Loss:10.511\\ Learning rate:0.00030\n",
      "[2019/03/17 20:53:56] Epoch 3\\Batch 12100\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:01] Epoch 3\\Batch 12150\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:06] Epoch 3\\Batch 12200\\ Train Loss:10.512\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:11] Epoch 3\\Batch 12250\\ Train Loss:10.511\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:16] Epoch 3\\Batch 12300\\ Train Loss:10.510\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:21] Epoch 3\\Batch 12350\\ Train Loss:10.510\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:26] Epoch 3\\Batch 12400\\ Train Loss:10.511\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:31] Epoch 3\\Batch 12450\\ Train Loss:10.510\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:36] Epoch 3\\Batch 12500\\ Train Loss:10.509\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:41] Epoch 3\\Batch 12550\\ Train Loss:10.509\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:46] Epoch 3\\Batch 12600\\ Train Loss:10.509\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:51] Epoch 3\\Batch 12650\\ Train Loss:10.509\\ Learning rate:0.00030\n",
      "[2019/03/17 20:54:56] Epoch 3\\Batch 12700\\ Train Loss:10.509\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:01] Epoch 3\\Batch 12750\\ Train Loss:10.510\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:06] Epoch 3\\Batch 12800\\ Train Loss:10.509\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:11] Epoch 3\\Batch 12850\\ Train Loss:10.508\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:15] Epoch 3\\Batch 12900\\ Train Loss:10.508\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:20] Epoch 3\\Batch 12950\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:25] Epoch 3\\Batch 13000\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:30] Epoch 3\\Batch 13050\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:35] Epoch 3\\Batch 13100\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:40] Epoch 3\\Batch 13150\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:45] Epoch 3\\Batch 13200\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:50] Epoch 3\\Batch 13250\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:55:54] Epoch 3\\Batch 13300\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:00] Epoch 3\\Batch 13350\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:04] Epoch 3\\Batch 13400\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:10] Epoch 3\\Batch 13450\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:15] Epoch 3\\Batch 13500\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:20] Epoch 3\\Batch 13550\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:25] Epoch 3\\Batch 13600\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:30] Epoch 3\\Batch 13650\\ Train Loss:10.507\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 20:56:35] Epoch 3\\Batch 13700\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:40] Epoch 3\\Batch 13750\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:45] Epoch 3\\Batch 13800\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:50] Epoch 3\\Batch 13850\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:56:55] Epoch 3\\Batch 13900\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:00] Epoch 3\\Batch 13950\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:05] Epoch 3\\Batch 14000\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:11] Epoch 3\\Batch 14050\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:16] Epoch 3\\Batch 14100\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:21] Epoch 3\\Batch 14150\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:26] Epoch 3\\Batch 14200\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:31] Epoch 3\\Batch 14250\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:36] Epoch 3\\Batch 14300\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:40] Epoch 3\\Batch 14350\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:44] Epoch 3\\Batch 14400\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:47] Epoch 3\\Batch 14450\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:51] Epoch 3\\Batch 14500\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:54] Epoch 3\\Batch 14550\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:57:58] Epoch 3\\Batch 14600\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:01] Epoch 3\\Batch 14650\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:05] Epoch 3\\Batch 14700\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:10] Epoch 3\\Batch 14750\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:15] Epoch 3\\Batch 14800\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:21] Epoch 3\\Batch 14850\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:26] Epoch 3\\Batch 14900\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:31] Epoch 3\\Batch 14950\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:58:36] Epoch 3\\Batch 15000\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5544.000001, 'TP': 1455.0000009999999, 'FP': 1704.0000009999999}\n",
      "[2019/03/17 20:58:58] Epoch 3/ Validation Loss:10.039/ F1_score:0.286/ Precision:0.461/ Recall:0.208\n",
      "[2019/03/17 20:59:04] Epoch 3\\Batch 15050\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:09] Epoch 3\\Batch 15100\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:13] Epoch 3\\Batch 15150\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:18] Epoch 3\\Batch 15200\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:23] Epoch 3\\Batch 15250\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:28] Epoch 3\\Batch 15300\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:34] Epoch 3\\Batch 15350\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:39] Epoch 3\\Batch 15400\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:44] Epoch 3\\Batch 15450\\ Train Loss:10.507\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:49] Epoch 3\\Batch 15500\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:54] Epoch 3\\Batch 15550\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 20:59:59] Epoch 3\\Batch 15600\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:04] Epoch 3\\Batch 15650\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:09] Epoch 3\\Batch 15700\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:14] Epoch 3\\Batch 15750\\ Train Loss:10.506\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:19] Epoch 3\\Batch 15800\\ Train Loss:10.505\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:24] Epoch 3\\Batch 15850\\ Train Loss:10.505\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:29] Epoch 3\\Batch 15900\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:34] Epoch 3\\Batch 15950\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:39] Epoch 3\\Batch 16000\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:44] Epoch 3\\Batch 16050\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:49] Epoch 3\\Batch 16100\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:54] Epoch 3\\Batch 16150\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:00:59] Epoch 3\\Batch 16200\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:04] Epoch 3\\Batch 16250\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:09] Epoch 3\\Batch 16300\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:14] Epoch 3\\Batch 16350\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:19] Epoch 3\\Batch 16400\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:24] Epoch 3\\Batch 16450\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:29] Epoch 3\\Batch 16500\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:34] Epoch 3\\Batch 16550\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:39] Epoch 3\\Batch 16600\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:44] Epoch 3\\Batch 16650\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:49] Epoch 3\\Batch 16700\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:54] Epoch 3\\Batch 16750\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:01:58] Epoch 3\\Batch 16800\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:03] Epoch 3\\Batch 16850\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:08] Epoch 3\\Batch 16900\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:14] Epoch 3\\Batch 16950\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:18] Epoch 3\\Batch 17000\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:23] Epoch 3\\Batch 17050\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:28] Epoch 3\\Batch 17100\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:34] Epoch 3\\Batch 17150\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:39] Epoch 3\\Batch 17200\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:43] Epoch 3\\Batch 17250\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:48] Epoch 3\\Batch 17300\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:53] Epoch 3\\Batch 17350\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:02:58] Epoch 3\\Batch 17400\\ Train Loss:10.504\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:04] Epoch 3\\Batch 17450\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:09] Epoch 3\\Batch 17500\\ Train Loss:10.503\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:14] Epoch 3\\Batch 17550\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:19] Epoch 3\\Batch 17600\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:24] Epoch 3\\Batch 17650\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:29] Epoch 3\\Batch 17700\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:34] Epoch 3\\Batch 17750\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:39] Epoch 3\\Batch 17800\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:44] Epoch 3\\Batch 17850\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:49] Epoch 3\\Batch 17900\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:03:55] Epoch 3\\Batch 17950\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:00] Epoch 3\\Batch 18000\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5526.000001, 'TP': 1473.0000009999999, 'FP': 1698.0000009999999}\n",
      "[2019/03/17 21:04:22] Epoch 3/ Validation Loss:10.036/ F1_score:0.290/ Precision:0.465/ Recall:0.210\n",
      "[2019/03/17 21:04:28] Epoch 3\\Batch 18050\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:33] Epoch 3\\Batch 18100\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:37] Epoch 3\\Batch 18150\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:41] Epoch 3\\Batch 18200\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:44] Epoch 3\\Batch 18250\\ Train Loss:10.500\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:04:48] Epoch 3\\Batch 18300\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:51] Epoch 3\\Batch 18350\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:54] Epoch 3\\Batch 18400\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:04:58] Epoch 3\\Batch 18450\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:02] Epoch 3\\Batch 18500\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:07] Epoch 3\\Batch 18550\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:12] Epoch 3\\Batch 18600\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:17] Epoch 3\\Batch 18650\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:22] Epoch 3\\Batch 18700\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:27] Epoch 3\\Batch 18750\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:32] Epoch 3\\Batch 18800\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:37] Epoch 3\\Batch 18850\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:41] Epoch 3\\Batch 18900\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:47] Epoch 3\\Batch 18950\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:52] Epoch 3\\Batch 19000\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:05:57] Epoch 3\\Batch 19050\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:02] Epoch 3\\Batch 19100\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:07] Epoch 3\\Batch 19150\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:12] Epoch 3\\Batch 19200\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:17] Epoch 3\\Batch 19250\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:22] Epoch 3\\Batch 19300\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:27] Epoch 3\\Batch 19350\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:32] Epoch 3\\Batch 19400\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:37] Epoch 3\\Batch 19450\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:42] Epoch 3\\Batch 19500\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:47] Epoch 3\\Batch 19550\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:52] Epoch 3\\Batch 19600\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:06:57] Epoch 3\\Batch 19650\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:02] Epoch 3\\Batch 19700\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:07] Epoch 3\\Batch 19750\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:12] Epoch 3\\Batch 19800\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:17] Epoch 3\\Batch 19850\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:22] Epoch 3\\Batch 19900\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:26] Epoch 3\\Batch 19950\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:31] Epoch 3\\Batch 20000\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:36] Epoch 3\\Batch 20050\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:41] Epoch 3\\Batch 20100\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:46] Epoch 3\\Batch 20150\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:51] Epoch 3\\Batch 20200\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:56] Epoch 3\\Batch 20250\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:07:59] Epoch 3\\Batch 20300\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:02] Epoch 3\\Batch 20350\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:05] Epoch 3\\Batch 20400\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:09] Epoch 3\\Batch 20450\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:12] Epoch 3\\Batch 20500\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:15] Epoch 3\\Batch 20550\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:19] Epoch 3\\Batch 20600\\ Train Loss:10.502\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:22] Epoch 3\\Batch 20650\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:25] Epoch 3\\Batch 20700\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:28] Epoch 3\\Batch 20750\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:32] Epoch 3\\Batch 20800\\ Train Loss:10.501\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:35] Epoch 3\\Batch 20850\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:38] Epoch 3\\Batch 20900\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:41] Epoch 3\\Batch 20950\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:08:45] Epoch 3\\Batch 21000\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5518.000001, 'TP': 1481.0000009999999, 'FP': 1700.0000009999999}\n",
      "[2019/03/17 21:09:08] Epoch 3/ Validation Loss:10.041/ F1_score:0.291/ Precision:0.466/ Recall:0.212\n",
      "[2019/03/17 21:09:13] Epoch 3\\Batch 21050\\ Train Loss:10.500\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:18] Epoch 3\\Batch 21100\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:23] Epoch 3\\Batch 21150\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:28] Epoch 3\\Batch 21200\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:33] Epoch 3\\Batch 21250\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:38] Epoch 3\\Batch 21300\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:43] Epoch 3\\Batch 21350\\ Train Loss:10.499\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:48] Epoch 3\\Batch 21400\\ Train Loss:10.498\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:53] Epoch 3\\Batch 21450\\ Train Loss:10.498\\ Learning rate:0.00030\n",
      "[2019/03/17 21:09:58] Epoch 3\\Batch 21500\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:03] Epoch 3\\Batch 21550\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:08] Epoch 3\\Batch 21600\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:13] Epoch 3\\Batch 21650\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:18] Epoch 3\\Batch 21700\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:23] Epoch 3\\Batch 21750\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:27] Epoch 3\\Batch 21800\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:32] Epoch 3\\Batch 21850\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:37] Epoch 3\\Batch 21900\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:42] Epoch 3\\Batch 21950\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:47] Epoch 3\\Batch 22000\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:52] Epoch 3\\Batch 22050\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:10:57] Epoch 3\\Batch 22100\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:02] Epoch 3\\Batch 22150\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:07] Epoch 3\\Batch 22200\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:12] Epoch 3\\Batch 22250\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:17] Epoch 3\\Batch 22300\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:22] Epoch 3\\Batch 22350\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:27] Epoch 3\\Batch 22400\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:32] Epoch 3\\Batch 22450\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:37] Epoch 3\\Batch 22500\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:41] Epoch 3\\Batch 22550\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:46] Epoch 3\\Batch 22600\\ Train Loss:10.497\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:51] Epoch 3\\Batch 22650\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:11:56] Epoch 3\\Batch 22700\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:01] Epoch 3\\Batch 22750\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:06] Epoch 3\\Batch 22800\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:11] Epoch 3\\Batch 22850\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:16] Epoch 3\\Batch 22900\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:21] Epoch 3\\Batch 22950\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:26] Epoch 3\\Batch 23000\\ Train Loss:10.495\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:12:31] Epoch 3\\Batch 23050\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:35] Epoch 3\\Batch 23100\\ Train Loss:10.496\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:40] Epoch 3\\Batch 23150\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:45] Epoch 3\\Batch 23200\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:50] Epoch 3\\Batch 23250\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:12:55] Epoch 3\\Batch 23300\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:00] Epoch 3\\Batch 23350\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:05] Epoch 3\\Batch 23400\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:10] Epoch 3\\Batch 23450\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:15] Epoch 3\\Batch 23500\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:20] Epoch 3\\Batch 23550\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:24] Epoch 3\\Batch 23600\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:29] Epoch 3\\Batch 23650\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:34] Epoch 3\\Batch 23700\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:39] Epoch 3\\Batch 23750\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:44] Epoch 3\\Batch 23800\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:49] Epoch 3\\Batch 23850\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:54] Epoch 3\\Batch 23900\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:13:59] Epoch 3\\Batch 23950\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "[2019/03/17 21:14:04] Epoch 3\\Batch 24000\\ Train Loss:10.495\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5535.000001, 'TP': 1464.0000009999999, 'FP': 1706.0000009999999}\n",
      "[2019/03/17 21:14:27] Epoch 3/ Validation Loss:10.007/ F1_score:0.288/ Precision:0.462/ Recall:0.209\n",
      "[2019/03/17 21:14:32] Epoch 3\\Batch 24050\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:14:37] Epoch 3\\Batch 24100\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:14:42] Epoch 3\\Batch 24150\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:14:47] Epoch 3\\Batch 24200\\ Train Loss:10.494\\ Learning rate:0.00030\n",
      "[2019/03/17 21:14:52] Epoch 3\\Batch 24250\\ Train Loss:10.493\\ Learning rate:0.00030\n",
      "[2019/03/17 21:14:57] Epoch 3\\Batch 24300\\ Train Loss:10.493\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:01] Epoch 3\\Batch 24350\\ Train Loss:10.493\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:06] Epoch 3\\Batch 24400\\ Train Loss:10.492\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:11] Epoch 3\\Batch 24450\\ Train Loss:10.493\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:16] Epoch 3\\Batch 24500\\ Train Loss:10.492\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:20] Epoch 3\\Batch 24550\\ Train Loss:10.492\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:23] Epoch 3\\Batch 24600\\ Train Loss:10.491\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:26] Epoch 3\\Batch 24650\\ Train Loss:10.491\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:29] Epoch 3\\Batch 24700\\ Train Loss:10.491\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:32] Epoch 3\\Batch 24750\\ Train Loss:10.491\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:36] Epoch 3\\Batch 24800\\ Train Loss:10.491\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:39] Epoch 3\\Batch 24850\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:42] Epoch 3\\Batch 24900\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:47] Epoch 3\\Batch 24950\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:52] Epoch 3\\Batch 25000\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:15:57] Epoch 3\\Batch 25050\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:02] Epoch 3\\Batch 25100\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:07] Epoch 3\\Batch 25150\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:12] Epoch 3\\Batch 25200\\ Train Loss:10.489\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:17] Epoch 3\\Batch 25250\\ Train Loss:10.490\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:22] Epoch 3\\Batch 25300\\ Train Loss:10.489\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:27] Epoch 3\\Batch 25350\\ Train Loss:10.489\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:31] Epoch 3\\Batch 25400\\ Train Loss:10.489\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:36] Epoch 3\\Batch 25450\\ Train Loss:10.489\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:41] Epoch 3\\Batch 25500\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:46] Epoch 3\\Batch 25550\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:51] Epoch 3\\Batch 25600\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:16:56] Epoch 3\\Batch 25650\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:01] Epoch 3\\Batch 25700\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:06] Epoch 3\\Batch 25750\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:11] Epoch 3\\Batch 25800\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:16] Epoch 3\\Batch 25850\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:21] Epoch 3\\Batch 25900\\ Train Loss:10.488\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:26] Epoch 3\\Batch 25950\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:31] Epoch 3\\Batch 26000\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:36] Epoch 3\\Batch 26050\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:41] Epoch 3\\Batch 26100\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:46] Epoch 3\\Batch 26150\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:50] Epoch 3\\Batch 26200\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:17:55] Epoch 3\\Batch 26250\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:00] Epoch 3\\Batch 26300\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:05] Epoch 3\\Batch 26350\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:10] Epoch 3\\Batch 26400\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:15] Epoch 3\\Batch 26450\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:20] Epoch 3\\Batch 26500\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:25] Epoch 3\\Batch 26550\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:30] Epoch 3\\Batch 26600\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:35] Epoch 3\\Batch 26650\\ Train Loss:10.487\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:40] Epoch 3\\Batch 26700\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:45] Epoch 3\\Batch 26750\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:50] Epoch 3\\Batch 26800\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:54] Epoch 3\\Batch 26850\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:18:59] Epoch 3\\Batch 26900\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:19:04] Epoch 3\\Batch 26950\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:19:09] Epoch 3\\Batch 27000\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5549.000001, 'TP': 1450.0000009999999, 'FP': 1715.0000009999999}\n",
      "[2019/03/17 21:19:32] Epoch 3/ Validation Loss:9.989/ F1_score:0.285/ Precision:0.458/ Recall:0.207\n",
      "[2019/03/17 21:19:37] Epoch 3\\Batch 27050\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:19:42] Epoch 3\\Batch 27100\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:19:47] Epoch 3\\Batch 27150\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:19:52] Epoch 3\\Batch 27200\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:19:57] Epoch 3\\Batch 27250\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:02] Epoch 3\\Batch 27300\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:07] Epoch 3\\Batch 27350\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:12] Epoch 3\\Batch 27400\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:17] Epoch 3\\Batch 27450\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:22] Epoch 3\\Batch 27500\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:27] Epoch 3\\Batch 27550\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:32] Epoch 3\\Batch 27600\\ Train Loss:10.485\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:20:37] Epoch 3\\Batch 27650\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:42] Epoch 3\\Batch 27700\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:47] Epoch 3\\Batch 27750\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:53] Epoch 3\\Batch 27800\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:20:57] Epoch 3\\Batch 27850\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:02] Epoch 3\\Batch 27900\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:08] Epoch 3\\Batch 27950\\ Train Loss:10.485\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:13] Epoch 3\\Batch 28000\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:17] Epoch 3\\Batch 28050\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:23] Epoch 3\\Batch 28100\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:28] Epoch 3\\Batch 28150\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:33] Epoch 3\\Batch 28200\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:38] Epoch 3\\Batch 28250\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:43] Epoch 3\\Batch 28300\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:48] Epoch 3\\Batch 28350\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:52] Epoch 3\\Batch 28400\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:21:57] Epoch 3\\Batch 28450\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:02] Epoch 3\\Batch 28500\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:07] Epoch 3\\Batch 28550\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:12] Epoch 3\\Batch 28600\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:17] Epoch 3\\Batch 28650\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:22] Epoch 3\\Batch 28700\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:27] Epoch 3\\Batch 28750\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:31] Epoch 3\\Batch 28800\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:34] Epoch 3\\Batch 28850\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:38] Epoch 3\\Batch 28900\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:41] Epoch 3\\Batch 28950\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:45] Epoch 3\\Batch 29000\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:48] Epoch 3\\Batch 29050\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:52] Epoch 3\\Batch 29100\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:22:56] Epoch 3\\Batch 29150\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:01] Epoch 3\\Batch 29200\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:06] Epoch 3\\Batch 29250\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:11] Epoch 3\\Batch 29300\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:16] Epoch 3\\Batch 29350\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:21] Epoch 3\\Batch 29400\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:26] Epoch 3\\Batch 29450\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:31] Epoch 3\\Batch 29500\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:36] Epoch 3\\Batch 29550\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:41] Epoch 3\\Batch 29600\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:46] Epoch 3\\Batch 29650\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:51] Epoch 3\\Batch 29700\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:23:55] Epoch 3\\Batch 29750\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:00] Epoch 3\\Batch 29800\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:05] Epoch 3\\Batch 29850\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:10] Epoch 3\\Batch 29900\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:15] Epoch 3\\Batch 29950\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:20] Epoch 3\\Batch 30000\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5519.000001, 'TP': 1480.0000009999999, 'FP': 1705.0000009999999}\n",
      "[2019/03/17 21:24:43] Epoch 3/ Validation Loss:10.003/ F1_score:0.291/ Precision:0.465/ Recall:0.211\n",
      "[2019/03/17 21:24:48] Epoch 3\\Batch 30050\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:54] Epoch 3\\Batch 30100\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:24:59] Epoch 3\\Batch 30150\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:04] Epoch 3\\Batch 30200\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:09] Epoch 3\\Batch 30250\\ Train Loss:10.483\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:14] Epoch 3\\Batch 30300\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:19] Epoch 3\\Batch 30350\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:25] Epoch 3\\Batch 30400\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:30] Epoch 3\\Batch 30450\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:35] Epoch 3\\Batch 30500\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:40] Epoch 3\\Batch 30550\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:45] Epoch 3\\Batch 30600\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:50] Epoch 3\\Batch 30650\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:25:55] Epoch 3\\Batch 30700\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:00] Epoch 3\\Batch 30750\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:05] Epoch 3\\Batch 30800\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:10] Epoch 3\\Batch 30850\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:15] Epoch 3\\Batch 30900\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:20] Epoch 3\\Batch 30950\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:25] Epoch 3\\Batch 31000\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:30] Epoch 3\\Batch 31050\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:35] Epoch 3\\Batch 31100\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:40] Epoch 3\\Batch 31150\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:45] Epoch 3\\Batch 31200\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:50] Epoch 3\\Batch 31250\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:26:55] Epoch 3\\Batch 31300\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:00] Epoch 3\\Batch 31350\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:05] Epoch 3\\Batch 31400\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:10] Epoch 3\\Batch 31450\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:15] Epoch 3\\Batch 31500\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:20] Epoch 3\\Batch 31550\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:25] Epoch 3\\Batch 31600\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:30] Epoch 3\\Batch 31650\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:35] Epoch 3\\Batch 31700\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:40] Epoch 3\\Batch 31750\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:45] Epoch 3\\Batch 31800\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:50] Epoch 3\\Batch 31850\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:27:55] Epoch 3\\Batch 31900\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:00] Epoch 3\\Batch 31950\\ Train Loss:10.482\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:05] Epoch 3\\Batch 32000\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:10] Epoch 3\\Batch 32050\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:14] Epoch 3\\Batch 32100\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:19] Epoch 3\\Batch 32150\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:24] Epoch 3\\Batch 32200\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:30] Epoch 3\\Batch 32250\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:34] Epoch 3\\Batch 32300\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:40] Epoch 3\\Batch 32350\\ Train Loss:10.481\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:28:45] Epoch 3\\Batch 32400\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:50] Epoch 3\\Batch 32450\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:28:55] Epoch 3\\Batch 32500\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:00] Epoch 3\\Batch 32550\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:05] Epoch 3\\Batch 32600\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:10] Epoch 3\\Batch 32650\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:15] Epoch 3\\Batch 32700\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:20] Epoch 3\\Batch 32750\\ Train Loss:10.481\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:26] Epoch 3\\Batch 32800\\ Train Loss:10.480\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:31] Epoch 3\\Batch 32850\\ Train Loss:10.480\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:36] Epoch 3\\Batch 32900\\ Train Loss:10.480\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:41] Epoch 3\\Batch 32950\\ Train Loss:10.480\\ Learning rate:0.00030\n",
      "[2019/03/17 21:29:45] Epoch 3\\Batch 33000\\ Train Loss:10.480\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5502.000001, 'TP': 1497.0000009999999, 'FP': 1683.0000009999999}\n",
      "[2019/03/17 21:30:02] Epoch 3/ Validation Loss:9.990/ F1_score:0.294/ Precision:0.471/ Recall:0.214\n",
      "[2019/03/17 21:30:08] Epoch 3\\Batch 33050\\ Train Loss:10.480\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:13] Epoch 3\\Batch 33100\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:18] Epoch 3\\Batch 33150\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:24] Epoch 3\\Batch 33200\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:29] Epoch 3\\Batch 33250\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:34] Epoch 3\\Batch 33300\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:40] Epoch 3\\Batch 33350\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:45] Epoch 3\\Batch 33400\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:50] Epoch 3\\Batch 33450\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:30:56] Epoch 3\\Batch 33500\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:01] Epoch 3\\Batch 33550\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:06] Epoch 3\\Batch 33600\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:11] Epoch 3\\Batch 33650\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:17] Epoch 3\\Batch 33700\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:22] Epoch 3\\Batch 33750\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:27] Epoch 3\\Batch 33800\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:33] Epoch 3\\Batch 33850\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:38] Epoch 3\\Batch 33900\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:43] Epoch 3\\Batch 33950\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:49] Epoch 3\\Batch 34000\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:54] Epoch 3\\Batch 34050\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:31:59] Epoch 3\\Batch 34100\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:04] Epoch 3\\Batch 34150\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:10] Epoch 3\\Batch 34200\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:15] Epoch 3\\Batch 34250\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:20] Epoch 3\\Batch 34300\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:25] Epoch 3\\Batch 34350\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:31] Epoch 3\\Batch 34400\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:36] Epoch 3\\Batch 34450\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:41] Epoch 3\\Batch 34500\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:46] Epoch 3\\Batch 34550\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:52] Epoch 3\\Batch 34600\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:32:57] Epoch 3\\Batch 34650\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:02] Epoch 3\\Batch 34700\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:08] Epoch 3\\Batch 34750\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:13] Epoch 3\\Batch 34800\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:18] Epoch 3\\Batch 34850\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:23] Epoch 3\\Batch 34900\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:29] Epoch 3\\Batch 34950\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:34] Epoch 3\\Batch 35000\\ Train Loss:10.479\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:39] Epoch 3\\Batch 35050\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:44] Epoch 3\\Batch 35100\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:50] Epoch 3\\Batch 35150\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:33:55] Epoch 3\\Batch 35200\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:00] Epoch 3\\Batch 35250\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:05] Epoch 3\\Batch 35300\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:11] Epoch 3\\Batch 35350\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:16] Epoch 3\\Batch 35400\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:21] Epoch 3\\Batch 35450\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:27] Epoch 3\\Batch 35500\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:32] Epoch 3\\Batch 35550\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:38] Epoch 3\\Batch 35600\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:43] Epoch 3\\Batch 35650\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:48] Epoch 3\\Batch 35700\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:54] Epoch 3\\Batch 35750\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:34:59] Epoch 3\\Batch 35800\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:35:05] Epoch 3\\Batch 35850\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:35:10] Epoch 3\\Batch 35900\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:35:15] Epoch 3\\Batch 35950\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:35:21] Epoch 3\\Batch 36000\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5493.000001, 'TP': 1506.0000009999999, 'FP': 1675.0000009999999}\n",
      "[2019/03/17 21:35:43] Epoch 3/ Validation Loss:9.979/ F1_score:0.296/ Precision:0.473/ Recall:0.215\n",
      "[2019/03/17 21:35:49] Epoch 3\\Batch 36050\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:35:54] Epoch 3\\Batch 36100\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:35:59] Epoch 3\\Batch 36150\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:04] Epoch 3\\Batch 36200\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:09] Epoch 3\\Batch 36250\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:14] Epoch 3\\Batch 36300\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:20] Epoch 3\\Batch 36350\\ Train Loss:10.476\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:25] Epoch 3\\Batch 36400\\ Train Loss:10.476\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:30] Epoch 3\\Batch 36450\\ Train Loss:10.476\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:35] Epoch 3\\Batch 36500\\ Train Loss:10.476\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:40] Epoch 3\\Batch 36550\\ Train Loss:10.476\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:46] Epoch 3\\Batch 36600\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:51] Epoch 3\\Batch 36650\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:36:56] Epoch 3\\Batch 36700\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:01] Epoch 3\\Batch 36750\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:06] Epoch 3\\Batch 36800\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:09] Epoch 3\\Batch 36850\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:13] Epoch 3\\Batch 36900\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:16] Epoch 3\\Batch 36950\\ Train Loss:10.475\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:37:20] Epoch 3\\Batch 37000\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:23] Epoch 3\\Batch 37050\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:27] Epoch 3\\Batch 37100\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:30] Epoch 3\\Batch 37150\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:35] Epoch 3\\Batch 37200\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:40] Epoch 3\\Batch 37250\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:46] Epoch 3\\Batch 37300\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:51] Epoch 3\\Batch 37350\\ Train Loss:10.475\\ Learning rate:0.00030\n",
      "[2019/03/17 21:37:56] Epoch 3\\Batch 37400\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:01] Epoch 3\\Batch 37450\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:06] Epoch 3\\Batch 37500\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:11] Epoch 3\\Batch 37550\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:17] Epoch 3\\Batch 37600\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:22] Epoch 3\\Batch 37650\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:27] Epoch 3\\Batch 37700\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:32] Epoch 3\\Batch 37750\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:37] Epoch 3\\Batch 37800\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:42] Epoch 3\\Batch 37850\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:47] Epoch 3\\Batch 37900\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:53] Epoch 3\\Batch 37950\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:38:58] Epoch 3\\Batch 38000\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:03] Epoch 3\\Batch 38050\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:08] Epoch 3\\Batch 38100\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:13] Epoch 3\\Batch 38150\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:18] Epoch 3\\Batch 38200\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:23] Epoch 3\\Batch 38250\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:28] Epoch 3\\Batch 38300\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:33] Epoch 3\\Batch 38350\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:38] Epoch 3\\Batch 38400\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:44] Epoch 3\\Batch 38450\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:49] Epoch 3\\Batch 38500\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:54] Epoch 3\\Batch 38550\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:39:59] Epoch 3\\Batch 38600\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:04] Epoch 3\\Batch 38650\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:09] Epoch 3\\Batch 38700\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:14] Epoch 3\\Batch 38750\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:19] Epoch 3\\Batch 38800\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:24] Epoch 3\\Batch 38850\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:30] Epoch 3\\Batch 38900\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:35] Epoch 3\\Batch 38950\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:40:40] Epoch 3\\Batch 39000\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5496.000001, 'TP': 1503.0000009999999, 'FP': 1698.0000009999999}\n",
      "[2019/03/17 21:41:03] Epoch 3/ Validation Loss:10.006/ F1_score:0.295/ Precision:0.470/ Recall:0.215\n",
      "[2019/03/17 21:41:08] Epoch 3\\Batch 39050\\ Train Loss:10.474\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:13] Epoch 3\\Batch 39100\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:18] Epoch 3\\Batch 39150\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:23] Epoch 3\\Batch 39200\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:28] Epoch 3\\Batch 39250\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:33] Epoch 3\\Batch 39300\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:38] Epoch 3\\Batch 39350\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:43] Epoch 3\\Batch 39400\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:48] Epoch 3\\Batch 39450\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:53] Epoch 3\\Batch 39500\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:41:58] Epoch 3\\Batch 39550\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:03] Epoch 3\\Batch 39600\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:09] Epoch 3\\Batch 39650\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:14] Epoch 3\\Batch 39700\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:19] Epoch 3\\Batch 39750\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:24] Epoch 3\\Batch 39800\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:29] Epoch 3\\Batch 39850\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:35] Epoch 3\\Batch 39900\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:40] Epoch 3\\Batch 39950\\ Train Loss:10.473\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:45] Epoch 3\\Batch 40000\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:50] Epoch 3\\Batch 40050\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:42:55] Epoch 3\\Batch 40100\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:00] Epoch 3\\Batch 40150\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:05] Epoch 3\\Batch 40200\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:10] Epoch 3\\Batch 40250\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:16] Epoch 3\\Batch 40300\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:21] Epoch 3\\Batch 40350\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:26] Epoch 3\\Batch 40400\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:31] Epoch 3\\Batch 40450\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:37] Epoch 3\\Batch 40500\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:42] Epoch 3\\Batch 40550\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:47] Epoch 3\\Batch 40600\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:53] Epoch 3\\Batch 40650\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:43:58] Epoch 3\\Batch 40700\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:03] Epoch 3\\Batch 40750\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:09] Epoch 3\\Batch 40800\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:14] Epoch 3\\Batch 40850\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:19] Epoch 3\\Batch 40900\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:25] Epoch 3\\Batch 40950\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:30] Epoch 3\\Batch 41000\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:35] Epoch 3\\Batch 41050\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:40] Epoch 3\\Batch 41100\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:43] Epoch 3\\Batch 41150\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:46] Epoch 3\\Batch 41200\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:50] Epoch 3\\Batch 41250\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:53] Epoch 3\\Batch 41300\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:44:56] Epoch 3\\Batch 41350\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:00] Epoch 3\\Batch 41400\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:03] Epoch 3\\Batch 41450\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:08] Epoch 3\\Batch 41500\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:14] Epoch 3\\Batch 41550\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:19] Epoch 3\\Batch 41600\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:24] Epoch 3\\Batch 41650\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:29] Epoch 3\\Batch 41700\\ Train Loss:10.471\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:45:34] Epoch 3\\Batch 41750\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:40] Epoch 3\\Batch 41800\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:45] Epoch 3\\Batch 41850\\ Train Loss:10.471\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:50] Epoch 3\\Batch 41900\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:45:56] Epoch 3\\Batch 41950\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:46:01] Epoch 3\\Batch 42000\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5501.000001, 'TP': 1498.0000009999999, 'FP': 1679.0000009999999}\n",
      "[2019/03/17 21:46:24] Epoch 3/ Validation Loss:9.984/ F1_score:0.294/ Precision:0.472/ Recall:0.214\n",
      "[2019/03/17 21:46:29] Epoch 3\\Batch 42050\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:46:34] Epoch 3\\Batch 42100\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:46:39] Epoch 3\\Batch 42150\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:46:45] Epoch 3\\Batch 42200\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:46:50] Epoch 3\\Batch 42250\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:46:55] Epoch 3\\Batch 42300\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:00] Epoch 3\\Batch 42350\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:05] Epoch 3\\Batch 42400\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:11] Epoch 3\\Batch 42450\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:16] Epoch 3\\Batch 42500\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:21] Epoch 3\\Batch 42550\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:26] Epoch 3\\Batch 42600\\ Train Loss:10.470\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:31] Epoch 3\\Batch 42650\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:36] Epoch 3\\Batch 42700\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:41] Epoch 3\\Batch 42750\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:47] Epoch 3\\Batch 42800\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:52] Epoch 3\\Batch 42850\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:47:57] Epoch 3\\Batch 42900\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:02] Epoch 3\\Batch 42950\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:07] Epoch 3\\Batch 43000\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:12] Epoch 3\\Batch 43050\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:17] Epoch 3\\Batch 43100\\ Train Loss:10.469\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:23] Epoch 3\\Batch 43150\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:28] Epoch 3\\Batch 43200\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:34] Epoch 3\\Batch 43250\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:40] Epoch 3\\Batch 43300\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:45] Epoch 3\\Batch 43350\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:51] Epoch 3\\Batch 43400\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:48:56] Epoch 3\\Batch 43450\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:02] Epoch 3\\Batch 43500\\ Train Loss:10.468\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:07] Epoch 3\\Batch 43550\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:13] Epoch 3\\Batch 43600\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:19] Epoch 3\\Batch 43650\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:24] Epoch 3\\Batch 43700\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:30] Epoch 3\\Batch 43750\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:35] Epoch 3\\Batch 43800\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:41] Epoch 3\\Batch 43850\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:46] Epoch 3\\Batch 43900\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:51] Epoch 3\\Batch 43950\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:49:56] Epoch 3\\Batch 44000\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:02] Epoch 3\\Batch 44050\\ Train Loss:10.467\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:07] Epoch 3\\Batch 44100\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:12] Epoch 3\\Batch 44150\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:17] Epoch 3\\Batch 44200\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:22] Epoch 3\\Batch 44250\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:27] Epoch 3\\Batch 44300\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:32] Epoch 3\\Batch 44350\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:38] Epoch 3\\Batch 44400\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:43] Epoch 3\\Batch 44450\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:48] Epoch 3\\Batch 44500\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:53] Epoch 3\\Batch 44550\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:50:58] Epoch 3\\Batch 44600\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:03] Epoch 3\\Batch 44650\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:09] Epoch 3\\Batch 44700\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:14] Epoch 3\\Batch 44750\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:19] Epoch 3\\Batch 44800\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:25] Epoch 3\\Batch 44850\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:30] Epoch 3\\Batch 44900\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:35] Epoch 3\\Batch 44950\\ Train Loss:10.466\\ Learning rate:0.00030\n",
      "[2019/03/17 21:51:39] Epoch 3\\Batch 45000\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5506.000001, 'TP': 1493.0000009999999, 'FP': 1693.0000009999999}\n",
      "[2019/03/17 21:52:02] Epoch 3/ Validation Loss:9.987/ F1_score:0.293/ Precision:0.469/ Recall:0.213\n",
      "[2019/03/17 21:52:05] Epoch 3\\Batch 45050\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:08] Epoch 3\\Batch 45100\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:12] Epoch 3\\Batch 45150\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:15] Epoch 3\\Batch 45200\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:18] Epoch 3\\Batch 45250\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:21] Epoch 3\\Batch 45300\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:24] Epoch 3\\Batch 45350\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:28] Epoch 3\\Batch 45400\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:33] Epoch 3\\Batch 45450\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:38] Epoch 3\\Batch 45500\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:42] Epoch 3\\Batch 45550\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:47] Epoch 3\\Batch 45600\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:52] Epoch 3\\Batch 45650\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:52:57] Epoch 3\\Batch 45700\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:02] Epoch 3\\Batch 45750\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:07] Epoch 3\\Batch 45800\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:12] Epoch 3\\Batch 45850\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:17] Epoch 3\\Batch 45900\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:22] Epoch 3\\Batch 45950\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:26] Epoch 3\\Batch 46000\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:32] Epoch 3\\Batch 46050\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:36] Epoch 3\\Batch 46100\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:41] Epoch 3\\Batch 46150\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:46] Epoch 3\\Batch 46200\\ Train Loss:10.464\\ Learning rate:0.00030\n",
      "[2019/03/17 21:53:56] Epoch 4\\Batch 50\\ Train Loss:10.428\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:01] Epoch 4\\Batch 100\\ Train Loss:10.492\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 21:54:06] Epoch 4\\Batch 150\\ Train Loss:10.513\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:11] Epoch 4\\Batch 200\\ Train Loss:10.477\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:16] Epoch 4\\Batch 250\\ Train Loss:10.462\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:21] Epoch 4\\Batch 300\\ Train Loss:10.478\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:26] Epoch 4\\Batch 350\\ Train Loss:10.484\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:31] Epoch 4\\Batch 400\\ Train Loss:10.486\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:36] Epoch 4\\Batch 450\\ Train Loss:10.465\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:41] Epoch 4\\Batch 500\\ Train Loss:10.453\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:46] Epoch 4\\Batch 550\\ Train Loss:10.439\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:51] Epoch 4\\Batch 600\\ Train Loss:10.446\\ Learning rate:0.00030\n",
      "[2019/03/17 21:54:56] Epoch 4\\Batch 650\\ Train Loss:10.443\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:01] Epoch 4\\Batch 700\\ Train Loss:10.449\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:06] Epoch 4\\Batch 750\\ Train Loss:10.445\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:12] Epoch 4\\Batch 800\\ Train Loss:10.439\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:17] Epoch 4\\Batch 850\\ Train Loss:10.442\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:22] Epoch 4\\Batch 900\\ Train Loss:10.449\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:28] Epoch 4\\Batch 950\\ Train Loss:10.440\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:33] Epoch 4\\Batch 1000\\ Train Loss:10.433\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:38] Epoch 4\\Batch 1050\\ Train Loss:10.432\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:43] Epoch 4\\Batch 1100\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:49] Epoch 4\\Batch 1150\\ Train Loss:10.432\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:54] Epoch 4\\Batch 1200\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:55:59] Epoch 4\\Batch 1250\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:05] Epoch 4\\Batch 1300\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:10] Epoch 4\\Batch 1350\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:15] Epoch 4\\Batch 1400\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:20] Epoch 4\\Batch 1450\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:25] Epoch 4\\Batch 1500\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:30] Epoch 4\\Batch 1550\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:36] Epoch 4\\Batch 1600\\ Train Loss:10.414\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:41] Epoch 4\\Batch 1650\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:46] Epoch 4\\Batch 1700\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:51] Epoch 4\\Batch 1750\\ Train Loss:10.414\\ Learning rate:0.00030\n",
      "[2019/03/17 21:56:57] Epoch 4\\Batch 1800\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:02] Epoch 4\\Batch 1850\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:07] Epoch 4\\Batch 1900\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:12] Epoch 4\\Batch 1950\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:17] Epoch 4\\Batch 2000\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:22] Epoch 4\\Batch 2050\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:27] Epoch 4\\Batch 2100\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:32] Epoch 4\\Batch 2150\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:37] Epoch 4\\Batch 2200\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:42] Epoch 4\\Batch 2250\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:47] Epoch 4\\Batch 2300\\ Train Loss:10.428\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:53] Epoch 4\\Batch 2350\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 21:57:58] Epoch 4\\Batch 2400\\ Train Loss:10.427\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:03] Epoch 4\\Batch 2450\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:09] Epoch 4\\Batch 2500\\ Train Loss:10.429\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:14] Epoch 4\\Batch 2550\\ Train Loss:10.428\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:19] Epoch 4\\Batch 2600\\ Train Loss:10.427\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:25] Epoch 4\\Batch 2650\\ Train Loss:10.428\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:30] Epoch 4\\Batch 2700\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:35] Epoch 4\\Batch 2750\\ Train Loss:10.431\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:41] Epoch 4\\Batch 2800\\ Train Loss:10.430\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:46] Epoch 4\\Batch 2850\\ Train Loss:10.429\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:52] Epoch 4\\Batch 2900\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:58:57] Epoch 4\\Batch 2950\\ Train Loss:10.427\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:02] Epoch 4\\Batch 3000\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5506.000001, 'TP': 1493.0000009999999, 'FP': 1673.0000009999999}\n",
      "[2019/03/17 21:59:25] Epoch 4/ Validation Loss:9.988/ F1_score:0.294/ Precision:0.472/ Recall:0.213\n",
      "[2019/03/17 21:59:28] Epoch 4\\Batch 3050\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:31] Epoch 4\\Batch 3100\\ Train Loss:10.428\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:35] Epoch 4\\Batch 3150\\ Train Loss:10.428\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:38] Epoch 4\\Batch 3200\\ Train Loss:10.426\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:41] Epoch 4\\Batch 3250\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:45] Epoch 4\\Batch 3300\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:49] Epoch 4\\Batch 3350\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:54] Epoch 4\\Batch 3400\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 21:59:59] Epoch 4\\Batch 3450\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:04] Epoch 4\\Batch 3500\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:10] Epoch 4\\Batch 3550\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:15] Epoch 4\\Batch 3600\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:20] Epoch 4\\Batch 3650\\ Train Loss:10.414\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:25] Epoch 4\\Batch 3700\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:30] Epoch 4\\Batch 3750\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:35] Epoch 4\\Batch 3800\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:41] Epoch 4\\Batch 3850\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:46] Epoch 4\\Batch 3900\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:51] Epoch 4\\Batch 3950\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:00:56] Epoch 4\\Batch 4000\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:01] Epoch 4\\Batch 4050\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:06] Epoch 4\\Batch 4100\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:11] Epoch 4\\Batch 4150\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:16] Epoch 4\\Batch 4200\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:21] Epoch 4\\Batch 4250\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:26] Epoch 4\\Batch 4300\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:31] Epoch 4\\Batch 4350\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:36] Epoch 4\\Batch 4400\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:41] Epoch 4\\Batch 4450\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:46] Epoch 4\\Batch 4500\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:50] Epoch 4\\Batch 4550\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:01:55] Epoch 4\\Batch 4600\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:00] Epoch 4\\Batch 4650\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:05] Epoch 4\\Batch 4700\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:10] Epoch 4\\Batch 4750\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:15] Epoch 4\\Batch 4800\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:20] Epoch 4\\Batch 4850\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:25] Epoch 4\\Batch 4900\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:31] Epoch 4\\Batch 4950\\ Train Loss:10.419\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 22:02:36] Epoch 4\\Batch 5000\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:41] Epoch 4\\Batch 5050\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:47] Epoch 4\\Batch 5100\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:52] Epoch 4\\Batch 5150\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:02:57] Epoch 4\\Batch 5200\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:03] Epoch 4\\Batch 5250\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:07] Epoch 4\\Batch 5300\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:13] Epoch 4\\Batch 5350\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:18] Epoch 4\\Batch 5400\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:23] Epoch 4\\Batch 5450\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:28] Epoch 4\\Batch 5500\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:33] Epoch 4\\Batch 5550\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:38] Epoch 4\\Batch 5600\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:44] Epoch 4\\Batch 5650\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:49] Epoch 4\\Batch 5700\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:54] Epoch 4\\Batch 5750\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:03:59] Epoch 4\\Batch 5800\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:04:04] Epoch 4\\Batch 5850\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 22:04:10] Epoch 4\\Batch 5900\\ Train Loss:10.424\\ Learning rate:0.00030\n",
      "[2019/03/17 22:04:15] Epoch 4\\Batch 5950\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:04:20] Epoch 4\\Batch 6000\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5503.000001, 'TP': 1496.0000009999999, 'FP': 1686.0000009999999}\n",
      "[2019/03/17 22:04:43] Epoch 4/ Validation Loss:9.980/ F1_score:0.294/ Precision:0.470/ Recall:0.214\n",
      "[2019/03/17 22:04:48] Epoch 4\\Batch 6050\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:04:53] Epoch 4\\Batch 6100\\ Train Loss:10.424\\ Learning rate:0.00030\n",
      "[2019/03/17 22:04:58] Epoch 4\\Batch 6150\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:03] Epoch 4\\Batch 6200\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:08] Epoch 4\\Batch 6250\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:13] Epoch 4\\Batch 6300\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:18] Epoch 4\\Batch 6350\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:23] Epoch 4\\Batch 6400\\ Train Loss:10.425\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:28] Epoch 4\\Batch 6450\\ Train Loss:10.424\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:33] Epoch 4\\Batch 6500\\ Train Loss:10.424\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:38] Epoch 4\\Batch 6550\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:43] Epoch 4\\Batch 6600\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:48] Epoch 4\\Batch 6650\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:53] Epoch 4\\Batch 6700\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:05:58] Epoch 4\\Batch 6750\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:03] Epoch 4\\Batch 6800\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:08] Epoch 4\\Batch 6850\\ Train Loss:10.423\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:13] Epoch 4\\Batch 6900\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:18] Epoch 4\\Batch 6950\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:23] Epoch 4\\Batch 7000\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:28] Epoch 4\\Batch 7050\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:32] Epoch 4\\Batch 7100\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:37] Epoch 4\\Batch 7150\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:42] Epoch 4\\Batch 7200\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:46] Epoch 4\\Batch 7250\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:49] Epoch 4\\Batch 7300\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:53] Epoch 4\\Batch 7350\\ Train Loss:10.422\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:56] Epoch 4\\Batch 7400\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:06:59] Epoch 4\\Batch 7450\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:02] Epoch 4\\Batch 7500\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:06] Epoch 4\\Batch 7550\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:09] Epoch 4\\Batch 7600\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:14] Epoch 4\\Batch 7650\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:19] Epoch 4\\Batch 7700\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:24] Epoch 4\\Batch 7750\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:29] Epoch 4\\Batch 7800\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:34] Epoch 4\\Batch 7850\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:39] Epoch 4\\Batch 7900\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:44] Epoch 4\\Batch 7950\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:49] Epoch 4\\Batch 8000\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:54] Epoch 4\\Batch 8050\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:07:59] Epoch 4\\Batch 8100\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:04] Epoch 4\\Batch 8150\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:09] Epoch 4\\Batch 8200\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:14] Epoch 4\\Batch 8250\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:19] Epoch 4\\Batch 8300\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:25] Epoch 4\\Batch 8350\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:30] Epoch 4\\Batch 8400\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:35] Epoch 4\\Batch 8450\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:40] Epoch 4\\Batch 8500\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:45] Epoch 4\\Batch 8550\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:50] Epoch 4\\Batch 8600\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:08:56] Epoch 4\\Batch 8650\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:01] Epoch 4\\Batch 8700\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:06] Epoch 4\\Batch 8750\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:11] Epoch 4\\Batch 8800\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:16] Epoch 4\\Batch 8850\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:21] Epoch 4\\Batch 8900\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:26] Epoch 4\\Batch 8950\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:09:32] Epoch 4\\Batch 9000\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5522.000001, 'TP': 1477.0000009999999, 'FP': 1706.0000009999999}\n",
      "[2019/03/17 22:09:54] Epoch 4/ Validation Loss:9.984/ F1_score:0.290/ Precision:0.464/ Recall:0.211\n",
      "[2019/03/17 22:10:00] Epoch 4\\Batch 9050\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:05] Epoch 4\\Batch 9100\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:10] Epoch 4\\Batch 9150\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:15] Epoch 4\\Batch 9200\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:20] Epoch 4\\Batch 9250\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:25] Epoch 4\\Batch 9300\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:30] Epoch 4\\Batch 9350\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:35] Epoch 4\\Batch 9400\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:40] Epoch 4\\Batch 9450\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:45] Epoch 4\\Batch 9500\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:50] Epoch 4\\Batch 9550\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:10:55] Epoch 4\\Batch 9600\\ Train Loss:10.419\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 22:11:00] Epoch 4\\Batch 9650\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:05] Epoch 4\\Batch 9700\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:10] Epoch 4\\Batch 9750\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:15] Epoch 4\\Batch 9800\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:20] Epoch 4\\Batch 9850\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:25] Epoch 4\\Batch 9900\\ Train Loss:10.421\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:30] Epoch 4\\Batch 9950\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:35] Epoch 4\\Batch 10000\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:40] Epoch 4\\Batch 10050\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:45] Epoch 4\\Batch 10100\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:51] Epoch 4\\Batch 10150\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:11:56] Epoch 4\\Batch 10200\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:01] Epoch 4\\Batch 10250\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:06] Epoch 4\\Batch 10300\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:11] Epoch 4\\Batch 10350\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:16] Epoch 4\\Batch 10400\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:21] Epoch 4\\Batch 10450\\ Train Loss:10.420\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:26] Epoch 4\\Batch 10500\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:31] Epoch 4\\Batch 10550\\ Train Loss:10.419\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:36] Epoch 4\\Batch 10600\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:40] Epoch 4\\Batch 10650\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:45] Epoch 4\\Batch 10700\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:50] Epoch 4\\Batch 10750\\ Train Loss:10.418\\ Learning rate:0.00030\n",
      "[2019/03/17 22:12:55] Epoch 4\\Batch 10800\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:00] Epoch 4\\Batch 10850\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:06] Epoch 4\\Batch 10900\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:10] Epoch 4\\Batch 10950\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:16] Epoch 4\\Batch 11000\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:21] Epoch 4\\Batch 11050\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:26] Epoch 4\\Batch 11100\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:31] Epoch 4\\Batch 11150\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:36] Epoch 4\\Batch 11200\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:41] Epoch 4\\Batch 11250\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:47] Epoch 4\\Batch 11300\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:52] Epoch 4\\Batch 11350\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:13:57] Epoch 4\\Batch 11400\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:00] Epoch 4\\Batch 11450\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:04] Epoch 4\\Batch 11500\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:07] Epoch 4\\Batch 11550\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:11] Epoch 4\\Batch 11600\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:14] Epoch 4\\Batch 11650\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:17] Epoch 4\\Batch 11700\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:21] Epoch 4\\Batch 11750\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:25] Epoch 4\\Batch 11800\\ Train Loss:10.417\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:30] Epoch 4\\Batch 11850\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:35] Epoch 4\\Batch 11900\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:40] Epoch 4\\Batch 11950\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:14:45] Epoch 4\\Batch 12000\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5485.000001, 'TP': 1514.0000009999999, 'FP': 1679.0000009999999}\n",
      "[2019/03/17 22:15:08] Epoch 4/ Validation Loss:9.957/ F1_score:0.297/ Precision:0.474/ Recall:0.216\n",
      "[2019/03/17 22:15:14] Epoch 4\\Batch 12050\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:19] Epoch 4\\Batch 12100\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:24] Epoch 4\\Batch 12150\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:29] Epoch 4\\Batch 12200\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:34] Epoch 4\\Batch 12250\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:39] Epoch 4\\Batch 12300\\ Train Loss:10.414\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:44] Epoch 4\\Batch 12350\\ Train Loss:10.414\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:49] Epoch 4\\Batch 12400\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:54] Epoch 4\\Batch 12450\\ Train Loss:10.414\\ Learning rate:0.00030\n",
      "[2019/03/17 22:15:59] Epoch 4\\Batch 12500\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:04] Epoch 4\\Batch 12550\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:10] Epoch 4\\Batch 12600\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:15] Epoch 4\\Batch 12650\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:19] Epoch 4\\Batch 12700\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:25] Epoch 4\\Batch 12750\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:30] Epoch 4\\Batch 12800\\ Train Loss:10.413\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:35] Epoch 4\\Batch 12850\\ Train Loss:10.412\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:40] Epoch 4\\Batch 12900\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:45] Epoch 4\\Batch 12950\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:50] Epoch 4\\Batch 13000\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:16:55] Epoch 4\\Batch 13050\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:00] Epoch 4\\Batch 13100\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:04] Epoch 4\\Batch 13150\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:10] Epoch 4\\Batch 13200\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:15] Epoch 4\\Batch 13250\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:20] Epoch 4\\Batch 13300\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:24] Epoch 4\\Batch 13350\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:29] Epoch 4\\Batch 13400\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:34] Epoch 4\\Batch 13450\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:39] Epoch 4\\Batch 13500\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:44] Epoch 4\\Batch 13550\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:49] Epoch 4\\Batch 13600\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:54] Epoch 4\\Batch 13650\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:17:59] Epoch 4\\Batch 13700\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:04] Epoch 4\\Batch 13750\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:09] Epoch 4\\Batch 13800\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:14] Epoch 4\\Batch 13850\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:20] Epoch 4\\Batch 13900\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:24] Epoch 4\\Batch 13950\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:30] Epoch 4\\Batch 14000\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:35] Epoch 4\\Batch 14050\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:40] Epoch 4\\Batch 14100\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:45] Epoch 4\\Batch 14150\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:50] Epoch 4\\Batch 14200\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:18:55] Epoch 4\\Batch 14250\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:00] Epoch 4\\Batch 14300\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:05] Epoch 4\\Batch 14350\\ Train Loss:10.410\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 22:19:10] Epoch 4\\Batch 14400\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:15] Epoch 4\\Batch 14450\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:20] Epoch 4\\Batch 14500\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:25] Epoch 4\\Batch 14550\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:30] Epoch 4\\Batch 14600\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:35] Epoch 4\\Batch 14650\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:40] Epoch 4\\Batch 14700\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:45] Epoch 4\\Batch 14750\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:50] Epoch 4\\Batch 14800\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:19:55] Epoch 4\\Batch 14850\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:00] Epoch 4\\Batch 14900\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:05] Epoch 4\\Batch 14950\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:10] Epoch 4\\Batch 15000\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5509.000001, 'TP': 1490.0000009999999, 'FP': 1685.0000009999999}\n",
      "[2019/03/17 22:20:33] Epoch 4/ Validation Loss:9.951/ F1_score:0.293/ Precision:0.469/ Recall:0.213\n",
      "[2019/03/17 22:20:39] Epoch 4\\Batch 15050\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:44] Epoch 4\\Batch 15100\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:49] Epoch 4\\Batch 15150\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:54] Epoch 4\\Batch 15200\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:20:59] Epoch 4\\Batch 15250\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:02] Epoch 4\\Batch 15300\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:06] Epoch 4\\Batch 15350\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:09] Epoch 4\\Batch 15400\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:12] Epoch 4\\Batch 15450\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:16] Epoch 4\\Batch 15500\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:19] Epoch 4\\Batch 15550\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:23] Epoch 4\\Batch 15600\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:28] Epoch 4\\Batch 15650\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:33] Epoch 4\\Batch 15700\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:38] Epoch 4\\Batch 15750\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:44] Epoch 4\\Batch 15800\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:49] Epoch 4\\Batch 15850\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:54] Epoch 4\\Batch 15900\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:21:59] Epoch 4\\Batch 15950\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:04] Epoch 4\\Batch 16000\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:09] Epoch 4\\Batch 16050\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:15] Epoch 4\\Batch 16100\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:20] Epoch 4\\Batch 16150\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:25] Epoch 4\\Batch 16200\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:30] Epoch 4\\Batch 16250\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:36] Epoch 4\\Batch 16300\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:41] Epoch 4\\Batch 16350\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:46] Epoch 4\\Batch 16400\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:52] Epoch 4\\Batch 16450\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:22:57] Epoch 4\\Batch 16500\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:02] Epoch 4\\Batch 16550\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:07] Epoch 4\\Batch 16600\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:13] Epoch 4\\Batch 16650\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:18] Epoch 4\\Batch 16700\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:23] Epoch 4\\Batch 16750\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:28] Epoch 4\\Batch 16800\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:33] Epoch 4\\Batch 16850\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:38] Epoch 4\\Batch 16900\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:43] Epoch 4\\Batch 16950\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:48] Epoch 4\\Batch 17000\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:53] Epoch 4\\Batch 17050\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:23:58] Epoch 4\\Batch 17100\\ Train Loss:10.409\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:03] Epoch 4\\Batch 17150\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:08] Epoch 4\\Batch 17200\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:13] Epoch 4\\Batch 17250\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:18] Epoch 4\\Batch 17300\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:23] Epoch 4\\Batch 17350\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:28] Epoch 4\\Batch 17400\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:33] Epoch 4\\Batch 17450\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:38] Epoch 4\\Batch 17500\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:43] Epoch 4\\Batch 17550\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:48] Epoch 4\\Batch 17600\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:53] Epoch 4\\Batch 17650\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:24:58] Epoch 4\\Batch 17700\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:25:03] Epoch 4\\Batch 17750\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:25:08] Epoch 4\\Batch 17800\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:25:13] Epoch 4\\Batch 17850\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:25:18] Epoch 4\\Batch 17900\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:25:23] Epoch 4\\Batch 17950\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:25:28] Epoch 4\\Batch 18000\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5489.000001, 'TP': 1510.0000009999999, 'FP': 1691.0000009999999}\n",
      "[2019/03/17 22:25:52] Epoch 4/ Validation Loss:9.944/ F1_score:0.296/ Precision:0.472/ Recall:0.216\n",
      "[2019/03/17 22:25:57] Epoch 4\\Batch 18050\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:02] Epoch 4\\Batch 18100\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:08] Epoch 4\\Batch 18150\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:13] Epoch 4\\Batch 18200\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:18] Epoch 4\\Batch 18250\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:23] Epoch 4\\Batch 18300\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:28] Epoch 4\\Batch 18350\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:34] Epoch 4\\Batch 18400\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:39] Epoch 4\\Batch 18450\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:44] Epoch 4\\Batch 18500\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:50] Epoch 4\\Batch 18550\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:26:55] Epoch 4\\Batch 18600\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:00] Epoch 4\\Batch 18650\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:05] Epoch 4\\Batch 18700\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:11] Epoch 4\\Batch 18750\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:16] Epoch 4\\Batch 18800\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:21] Epoch 4\\Batch 18850\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:26] Epoch 4\\Batch 18900\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:32] Epoch 4\\Batch 18950\\ Train Loss:10.405\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 22:27:37] Epoch 4\\Batch 19000\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:43] Epoch 4\\Batch 19050\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:48] Epoch 4\\Batch 19100\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:53] Epoch 4\\Batch 19150\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:27:58] Epoch 4\\Batch 19200\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:03] Epoch 4\\Batch 19250\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:08] Epoch 4\\Batch 19300\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:13] Epoch 4\\Batch 19350\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:18] Epoch 4\\Batch 19400\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:22] Epoch 4\\Batch 19450\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:26] Epoch 4\\Batch 19500\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:29] Epoch 4\\Batch 19550\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:33] Epoch 4\\Batch 19600\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:36] Epoch 4\\Batch 19650\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:40] Epoch 4\\Batch 19700\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:43] Epoch 4\\Batch 19750\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:47] Epoch 4\\Batch 19800\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:52] Epoch 4\\Batch 19850\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:28:57] Epoch 4\\Batch 19900\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:02] Epoch 4\\Batch 19950\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:07] Epoch 4\\Batch 20000\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:12] Epoch 4\\Batch 20050\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:17] Epoch 4\\Batch 20100\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:23] Epoch 4\\Batch 20150\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:28] Epoch 4\\Batch 20200\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:33] Epoch 4\\Batch 20250\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:38] Epoch 4\\Batch 20300\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:43] Epoch 4\\Batch 20350\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:48] Epoch 4\\Batch 20400\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:53] Epoch 4\\Batch 20450\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:29:58] Epoch 4\\Batch 20500\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:03] Epoch 4\\Batch 20550\\ Train Loss:10.408\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:08] Epoch 4\\Batch 20600\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:13] Epoch 4\\Batch 20650\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:18] Epoch 4\\Batch 20700\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:23] Epoch 4\\Batch 20750\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:28] Epoch 4\\Batch 20800\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:33] Epoch 4\\Batch 20850\\ Train Loss:10.407\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:38] Epoch 4\\Batch 20900\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:43] Epoch 4\\Batch 20950\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:30:48] Epoch 4\\Batch 21000\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5482.000001, 'TP': 1517.0000009999999, 'FP': 1680.0000009999999}\n",
      "[2019/03/17 22:31:11] Epoch 4/ Validation Loss:9.932/ F1_score:0.298/ Precision:0.475/ Recall:0.217\n",
      "[2019/03/17 22:31:16] Epoch 4\\Batch 21050\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:21] Epoch 4\\Batch 21100\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:27] Epoch 4\\Batch 21150\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:32] Epoch 4\\Batch 21200\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:37] Epoch 4\\Batch 21250\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:42] Epoch 4\\Batch 21300\\ Train Loss:10.406\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:47] Epoch 4\\Batch 21350\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:52] Epoch 4\\Batch 21400\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:31:58] Epoch 4\\Batch 21450\\ Train Loss:10.405\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:03] Epoch 4\\Batch 21500\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:08] Epoch 4\\Batch 21550\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:13] Epoch 4\\Batch 21600\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:18] Epoch 4\\Batch 21650\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:23] Epoch 4\\Batch 21700\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:29] Epoch 4\\Batch 21750\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:34] Epoch 4\\Batch 21800\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:39] Epoch 4\\Batch 21850\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:44] Epoch 4\\Batch 21900\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:49] Epoch 4\\Batch 21950\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:32:54] Epoch 4\\Batch 22000\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:00] Epoch 4\\Batch 22050\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:05] Epoch 4\\Batch 22100\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:10] Epoch 4\\Batch 22150\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:15] Epoch 4\\Batch 22200\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:20] Epoch 4\\Batch 22250\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:25] Epoch 4\\Batch 22300\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:30] Epoch 4\\Batch 22350\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:35] Epoch 4\\Batch 22400\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:40] Epoch 4\\Batch 22450\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:45] Epoch 4\\Batch 22500\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:50] Epoch 4\\Batch 22550\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:33:55] Epoch 4\\Batch 22600\\ Train Loss:10.404\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:00] Epoch 4\\Batch 22650\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:05] Epoch 4\\Batch 22700\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:10] Epoch 4\\Batch 22750\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:15] Epoch 4\\Batch 22800\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:20] Epoch 4\\Batch 22850\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:25] Epoch 4\\Batch 22900\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:30] Epoch 4\\Batch 22950\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:35] Epoch 4\\Batch 23000\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:40] Epoch 4\\Batch 23050\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:45] Epoch 4\\Batch 23100\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:50] Epoch 4\\Batch 23150\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:54] Epoch 4\\Batch 23200\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:34:59] Epoch 4\\Batch 23250\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:04] Epoch 4\\Batch 23300\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:09] Epoch 4\\Batch 23350\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:14] Epoch 4\\Batch 23400\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:19] Epoch 4\\Batch 23450\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:24] Epoch 4\\Batch 23500\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:29] Epoch 4\\Batch 23550\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:34] Epoch 4\\Batch 23600\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:39] Epoch 4\\Batch 23650\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:43] Epoch 4\\Batch 23700\\ Train Loss:10.402\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 22:35:46] Epoch 4\\Batch 23750\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:49] Epoch 4\\Batch 23800\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:53] Epoch 4\\Batch 23850\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:56] Epoch 4\\Batch 23900\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:35:59] Epoch 4\\Batch 23950\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:02] Epoch 4\\Batch 24000\\ Train Loss:10.403\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5479.000001, 'TP': 1520.0000009999999, 'FP': 1663.0000009999999}\n",
      "[2019/03/17 22:36:24] Epoch 4/ Validation Loss:9.915/ F1_score:0.299/ Precision:0.478/ Recall:0.217\n",
      "[2019/03/17 22:36:29] Epoch 4\\Batch 24050\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:34] Epoch 4\\Batch 24100\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:39] Epoch 4\\Batch 24150\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:44] Epoch 4\\Batch 24200\\ Train Loss:10.402\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:49] Epoch 4\\Batch 24250\\ Train Loss:10.401\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:54] Epoch 4\\Batch 24300\\ Train Loss:10.401\\ Learning rate:0.00030\n",
      "[2019/03/17 22:36:59] Epoch 4\\Batch 24350\\ Train Loss:10.401\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:04] Epoch 4\\Batch 24400\\ Train Loss:10.401\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:09] Epoch 4\\Batch 24450\\ Train Loss:10.401\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:14] Epoch 4\\Batch 24500\\ Train Loss:10.400\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:19] Epoch 4\\Batch 24550\\ Train Loss:10.400\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:24] Epoch 4\\Batch 24600\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:29] Epoch 4\\Batch 24650\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:34] Epoch 4\\Batch 24700\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:39] Epoch 4\\Batch 24750\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:44] Epoch 4\\Batch 24800\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:49] Epoch 4\\Batch 24850\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:54] Epoch 4\\Batch 24900\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:37:59] Epoch 4\\Batch 24950\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:04] Epoch 4\\Batch 25000\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:09] Epoch 4\\Batch 25050\\ Train Loss:10.399\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:14] Epoch 4\\Batch 25100\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:19] Epoch 4\\Batch 25150\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:24] Epoch 4\\Batch 25200\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:29] Epoch 4\\Batch 25250\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:34] Epoch 4\\Batch 25300\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:39] Epoch 4\\Batch 25350\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:44] Epoch 4\\Batch 25400\\ Train Loss:10.398\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:49] Epoch 4\\Batch 25450\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:54] Epoch 4\\Batch 25500\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 22:38:59] Epoch 4\\Batch 25550\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:04] Epoch 4\\Batch 25600\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:09] Epoch 4\\Batch 25650\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:14] Epoch 4\\Batch 25700\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:19] Epoch 4\\Batch 25750\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:24] Epoch 4\\Batch 25800\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:29] Epoch 4\\Batch 25850\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:35] Epoch 4\\Batch 25900\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:40] Epoch 4\\Batch 25950\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:45] Epoch 4\\Batch 26000\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:50] Epoch 4\\Batch 26050\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:39:55] Epoch 4\\Batch 26100\\ Train Loss:10.396\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:00] Epoch 4\\Batch 26150\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:05] Epoch 4\\Batch 26200\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:10] Epoch 4\\Batch 26250\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:15] Epoch 4\\Batch 26300\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:20] Epoch 4\\Batch 26350\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:25] Epoch 4\\Batch 26400\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:30] Epoch 4\\Batch 26450\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:35] Epoch 4\\Batch 26500\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:40] Epoch 4\\Batch 26550\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:45] Epoch 4\\Batch 26600\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:50] Epoch 4\\Batch 26650\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:40:55] Epoch 4\\Batch 26700\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:00] Epoch 4\\Batch 26750\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:05] Epoch 4\\Batch 26800\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:10] Epoch 4\\Batch 26850\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:16] Epoch 4\\Batch 26900\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:21] Epoch 4\\Batch 26950\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:26] Epoch 4\\Batch 27000\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5494.000001, 'TP': 1505.0000009999999, 'FP': 1677.0000009999999}\n",
      "[2019/03/17 22:41:48] Epoch 4/ Validation Loss:9.926/ F1_score:0.296/ Precision:0.473/ Recall:0.215\n",
      "[2019/03/17 22:41:54] Epoch 4\\Batch 27050\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:41:59] Epoch 4\\Batch 27100\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:04] Epoch 4\\Batch 27150\\ Train Loss:10.395\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:08] Epoch 4\\Batch 27200\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:13] Epoch 4\\Batch 27250\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:18] Epoch 4\\Batch 27300\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:23] Epoch 4\\Batch 27350\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:28] Epoch 4\\Batch 27400\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:33] Epoch 4\\Batch 27450\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:38] Epoch 4\\Batch 27500\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:43] Epoch 4\\Batch 27550\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:46] Epoch 4\\Batch 27600\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:49] Epoch 4\\Batch 27650\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:53] Epoch 4\\Batch 27700\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:56] Epoch 4\\Batch 27750\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:42:59] Epoch 4\\Batch 27800\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:02] Epoch 4\\Batch 27850\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:06] Epoch 4\\Batch 27900\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:11] Epoch 4\\Batch 27950\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:16] Epoch 4\\Batch 28000\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:20] Epoch 4\\Batch 28050\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:25] Epoch 4\\Batch 28100\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:30] Epoch 4\\Batch 28150\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:35] Epoch 4\\Batch 28200\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:40] Epoch 4\\Batch 28250\\ Train Loss:10.394\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:45] Epoch 4\\Batch 28300\\ Train Loss:10.394\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 22:43:50] Epoch 4\\Batch 28350\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:43:55] Epoch 4\\Batch 28400\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:00] Epoch 4\\Batch 28450\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:05] Epoch 4\\Batch 28500\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:10] Epoch 4\\Batch 28550\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:15] Epoch 4\\Batch 28600\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:20] Epoch 4\\Batch 28650\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:25] Epoch 4\\Batch 28700\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:30] Epoch 4\\Batch 28750\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:35] Epoch 4\\Batch 28800\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:40] Epoch 4\\Batch 28850\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:45] Epoch 4\\Batch 28900\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:49] Epoch 4\\Batch 28950\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:54] Epoch 4\\Batch 29000\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:44:59] Epoch 4\\Batch 29050\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:05] Epoch 4\\Batch 29100\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:10] Epoch 4\\Batch 29150\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:15] Epoch 4\\Batch 29200\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:19] Epoch 4\\Batch 29250\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:24] Epoch 4\\Batch 29300\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:29] Epoch 4\\Batch 29350\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:34] Epoch 4\\Batch 29400\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:39] Epoch 4\\Batch 29450\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:44] Epoch 4\\Batch 29500\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:49] Epoch 4\\Batch 29550\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:54] Epoch 4\\Batch 29600\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:45:59] Epoch 4\\Batch 29650\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:04] Epoch 4\\Batch 29700\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:09] Epoch 4\\Batch 29750\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:14] Epoch 4\\Batch 29800\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:19] Epoch 4\\Batch 29850\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:24] Epoch 4\\Batch 29900\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:29] Epoch 4\\Batch 29950\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:46:34] Epoch 4\\Batch 30000\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5481.000001, 'TP': 1518.0000009999999, 'FP': 1672.0000009999999}\n",
      "[2019/03/17 22:46:57] Epoch 4/ Validation Loss:9.928/ F1_score:0.298/ Precision:0.476/ Recall:0.217\n",
      "[2019/03/17 22:47:02] Epoch 4\\Batch 30050\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:07] Epoch 4\\Batch 30100\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:12] Epoch 4\\Batch 30150\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:17] Epoch 4\\Batch 30200\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:22] Epoch 4\\Batch 30250\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:27] Epoch 4\\Batch 30300\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:32] Epoch 4\\Batch 30350\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:37] Epoch 4\\Batch 30400\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:42] Epoch 4\\Batch 30450\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:47] Epoch 4\\Batch 30500\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:51] Epoch 4\\Batch 30550\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:47:56] Epoch 4\\Batch 30600\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:01] Epoch 4\\Batch 30650\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:06] Epoch 4\\Batch 30700\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:11] Epoch 4\\Batch 30750\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:16] Epoch 4\\Batch 30800\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:21] Epoch 4\\Batch 30850\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:27] Epoch 4\\Batch 30900\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:32] Epoch 4\\Batch 30950\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:37] Epoch 4\\Batch 31000\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:42] Epoch 4\\Batch 31050\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:47] Epoch 4\\Batch 31100\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:52] Epoch 4\\Batch 31150\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:48:57] Epoch 4\\Batch 31200\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:03] Epoch 4\\Batch 31250\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:08] Epoch 4\\Batch 31300\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:13] Epoch 4\\Batch 31350\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:18] Epoch 4\\Batch 31400\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:23] Epoch 4\\Batch 31450\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:28] Epoch 4\\Batch 31500\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:33] Epoch 4\\Batch 31550\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:38] Epoch 4\\Batch 31600\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:44] Epoch 4\\Batch 31650\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:49] Epoch 4\\Batch 31700\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:54] Epoch 4\\Batch 31750\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:49:58] Epoch 4\\Batch 31800\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:02] Epoch 4\\Batch 31850\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:05] Epoch 4\\Batch 31900\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:09] Epoch 4\\Batch 31950\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:12] Epoch 4\\Batch 32000\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:15] Epoch 4\\Batch 32050\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:19] Epoch 4\\Batch 32100\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:22] Epoch 4\\Batch 32150\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:27] Epoch 4\\Batch 32200\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:32] Epoch 4\\Batch 32250\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:37] Epoch 4\\Batch 32300\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:42] Epoch 4\\Batch 32350\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:47] Epoch 4\\Batch 32400\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:52] Epoch 4\\Batch 32450\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:50:58] Epoch 4\\Batch 32500\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:03] Epoch 4\\Batch 32550\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:08] Epoch 4\\Batch 32600\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:13] Epoch 4\\Batch 32650\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:18] Epoch 4\\Batch 32700\\ Train Loss:10.393\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:23] Epoch 4\\Batch 32750\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:28] Epoch 4\\Batch 32800\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:33] Epoch 4\\Batch 32850\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:39] Epoch 4\\Batch 32900\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:44] Epoch 4\\Batch 32950\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:51:49] Epoch 4\\Batch 33000\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5466.000001, 'TP': 1533.0000009999999, 'FP': 1662.0000009999999}\n",
      "[2019/03/17 22:52:12] Epoch 4/ Validation Loss:9.933/ F1_score:0.301/ Precision:0.480/ Recall:0.219\n",
      "[2019/03/17 22:52:18] Epoch 4\\Batch 33050\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:23] Epoch 4\\Batch 33100\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:28] Epoch 4\\Batch 33150\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:33] Epoch 4\\Batch 33200\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:39] Epoch 4\\Batch 33250\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:44] Epoch 4\\Batch 33300\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:49] Epoch 4\\Batch 33350\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:52:55] Epoch 4\\Batch 33400\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:00] Epoch 4\\Batch 33450\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:05] Epoch 4\\Batch 33500\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:10] Epoch 4\\Batch 33550\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:15] Epoch 4\\Batch 33600\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:20] Epoch 4\\Batch 33650\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:26] Epoch 4\\Batch 33700\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:31] Epoch 4\\Batch 33750\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:36] Epoch 4\\Batch 33800\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:41] Epoch 4\\Batch 33850\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:46] Epoch 4\\Batch 33900\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:52] Epoch 4\\Batch 33950\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:53:57] Epoch 4\\Batch 34000\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:02] Epoch 4\\Batch 34050\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:07] Epoch 4\\Batch 34100\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:11] Epoch 4\\Batch 34150\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:16] Epoch 4\\Batch 34200\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:21] Epoch 4\\Batch 34250\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:26] Epoch 4\\Batch 34300\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:31] Epoch 4\\Batch 34350\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:36] Epoch 4\\Batch 34400\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:41] Epoch 4\\Batch 34450\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:46] Epoch 4\\Batch 34500\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:52] Epoch 4\\Batch 34550\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:54:57] Epoch 4\\Batch 34600\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:02] Epoch 4\\Batch 34650\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:07] Epoch 4\\Batch 34700\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:12] Epoch 4\\Batch 34750\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:17] Epoch 4\\Batch 34800\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:22] Epoch 4\\Batch 34850\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:27] Epoch 4\\Batch 34900\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:32] Epoch 4\\Batch 34950\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:37] Epoch 4\\Batch 35000\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:42] Epoch 4\\Batch 35050\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:47] Epoch 4\\Batch 35100\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:52] Epoch 4\\Batch 35150\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:55:57] Epoch 4\\Batch 35200\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:02] Epoch 4\\Batch 35250\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:07] Epoch 4\\Batch 35300\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:12] Epoch 4\\Batch 35350\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:17] Epoch 4\\Batch 35400\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:22] Epoch 4\\Batch 35450\\ Train Loss:10.392\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:26] Epoch 4\\Batch 35500\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:32] Epoch 4\\Batch 35550\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:36] Epoch 4\\Batch 35600\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:42] Epoch 4\\Batch 35650\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:46] Epoch 4\\Batch 35700\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:52] Epoch 4\\Batch 35750\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:56:56] Epoch 4\\Batch 35800\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:01] Epoch 4\\Batch 35850\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:06] Epoch 4\\Batch 35900\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:12] Epoch 4\\Batch 35950\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:17] Epoch 4\\Batch 36000\\ Train Loss:10.391\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5494.000001, 'TP': 1505.0000009999999, 'FP': 1669.0000009999999}\n",
      "[2019/03/17 22:57:33] Epoch 4/ Validation Loss:9.921/ F1_score:0.296/ Precision:0.474/ Recall:0.215\n",
      "[2019/03/17 22:57:38] Epoch 4\\Batch 36050\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:43] Epoch 4\\Batch 36100\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:48] Epoch 4\\Batch 36150\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:53] Epoch 4\\Batch 36200\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:57:57] Epoch 4\\Batch 36250\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:02] Epoch 4\\Batch 36300\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:07] Epoch 4\\Batch 36350\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:12] Epoch 4\\Batch 36400\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:17] Epoch 4\\Batch 36450\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:22] Epoch 4\\Batch 36500\\ Train Loss:10.390\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:27] Epoch 4\\Batch 36550\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:33] Epoch 4\\Batch 36600\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:38] Epoch 4\\Batch 36650\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:43] Epoch 4\\Batch 36700\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:48] Epoch 4\\Batch 36750\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:53] Epoch 4\\Batch 36800\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:58:58] Epoch 4\\Batch 36850\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:04] Epoch 4\\Batch 36900\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:09] Epoch 4\\Batch 36950\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:14] Epoch 4\\Batch 37000\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:19] Epoch 4\\Batch 37050\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:24] Epoch 4\\Batch 37100\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:29] Epoch 4\\Batch 37150\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:35] Epoch 4\\Batch 37200\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:40] Epoch 4\\Batch 37250\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:46] Epoch 4\\Batch 37300\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:51] Epoch 4\\Batch 37350\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 22:59:57] Epoch 4\\Batch 37400\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:02] Epoch 4\\Batch 37450\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:07] Epoch 4\\Batch 37500\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:13] Epoch 4\\Batch 37550\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:18] Epoch 4\\Batch 37600\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:23] Epoch 4\\Batch 37650\\ Train Loss:10.388\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:00:29] Epoch 4\\Batch 37700\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:34] Epoch 4\\Batch 37750\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:37] Epoch 4\\Batch 37800\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:40] Epoch 4\\Batch 37850\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:44] Epoch 4\\Batch 37900\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:47] Epoch 4\\Batch 37950\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:51] Epoch 4\\Batch 38000\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:54] Epoch 4\\Batch 38050\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:00:57] Epoch 4\\Batch 38100\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:01] Epoch 4\\Batch 38150\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:04] Epoch 4\\Batch 38200\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:07] Epoch 4\\Batch 38250\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:11] Epoch 4\\Batch 38300\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:14] Epoch 4\\Batch 38350\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:18] Epoch 4\\Batch 38400\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:21] Epoch 4\\Batch 38450\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:26] Epoch 4\\Batch 38500\\ Train Loss:10.389\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:31] Epoch 4\\Batch 38550\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:36] Epoch 4\\Batch 38600\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:41] Epoch 4\\Batch 38650\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:47] Epoch 4\\Batch 38700\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:52] Epoch 4\\Batch 38750\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:01:57] Epoch 4\\Batch 38800\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:02:02] Epoch 4\\Batch 38850\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:02:07] Epoch 4\\Batch 38900\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:02:12] Epoch 4\\Batch 38950\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:02:18] Epoch 4\\Batch 39000\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5479.000001, 'TP': 1520.0000009999999, 'FP': 1688.0000009999999}\n",
      "[2019/03/17 23:02:41] Epoch 4/ Validation Loss:9.958/ F1_score:0.298/ Precision:0.474/ Recall:0.217\n",
      "[2019/03/17 23:02:46] Epoch 4\\Batch 39050\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:02:51] Epoch 4\\Batch 39100\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:02:57] Epoch 4\\Batch 39150\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:02] Epoch 4\\Batch 39200\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:07] Epoch 4\\Batch 39250\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:12] Epoch 4\\Batch 39300\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:17] Epoch 4\\Batch 39350\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:23] Epoch 4\\Batch 39400\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:28] Epoch 4\\Batch 39450\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:33] Epoch 4\\Batch 39500\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:38] Epoch 4\\Batch 39550\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:43] Epoch 4\\Batch 39600\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:48] Epoch 4\\Batch 39650\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:53] Epoch 4\\Batch 39700\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:03:58] Epoch 4\\Batch 39750\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:04] Epoch 4\\Batch 39800\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:09] Epoch 4\\Batch 39850\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:14] Epoch 4\\Batch 39900\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:19] Epoch 4\\Batch 39950\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:24] Epoch 4\\Batch 40000\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:29] Epoch 4\\Batch 40050\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:34] Epoch 4\\Batch 40100\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:39] Epoch 4\\Batch 40150\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:44] Epoch 4\\Batch 40200\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:49] Epoch 4\\Batch 40250\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:54] Epoch 4\\Batch 40300\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:04:59] Epoch 4\\Batch 40350\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:04] Epoch 4\\Batch 40400\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:09] Epoch 4\\Batch 40450\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:14] Epoch 4\\Batch 40500\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:19] Epoch 4\\Batch 40550\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:24] Epoch 4\\Batch 40600\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:29] Epoch 4\\Batch 40650\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:34] Epoch 4\\Batch 40700\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:39] Epoch 4\\Batch 40750\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:44] Epoch 4\\Batch 40800\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:49] Epoch 4\\Batch 40850\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:54] Epoch 4\\Batch 40900\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:05:59] Epoch 4\\Batch 40950\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:04] Epoch 4\\Batch 41000\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:09] Epoch 4\\Batch 41050\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:14] Epoch 4\\Batch 41100\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:19] Epoch 4\\Batch 41150\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:24] Epoch 4\\Batch 41200\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:29] Epoch 4\\Batch 41250\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:34] Epoch 4\\Batch 41300\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:39] Epoch 4\\Batch 41350\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:44] Epoch 4\\Batch 41400\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:49] Epoch 4\\Batch 41450\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:54] Epoch 4\\Batch 41500\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:06:59] Epoch 4\\Batch 41550\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:04] Epoch 4\\Batch 41600\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:08] Epoch 4\\Batch 41650\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:13] Epoch 4\\Batch 41700\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:18] Epoch 4\\Batch 41750\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:23] Epoch 4\\Batch 41800\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:28] Epoch 4\\Batch 41850\\ Train Loss:10.387\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:33] Epoch 4\\Batch 41900\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:38] Epoch 4\\Batch 41950\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:07:43] Epoch 4\\Batch 42000\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5467.000001, 'TP': 1532.0000009999999, 'FP': 1661.0000009999999}\n",
      "[2019/03/17 23:08:05] Epoch 4/ Validation Loss:9.923/ F1_score:0.301/ Precision:0.480/ Recall:0.219\n",
      "[2019/03/17 23:08:08] Epoch 4\\Batch 42050\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:12] Epoch 4\\Batch 42100\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:15] Epoch 4\\Batch 42150\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:18] Epoch 4\\Batch 42200\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:21] Epoch 4\\Batch 42250\\ Train Loss:10.386\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:08:24] Epoch 4\\Batch 42300\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:28] Epoch 4\\Batch 42350\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:33] Epoch 4\\Batch 42400\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:38] Epoch 4\\Batch 42450\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:43] Epoch 4\\Batch 42500\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:48] Epoch 4\\Batch 42550\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:53] Epoch 4\\Batch 42600\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:08:58] Epoch 4\\Batch 42650\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:03] Epoch 4\\Batch 42700\\ Train Loss:10.386\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:08] Epoch 4\\Batch 42750\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:13] Epoch 4\\Batch 42800\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:18] Epoch 4\\Batch 42850\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:23] Epoch 4\\Batch 42900\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:28] Epoch 4\\Batch 42950\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:33] Epoch 4\\Batch 43000\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:38] Epoch 4\\Batch 43050\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:43] Epoch 4\\Batch 43100\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:48] Epoch 4\\Batch 43150\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:52] Epoch 4\\Batch 43200\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:09:57] Epoch 4\\Batch 43250\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:02] Epoch 4\\Batch 43300\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:07] Epoch 4\\Batch 43350\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:12] Epoch 4\\Batch 43400\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:17] Epoch 4\\Batch 43450\\ Train Loss:10.385\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:23] Epoch 4\\Batch 43500\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:28] Epoch 4\\Batch 43550\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:33] Epoch 4\\Batch 43600\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:38] Epoch 4\\Batch 43650\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:43] Epoch 4\\Batch 43700\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:48] Epoch 4\\Batch 43750\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:54] Epoch 4\\Batch 43800\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:10:59] Epoch 4\\Batch 43850\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:04] Epoch 4\\Batch 43900\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:09] Epoch 4\\Batch 43950\\ Train Loss:10.384\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:14] Epoch 4\\Batch 44000\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:19] Epoch 4\\Batch 44050\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:25] Epoch 4\\Batch 44100\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:29] Epoch 4\\Batch 44150\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:34] Epoch 4\\Batch 44200\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:39] Epoch 4\\Batch 44250\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:44] Epoch 4\\Batch 44300\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:49] Epoch 4\\Batch 44350\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:11:54] Epoch 4\\Batch 44400\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:00] Epoch 4\\Batch 44450\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:05] Epoch 4\\Batch 44500\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:10] Epoch 4\\Batch 44550\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:15] Epoch 4\\Batch 44600\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:20] Epoch 4\\Batch 44650\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:25] Epoch 4\\Batch 44700\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:31] Epoch 4\\Batch 44750\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:36] Epoch 4\\Batch 44800\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:41] Epoch 4\\Batch 44850\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:46] Epoch 4\\Batch 44900\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:51] Epoch 4\\Batch 44950\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:12:57] Epoch 4\\Batch 45000\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5495.000001, 'TP': 1504.0000009999999, 'FP': 1681.0000009999999}\n",
      "[2019/03/17 23:13:20] Epoch 4/ Validation Loss:9.928/ F1_score:0.295/ Precision:0.472/ Recall:0.215\n",
      "[2019/03/17 23:13:26] Epoch 4\\Batch 45050\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:13:31] Epoch 4\\Batch 45100\\ Train Loss:10.383\\ Learning rate:0.00030\n",
      "[2019/03/17 23:13:36] Epoch 4\\Batch 45150\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:13:41] Epoch 4\\Batch 45200\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:13:46] Epoch 4\\Batch 45250\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:13:51] Epoch 4\\Batch 45300\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:13:56] Epoch 4\\Batch 45350\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:01] Epoch 4\\Batch 45400\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:07] Epoch 4\\Batch 45450\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:12] Epoch 4\\Batch 45500\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:17] Epoch 4\\Batch 45550\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:22] Epoch 4\\Batch 45600\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:27] Epoch 4\\Batch 45650\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:32] Epoch 4\\Batch 45700\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:37] Epoch 4\\Batch 45750\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:42] Epoch 4\\Batch 45800\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:48] Epoch 4\\Batch 45850\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:52] Epoch 4\\Batch 45900\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:14:58] Epoch 4\\Batch 45950\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:03] Epoch 4\\Batch 46000\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:08] Epoch 4\\Batch 46050\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:13] Epoch 4\\Batch 46100\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:18] Epoch 4\\Batch 46150\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:23] Epoch 4\\Batch 46200\\ Train Loss:10.382\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:31] Epoch 5\\Batch 50\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:34] Epoch 5\\Batch 100\\ Train Loss:10.438\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:38] Epoch 5\\Batch 150\\ Train Loss:10.472\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:41] Epoch 5\\Batch 200\\ Train Loss:10.430\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:44] Epoch 5\\Batch 250\\ Train Loss:10.411\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:48] Epoch 5\\Batch 300\\ Train Loss:10.415\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:51] Epoch 5\\Batch 350\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 23:15:55] Epoch 5\\Batch 400\\ Train Loss:10.416\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:00] Epoch 5\\Batch 450\\ Train Loss:10.397\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:05] Epoch 5\\Batch 500\\ Train Loss:10.388\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:10] Epoch 5\\Batch 550\\ Train Loss:10.373\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:15] Epoch 5\\Batch 600\\ Train Loss:10.374\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:20] Epoch 5\\Batch 650\\ Train Loss:10.372\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:25] Epoch 5\\Batch 700\\ Train Loss:10.375\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:30] Epoch 5\\Batch 750\\ Train Loss:10.374\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:35] Epoch 5\\Batch 800\\ Train Loss:10.371\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:40] Epoch 5\\Batch 850\\ Train Loss:10.376\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:16:45] Epoch 5\\Batch 900\\ Train Loss:10.381\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:50] Epoch 5\\Batch 950\\ Train Loss:10.372\\ Learning rate:0.00030\n",
      "[2019/03/17 23:16:55] Epoch 5\\Batch 1000\\ Train Loss:10.362\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:00] Epoch 5\\Batch 1050\\ Train Loss:10.362\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:05] Epoch 5\\Batch 1100\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:09] Epoch 5\\Batch 1150\\ Train Loss:10.364\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:14] Epoch 5\\Batch 1200\\ Train Loss:10.359\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:19] Epoch 5\\Batch 1250\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:24] Epoch 5\\Batch 1300\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:29] Epoch 5\\Batch 1350\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:34] Epoch 5\\Batch 1400\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:39] Epoch 5\\Batch 1450\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:44] Epoch 5\\Batch 1500\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:49] Epoch 5\\Batch 1550\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:54] Epoch 5\\Batch 1600\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:17:59] Epoch 5\\Batch 1650\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:04] Epoch 5\\Batch 1700\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:09] Epoch 5\\Batch 1750\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:13] Epoch 5\\Batch 1800\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:18] Epoch 5\\Batch 1850\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:23] Epoch 5\\Batch 1900\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:28] Epoch 5\\Batch 1950\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:33] Epoch 5\\Batch 2000\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:38] Epoch 5\\Batch 2050\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:43] Epoch 5\\Batch 2100\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:48] Epoch 5\\Batch 2150\\ Train Loss:10.359\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:53] Epoch 5\\Batch 2200\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:18:58] Epoch 5\\Batch 2250\\ Train Loss:10.364\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:03] Epoch 5\\Batch 2300\\ Train Loss:10.366\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:08] Epoch 5\\Batch 2350\\ Train Loss:10.363\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:13] Epoch 5\\Batch 2400\\ Train Loss:10.364\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:18] Epoch 5\\Batch 2450\\ Train Loss:10.363\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:23] Epoch 5\\Batch 2500\\ Train Loss:10.366\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:28] Epoch 5\\Batch 2550\\ Train Loss:10.365\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:33] Epoch 5\\Batch 2600\\ Train Loss:10.365\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:38] Epoch 5\\Batch 2650\\ Train Loss:10.367\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:43] Epoch 5\\Batch 2700\\ Train Loss:10.365\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:48] Epoch 5\\Batch 2750\\ Train Loss:10.370\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:53] Epoch 5\\Batch 2800\\ Train Loss:10.367\\ Learning rate:0.00030\n",
      "[2019/03/17 23:19:58] Epoch 5\\Batch 2850\\ Train Loss:10.367\\ Learning rate:0.00030\n",
      "[2019/03/17 23:20:03] Epoch 5\\Batch 2900\\ Train Loss:10.362\\ Learning rate:0.00030\n",
      "[2019/03/17 23:20:08] Epoch 5\\Batch 2950\\ Train Loss:10.365\\ Learning rate:0.00030\n",
      "[2019/03/17 23:20:13] Epoch 5\\Batch 3000\\ Train Loss:10.363\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5482.000001, 'TP': 1517.0000009999999, 'FP': 1656.0000009999999}\n",
      "[2019/03/17 23:20:36] Epoch 5/ Validation Loss:9.926/ F1_score:0.298/ Precision:0.478/ Recall:0.217\n",
      "[2019/03/17 23:20:41] Epoch 5\\Batch 3050\\ Train Loss:10.364\\ Learning rate:0.00030\n",
      "[2019/03/17 23:20:46] Epoch 5\\Batch 3100\\ Train Loss:10.366\\ Learning rate:0.00030\n",
      "[2019/03/17 23:20:51] Epoch 5\\Batch 3150\\ Train Loss:10.365\\ Learning rate:0.00030\n",
      "[2019/03/17 23:20:56] Epoch 5\\Batch 3200\\ Train Loss:10.363\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:01] Epoch 5\\Batch 3250\\ Train Loss:10.362\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:06] Epoch 5\\Batch 3300\\ Train Loss:10.359\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:11] Epoch 5\\Batch 3350\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:16] Epoch 5\\Batch 3400\\ Train Loss:10.359\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:21] Epoch 5\\Batch 3450\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:26] Epoch 5\\Batch 3500\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:31] Epoch 5\\Batch 3550\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:36] Epoch 5\\Batch 3600\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:41] Epoch 5\\Batch 3650\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:46] Epoch 5\\Batch 3700\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:50] Epoch 5\\Batch 3750\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:21:55] Epoch 5\\Batch 3800\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:00] Epoch 5\\Batch 3850\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:05] Epoch 5\\Batch 3900\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:10] Epoch 5\\Batch 3950\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:15] Epoch 5\\Batch 4000\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:20] Epoch 5\\Batch 4050\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:25] Epoch 5\\Batch 4100\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:30] Epoch 5\\Batch 4150\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:35] Epoch 5\\Batch 4200\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:38] Epoch 5\\Batch 4250\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:42] Epoch 5\\Batch 4300\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:45] Epoch 5\\Batch 4350\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:48] Epoch 5\\Batch 4400\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:51] Epoch 5\\Batch 4450\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:55] Epoch 5\\Batch 4500\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:22:58] Epoch 5\\Batch 4550\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:02] Epoch 5\\Batch 4600\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:07] Epoch 5\\Batch 4650\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:11] Epoch 5\\Batch 4700\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:16] Epoch 5\\Batch 4750\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:21] Epoch 5\\Batch 4800\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:26] Epoch 5\\Batch 4850\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:31] Epoch 5\\Batch 4900\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:36] Epoch 5\\Batch 4950\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:41] Epoch 5\\Batch 5000\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:46] Epoch 5\\Batch 5050\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:51] Epoch 5\\Batch 5100\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:23:56] Epoch 5\\Batch 5150\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:01] Epoch 5\\Batch 5200\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:06] Epoch 5\\Batch 5250\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:11] Epoch 5\\Batch 5300\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:16] Epoch 5\\Batch 5350\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:21] Epoch 5\\Batch 5400\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:26] Epoch 5\\Batch 5450\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:31] Epoch 5\\Batch 5500\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:36] Epoch 5\\Batch 5550\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:41] Epoch 5\\Batch 5600\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:46] Epoch 5\\Batch 5650\\ Train Loss:10.353\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:24:52] Epoch 5\\Batch 5700\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:24:57] Epoch 5\\Batch 5750\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:25:02] Epoch 5\\Batch 5800\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:25:07] Epoch 5\\Batch 5850\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:25:12] Epoch 5\\Batch 5900\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "[2019/03/17 23:25:17] Epoch 5\\Batch 5950\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "[2019/03/17 23:25:22] Epoch 5\\Batch 6000\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5481.000001, 'TP': 1518.0000009999999, 'FP': 1659.0000009999999}\n",
      "[2019/03/17 23:25:45] Epoch 5/ Validation Loss:9.911/ F1_score:0.298/ Precision:0.478/ Recall:0.217\n",
      "[2019/03/17 23:25:51] Epoch 5\\Batch 6050\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "[2019/03/17 23:25:56] Epoch 5\\Batch 6100\\ Train Loss:10.359\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:01] Epoch 5\\Batch 6150\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:06] Epoch 5\\Batch 6200\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:11] Epoch 5\\Batch 6250\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:16] Epoch 5\\Batch 6300\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:21] Epoch 5\\Batch 6350\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:27] Epoch 5\\Batch 6400\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:32] Epoch 5\\Batch 6450\\ Train Loss:10.359\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:37] Epoch 5\\Batch 6500\\ Train Loss:10.358\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:42] Epoch 5\\Batch 6550\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:48] Epoch 5\\Batch 6600\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:52] Epoch 5\\Batch 6650\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:26:57] Epoch 5\\Batch 6700\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:02] Epoch 5\\Batch 6750\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:07] Epoch 5\\Batch 6800\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:12] Epoch 5\\Batch 6850\\ Train Loss:10.357\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:17] Epoch 5\\Batch 6900\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:22] Epoch 5\\Batch 6950\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:27] Epoch 5\\Batch 7000\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:32] Epoch 5\\Batch 7050\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:37] Epoch 5\\Batch 7100\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:42] Epoch 5\\Batch 7150\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:47] Epoch 5\\Batch 7200\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:52] Epoch 5\\Batch 7250\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:27:57] Epoch 5\\Batch 7300\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:02] Epoch 5\\Batch 7350\\ Train Loss:10.356\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:07] Epoch 5\\Batch 7400\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:12] Epoch 5\\Batch 7450\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:17] Epoch 5\\Batch 7500\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:21] Epoch 5\\Batch 7550\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:26] Epoch 5\\Batch 7600\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:31] Epoch 5\\Batch 7650\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:36] Epoch 5\\Batch 7700\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:41] Epoch 5\\Batch 7750\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:46] Epoch 5\\Batch 7800\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:51] Epoch 5\\Batch 7850\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:28:56] Epoch 5\\Batch 7900\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:01] Epoch 5\\Batch 7950\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:06] Epoch 5\\Batch 8000\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:11] Epoch 5\\Batch 8050\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:16] Epoch 5\\Batch 8100\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:21] Epoch 5\\Batch 8150\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:27] Epoch 5\\Batch 8200\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:32] Epoch 5\\Batch 8250\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:37] Epoch 5\\Batch 8300\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:42] Epoch 5\\Batch 8350\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:47] Epoch 5\\Batch 8400\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:52] Epoch 5\\Batch 8450\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:29:57] Epoch 5\\Batch 8500\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:00] Epoch 5\\Batch 8550\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:04] Epoch 5\\Batch 8600\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:07] Epoch 5\\Batch 8650\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:10] Epoch 5\\Batch 8700\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:14] Epoch 5\\Batch 8750\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:17] Epoch 5\\Batch 8800\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:20] Epoch 5\\Batch 8850\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:26] Epoch 5\\Batch 8900\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:31] Epoch 5\\Batch 8950\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:30:36] Epoch 5\\Batch 9000\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5507.000001, 'TP': 1492.0000009999999, 'FP': 1679.0000009999999}\n",
      "[2019/03/17 23:30:59] Epoch 5/ Validation Loss:9.924/ F1_score:0.293/ Precision:0.471/ Recall:0.213\n",
      "[2019/03/17 23:31:04] Epoch 5\\Batch 9050\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:09] Epoch 5\\Batch 9100\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:13] Epoch 5\\Batch 9150\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:18] Epoch 5\\Batch 9200\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:23] Epoch 5\\Batch 9250\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:28] Epoch 5\\Batch 9300\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:33] Epoch 5\\Batch 9350\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:38] Epoch 5\\Batch 9400\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:43] Epoch 5\\Batch 9450\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:48] Epoch 5\\Batch 9500\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:53] Epoch 5\\Batch 9550\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:31:58] Epoch 5\\Batch 9600\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:03] Epoch 5\\Batch 9650\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:08] Epoch 5\\Batch 9700\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:13] Epoch 5\\Batch 9750\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:18] Epoch 5\\Batch 9800\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:23] Epoch 5\\Batch 9850\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:28] Epoch 5\\Batch 9900\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:33] Epoch 5\\Batch 9950\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:38] Epoch 5\\Batch 10000\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:43] Epoch 5\\Batch 10050\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:47] Epoch 5\\Batch 10100\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:52] Epoch 5\\Batch 10150\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:32:57] Epoch 5\\Batch 10200\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:02] Epoch 5\\Batch 10250\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:07] Epoch 5\\Batch 10300\\ Train Loss:10.355\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:33:12] Epoch 5\\Batch 10350\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:17] Epoch 5\\Batch 10400\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:22] Epoch 5\\Batch 10450\\ Train Loss:10.355\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:27] Epoch 5\\Batch 10500\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:32] Epoch 5\\Batch 10550\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:37] Epoch 5\\Batch 10600\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:42] Epoch 5\\Batch 10650\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:47] Epoch 5\\Batch 10700\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:52] Epoch 5\\Batch 10750\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:33:57] Epoch 5\\Batch 10800\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:02] Epoch 5\\Batch 10850\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:07] Epoch 5\\Batch 10900\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:12] Epoch 5\\Batch 10950\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:17] Epoch 5\\Batch 11000\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:22] Epoch 5\\Batch 11050\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:26] Epoch 5\\Batch 11100\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:31] Epoch 5\\Batch 11150\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:36] Epoch 5\\Batch 11200\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:41] Epoch 5\\Batch 11250\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:46] Epoch 5\\Batch 11300\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:51] Epoch 5\\Batch 11350\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:34:56] Epoch 5\\Batch 11400\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:01] Epoch 5\\Batch 11450\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:06] Epoch 5\\Batch 11500\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:11] Epoch 5\\Batch 11550\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:16] Epoch 5\\Batch 11600\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:21] Epoch 5\\Batch 11650\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:26] Epoch 5\\Batch 11700\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:31] Epoch 5\\Batch 11750\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:36] Epoch 5\\Batch 11800\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:41] Epoch 5\\Batch 11850\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:46] Epoch 5\\Batch 11900\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:51] Epoch 5\\Batch 11950\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:35:56] Epoch 5\\Batch 12000\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5480.000001, 'TP': 1519.0000009999999, 'FP': 1676.0000009999999}\n",
      "[2019/03/17 23:36:18] Epoch 5/ Validation Loss:9.883/ F1_score:0.298/ Precision:0.475/ Recall:0.217\n",
      "[2019/03/17 23:36:23] Epoch 5\\Batch 12050\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:28] Epoch 5\\Batch 12100\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:33] Epoch 5\\Batch 12150\\ Train Loss:10.354\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:38] Epoch 5\\Batch 12200\\ Train Loss:10.353\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:43] Epoch 5\\Batch 12250\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:48] Epoch 5\\Batch 12300\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:52] Epoch 5\\Batch 12350\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:56] Epoch 5\\Batch 12400\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/17 23:36:59] Epoch 5\\Batch 12450\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:02] Epoch 5\\Batch 12500\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:05] Epoch 5\\Batch 12550\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:09] Epoch 5\\Batch 12600\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:12] Epoch 5\\Batch 12650\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:15] Epoch 5\\Batch 12700\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:20] Epoch 5\\Batch 12750\\ Train Loss:10.351\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:25] Epoch 5\\Batch 12800\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:29] Epoch 5\\Batch 12850\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:34] Epoch 5\\Batch 12900\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:39] Epoch 5\\Batch 12950\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:44] Epoch 5\\Batch 13000\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:49] Epoch 5\\Batch 13050\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:54] Epoch 5\\Batch 13100\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:37:59] Epoch 5\\Batch 13150\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:04] Epoch 5\\Batch 13200\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:09] Epoch 5\\Batch 13250\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:14] Epoch 5\\Batch 13300\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:19] Epoch 5\\Batch 13350\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:24] Epoch 5\\Batch 13400\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:29] Epoch 5\\Batch 13450\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:34] Epoch 5\\Batch 13500\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:39] Epoch 5\\Batch 13550\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:44] Epoch 5\\Batch 13600\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:49] Epoch 5\\Batch 13650\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:54] Epoch 5\\Batch 13700\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:38:59] Epoch 5\\Batch 13750\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:04] Epoch 5\\Batch 13800\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:09] Epoch 5\\Batch 13850\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:15] Epoch 5\\Batch 13900\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:20] Epoch 5\\Batch 13950\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:25] Epoch 5\\Batch 14000\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:30] Epoch 5\\Batch 14050\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:35] Epoch 5\\Batch 14100\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:40] Epoch 5\\Batch 14150\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:46] Epoch 5\\Batch 14200\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:51] Epoch 5\\Batch 14250\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:39:56] Epoch 5\\Batch 14300\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:01] Epoch 5\\Batch 14350\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:07] Epoch 5\\Batch 14400\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:12] Epoch 5\\Batch 14450\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:17] Epoch 5\\Batch 14500\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:22] Epoch 5\\Batch 14550\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:28] Epoch 5\\Batch 14600\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:33] Epoch 5\\Batch 14650\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:38] Epoch 5\\Batch 14700\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:43] Epoch 5\\Batch 14750\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:48] Epoch 5\\Batch 14800\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:53] Epoch 5\\Batch 14850\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:40:58] Epoch 5\\Batch 14900\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:41:04] Epoch 5\\Batch 14950\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:41:09] Epoch 5\\Batch 15000\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5516.000001, 'TP': 1483.0000009999999, 'FP': 1688.0000009999999}\n",
      "[2019/03/17 23:41:32] Epoch 5/ Validation Loss:9.885/ F1_score:0.292/ Precision:0.468/ Recall:0.212\n",
      "[2019/03/17 23:41:38] Epoch 5\\Batch 15050\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:41:43] Epoch 5\\Batch 15100\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:41:48] Epoch 5\\Batch 15150\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:41:53] Epoch 5\\Batch 15200\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:41:58] Epoch 5\\Batch 15250\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:03] Epoch 5\\Batch 15300\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:08] Epoch 5\\Batch 15350\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:13] Epoch 5\\Batch 15400\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:19] Epoch 5\\Batch 15450\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:24] Epoch 5\\Batch 15500\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:29] Epoch 5\\Batch 15550\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:34] Epoch 5\\Batch 15600\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:39] Epoch 5\\Batch 15650\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:44] Epoch 5\\Batch 15700\\ Train Loss:10.350\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:49] Epoch 5\\Batch 15750\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:42:54] Epoch 5\\Batch 15800\\ Train Loss:10.349\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:00] Epoch 5\\Batch 15850\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:05] Epoch 5\\Batch 15900\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:10] Epoch 5\\Batch 15950\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:15] Epoch 5\\Batch 16000\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:20] Epoch 5\\Batch 16050\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:25] Epoch 5\\Batch 16100\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:31] Epoch 5\\Batch 16150\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:36] Epoch 5\\Batch 16200\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:41] Epoch 5\\Batch 16250\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:46] Epoch 5\\Batch 16300\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:51] Epoch 5\\Batch 16350\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:43:56] Epoch 5\\Batch 16400\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:01] Epoch 5\\Batch 16450\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:06] Epoch 5\\Batch 16500\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:12] Epoch 5\\Batch 16550\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:15] Epoch 5\\Batch 16600\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:19] Epoch 5\\Batch 16650\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:22] Epoch 5\\Batch 16700\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:26] Epoch 5\\Batch 16750\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:29] Epoch 5\\Batch 16800\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:32] Epoch 5\\Batch 16850\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:36] Epoch 5\\Batch 16900\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:40] Epoch 5\\Batch 16950\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:45] Epoch 5\\Batch 17000\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:50] Epoch 5\\Batch 17050\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:44:55] Epoch 5\\Batch 17100\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:00] Epoch 5\\Batch 17150\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:05] Epoch 5\\Batch 17200\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:10] Epoch 5\\Batch 17250\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:16] Epoch 5\\Batch 17300\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:21] Epoch 5\\Batch 17350\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:26] Epoch 5\\Batch 17400\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:31] Epoch 5\\Batch 17450\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:37] Epoch 5\\Batch 17500\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:42] Epoch 5\\Batch 17550\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:47] Epoch 5\\Batch 17600\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:52] Epoch 5\\Batch 17650\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:45:57] Epoch 5\\Batch 17700\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:46:03] Epoch 5\\Batch 17750\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:46:08] Epoch 5\\Batch 17800\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:46:13] Epoch 5\\Batch 17850\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:46:18] Epoch 5\\Batch 17900\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:46:24] Epoch 5\\Batch 17950\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:46:29] Epoch 5\\Batch 18000\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5482.000001, 'TP': 1517.0000009999999, 'FP': 1677.0000009999999}\n",
      "[2019/03/17 23:46:52] Epoch 5/ Validation Loss:9.887/ F1_score:0.298/ Precision:0.475/ Recall:0.217\n",
      "[2019/03/17 23:46:57] Epoch 5\\Batch 18050\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:03] Epoch 5\\Batch 18100\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:08] Epoch 5\\Batch 18150\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:13] Epoch 5\\Batch 18200\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:18] Epoch 5\\Batch 18250\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:23] Epoch 5\\Batch 18300\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:29] Epoch 5\\Batch 18350\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:34] Epoch 5\\Batch 18400\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:39] Epoch 5\\Batch 18450\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:44] Epoch 5\\Batch 18500\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:50] Epoch 5\\Batch 18550\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:47:55] Epoch 5\\Batch 18600\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:00] Epoch 5\\Batch 18650\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:05] Epoch 5\\Batch 18700\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:10] Epoch 5\\Batch 18750\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:15] Epoch 5\\Batch 18800\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:21] Epoch 5\\Batch 18850\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:26] Epoch 5\\Batch 18900\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:31] Epoch 5\\Batch 18950\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:36] Epoch 5\\Batch 19000\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:41] Epoch 5\\Batch 19050\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:46] Epoch 5\\Batch 19100\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:52] Epoch 5\\Batch 19150\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:48:57] Epoch 5\\Batch 19200\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:02] Epoch 5\\Batch 19250\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:07] Epoch 5\\Batch 19300\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:12] Epoch 5\\Batch 19350\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:18] Epoch 5\\Batch 19400\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:23] Epoch 5\\Batch 19450\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:28] Epoch 5\\Batch 19500\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:33] Epoch 5\\Batch 19550\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:38] Epoch 5\\Batch 19600\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:44] Epoch 5\\Batch 19650\\ Train Loss:10.346\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:49:49] Epoch 5\\Batch 19700\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:54] Epoch 5\\Batch 19750\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:49:59] Epoch 5\\Batch 19800\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:04] Epoch 5\\Batch 19850\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:10] Epoch 5\\Batch 19900\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:15] Epoch 5\\Batch 19950\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:20] Epoch 5\\Batch 20000\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:25] Epoch 5\\Batch 20050\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:30] Epoch 5\\Batch 20100\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:36] Epoch 5\\Batch 20150\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:41] Epoch 5\\Batch 20200\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:46] Epoch 5\\Batch 20250\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:51] Epoch 5\\Batch 20300\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:50:56] Epoch 5\\Batch 20350\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:01] Epoch 5\\Batch 20400\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:06] Epoch 5\\Batch 20450\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:11] Epoch 5\\Batch 20500\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:16] Epoch 5\\Batch 20550\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:22] Epoch 5\\Batch 20600\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:27] Epoch 5\\Batch 20650\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:32] Epoch 5\\Batch 20700\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:37] Epoch 5\\Batch 20750\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:42] Epoch 5\\Batch 20800\\ Train Loss:10.347\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:45] Epoch 5\\Batch 20850\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:49] Epoch 5\\Batch 20900\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:52] Epoch 5\\Batch 20950\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:51:55] Epoch 5\\Batch 21000\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5471.000001, 'TP': 1528.0000009999999, 'FP': 1663.0000009999999}\n",
      "[2019/03/17 23:52:16] Epoch 5/ Validation Loss:9.891/ F1_score:0.300/ Precision:0.479/ Recall:0.218\n",
      "[2019/03/17 23:52:21] Epoch 5\\Batch 21050\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:26] Epoch 5\\Batch 21100\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:31] Epoch 5\\Batch 21150\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:37] Epoch 5\\Batch 21200\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:42] Epoch 5\\Batch 21250\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:47] Epoch 5\\Batch 21300\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:52] Epoch 5\\Batch 21350\\ Train Loss:10.345\\ Learning rate:0.00030\n",
      "[2019/03/17 23:52:57] Epoch 5\\Batch 21400\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:02] Epoch 5\\Batch 21450\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:07] Epoch 5\\Batch 21500\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:13] Epoch 5\\Batch 21550\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:17] Epoch 5\\Batch 21600\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:22] Epoch 5\\Batch 21650\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:27] Epoch 5\\Batch 21700\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:32] Epoch 5\\Batch 21750\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:37] Epoch 5\\Batch 21800\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:42] Epoch 5\\Batch 21850\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:47] Epoch 5\\Batch 21900\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:52] Epoch 5\\Batch 21950\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:53:57] Epoch 5\\Batch 22000\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:02] Epoch 5\\Batch 22050\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:07] Epoch 5\\Batch 22100\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:12] Epoch 5\\Batch 22150\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:17] Epoch 5\\Batch 22200\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:22] Epoch 5\\Batch 22250\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:27] Epoch 5\\Batch 22300\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:32] Epoch 5\\Batch 22350\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:37] Epoch 5\\Batch 22400\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:42] Epoch 5\\Batch 22450\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:47] Epoch 5\\Batch 22500\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:52] Epoch 5\\Batch 22550\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:54:56] Epoch 5\\Batch 22600\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:01] Epoch 5\\Batch 22650\\ Train Loss:10.344\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:06] Epoch 5\\Batch 22700\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:11] Epoch 5\\Batch 22750\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:16] Epoch 5\\Batch 22800\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:21] Epoch 5\\Batch 22850\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:26] Epoch 5\\Batch 22900\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:31] Epoch 5\\Batch 22950\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:35] Epoch 5\\Batch 23000\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:40] Epoch 5\\Batch 23050\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:45] Epoch 5\\Batch 23100\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:50] Epoch 5\\Batch 23150\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:55:55] Epoch 5\\Batch 23200\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:00] Epoch 5\\Batch 23250\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:05] Epoch 5\\Batch 23300\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:10] Epoch 5\\Batch 23350\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:15] Epoch 5\\Batch 23400\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:20] Epoch 5\\Batch 23450\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:25] Epoch 5\\Batch 23500\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:30] Epoch 5\\Batch 23550\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:34] Epoch 5\\Batch 23600\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:40] Epoch 5\\Batch 23650\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:44] Epoch 5\\Batch 23700\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:49] Epoch 5\\Batch 23750\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:54] Epoch 5\\Batch 23800\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:56:59] Epoch 5\\Batch 23850\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:57:04] Epoch 5\\Batch 23900\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:57:09] Epoch 5\\Batch 23950\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "[2019/03/17 23:57:14] Epoch 5\\Batch 24000\\ Train Loss:10.343\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5475.000001, 'TP': 1524.0000009999999, 'FP': 1664.0000009999999}\n",
      "[2019/03/17 23:57:37] Epoch 5/ Validation Loss:9.868/ F1_score:0.299/ Precision:0.478/ Recall:0.218\n",
      "[2019/03/17 23:57:42] Epoch 5\\Batch 24050\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:57:47] Epoch 5\\Batch 24100\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:57:52] Epoch 5\\Batch 24150\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:57:57] Epoch 5\\Batch 24200\\ Train Loss:10.342\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:02] Epoch 5\\Batch 24250\\ Train Loss:10.341\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/17 23:58:07] Epoch 5\\Batch 24300\\ Train Loss:10.341\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:12] Epoch 5\\Batch 24350\\ Train Loss:10.341\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:17] Epoch 5\\Batch 24400\\ Train Loss:10.340\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:21] Epoch 5\\Batch 24450\\ Train Loss:10.341\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:26] Epoch 5\\Batch 24500\\ Train Loss:10.340\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:31] Epoch 5\\Batch 24550\\ Train Loss:10.340\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:36] Epoch 5\\Batch 24600\\ Train Loss:10.340\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:41] Epoch 5\\Batch 24650\\ Train Loss:10.340\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:46] Epoch 5\\Batch 24700\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:49] Epoch 5\\Batch 24750\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:52] Epoch 5\\Batch 24800\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:56] Epoch 5\\Batch 24850\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:58:59] Epoch 5\\Batch 24900\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:02] Epoch 5\\Batch 24950\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:05] Epoch 5\\Batch 25000\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:09] Epoch 5\\Batch 25050\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:13] Epoch 5\\Batch 25100\\ Train Loss:10.339\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:18] Epoch 5\\Batch 25150\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:23] Epoch 5\\Batch 25200\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:27] Epoch 5\\Batch 25250\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:32] Epoch 5\\Batch 25300\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:37] Epoch 5\\Batch 25350\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:42] Epoch 5\\Batch 25400\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:47] Epoch 5\\Batch 25450\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:52] Epoch 5\\Batch 25500\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/17 23:59:57] Epoch 5\\Batch 25550\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:02] Epoch 5\\Batch 25600\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:07] Epoch 5\\Batch 25650\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:12] Epoch 5\\Batch 25700\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:17] Epoch 5\\Batch 25750\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:22] Epoch 5\\Batch 25800\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:26] Epoch 5\\Batch 25850\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:31] Epoch 5\\Batch 25900\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:36] Epoch 5\\Batch 25950\\ Train Loss:10.337\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:41] Epoch 5\\Batch 26000\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:46] Epoch 5\\Batch 26050\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:51] Epoch 5\\Batch 26100\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:00:56] Epoch 5\\Batch 26150\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:01] Epoch 5\\Batch 26200\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:06] Epoch 5\\Batch 26250\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:11] Epoch 5\\Batch 26300\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:16] Epoch 5\\Batch 26350\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:21] Epoch 5\\Batch 26400\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:26] Epoch 5\\Batch 26450\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:31] Epoch 5\\Batch 26500\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:36] Epoch 5\\Batch 26550\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:41] Epoch 5\\Batch 26600\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:46] Epoch 5\\Batch 26650\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:51] Epoch 5\\Batch 26700\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:01:56] Epoch 5\\Batch 26750\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:00] Epoch 5\\Batch 26800\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:05] Epoch 5\\Batch 26850\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:10] Epoch 5\\Batch 26900\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:15] Epoch 5\\Batch 26950\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:20] Epoch 5\\Batch 27000\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5496.000001, 'TP': 1503.0000009999999, 'FP': 1668.0000009999999}\n",
      "[2019/03/18 00:02:43] Epoch 5/ Validation Loss:9.878/ F1_score:0.296/ Precision:0.474/ Recall:0.215\n",
      "[2019/03/18 00:02:48] Epoch 5\\Batch 27050\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:53] Epoch 5\\Batch 27100\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:02:58] Epoch 5\\Batch 27150\\ Train Loss:10.336\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:03] Epoch 5\\Batch 27200\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:08] Epoch 5\\Batch 27250\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:13] Epoch 5\\Batch 27300\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:18] Epoch 5\\Batch 27350\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:23] Epoch 5\\Batch 27400\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:28] Epoch 5\\Batch 27450\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:34] Epoch 5\\Batch 27500\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:38] Epoch 5\\Batch 27550\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:43] Epoch 5\\Batch 27600\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:48] Epoch 5\\Batch 27650\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:53] Epoch 5\\Batch 27700\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:03:59] Epoch 5\\Batch 27750\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:04] Epoch 5\\Batch 27800\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:09] Epoch 5\\Batch 27850\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:14] Epoch 5\\Batch 27900\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:19] Epoch 5\\Batch 27950\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:24] Epoch 5\\Batch 28000\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:29] Epoch 5\\Batch 28050\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:34] Epoch 5\\Batch 28100\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:39] Epoch 5\\Batch 28150\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:44] Epoch 5\\Batch 28200\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:49] Epoch 5\\Batch 28250\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:54] Epoch 5\\Batch 28300\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:04:59] Epoch 5\\Batch 28350\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:04] Epoch 5\\Batch 28400\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:09] Epoch 5\\Batch 28450\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:14] Epoch 5\\Batch 28500\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:19] Epoch 5\\Batch 28550\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:24] Epoch 5\\Batch 28600\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:29] Epoch 5\\Batch 28650\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:34] Epoch 5\\Batch 28700\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:39] Epoch 5\\Batch 28750\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:44] Epoch 5\\Batch 28800\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:49] Epoch 5\\Batch 28850\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:54] Epoch 5\\Batch 28900\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:05:58] Epoch 5\\Batch 28950\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:01] Epoch 5\\Batch 29000\\ Train Loss:10.334\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:06:04] Epoch 5\\Batch 29050\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:08] Epoch 5\\Batch 29100\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:11] Epoch 5\\Batch 29150\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:14] Epoch 5\\Batch 29200\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:18] Epoch 5\\Batch 29250\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:22] Epoch 5\\Batch 29300\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:26] Epoch 5\\Batch 29350\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:31] Epoch 5\\Batch 29400\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:36] Epoch 5\\Batch 29450\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:41] Epoch 5\\Batch 29500\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:46] Epoch 5\\Batch 29550\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:51] Epoch 5\\Batch 29600\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:06:56] Epoch 5\\Batch 29650\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:01] Epoch 5\\Batch 29700\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:06] Epoch 5\\Batch 29750\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:11] Epoch 5\\Batch 29800\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:16] Epoch 5\\Batch 29850\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:21] Epoch 5\\Batch 29900\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:26] Epoch 5\\Batch 29950\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:07:31] Epoch 5\\Batch 30000\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5479.000001, 'TP': 1520.0000009999999, 'FP': 1665.0000009999999}\n",
      "[2019/03/18 00:07:54] Epoch 5/ Validation Loss:9.880/ F1_score:0.299/ Precision:0.477/ Recall:0.217\n",
      "[2019/03/18 00:07:59] Epoch 5\\Batch 30050\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:04] Epoch 5\\Batch 30100\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:09] Epoch 5\\Batch 30150\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:14] Epoch 5\\Batch 30200\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:19] Epoch 5\\Batch 30250\\ Train Loss:10.335\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:24] Epoch 5\\Batch 30300\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:29] Epoch 5\\Batch 30350\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:34] Epoch 5\\Batch 30400\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:39] Epoch 5\\Batch 30450\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:44] Epoch 5\\Batch 30500\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:49] Epoch 5\\Batch 30550\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:54] Epoch 5\\Batch 30600\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:08:59] Epoch 5\\Batch 30650\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:04] Epoch 5\\Batch 30700\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:09] Epoch 5\\Batch 30750\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:14] Epoch 5\\Batch 30800\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:19] Epoch 5\\Batch 30850\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:24] Epoch 5\\Batch 30900\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:29] Epoch 5\\Batch 30950\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:34] Epoch 5\\Batch 31000\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:39] Epoch 5\\Batch 31050\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:44] Epoch 5\\Batch 31100\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:49] Epoch 5\\Batch 31150\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:54] Epoch 5\\Batch 31200\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:09:59] Epoch 5\\Batch 31250\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:04] Epoch 5\\Batch 31300\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:08] Epoch 5\\Batch 31350\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:13] Epoch 5\\Batch 31400\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:18] Epoch 5\\Batch 31450\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:23] Epoch 5\\Batch 31500\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:28] Epoch 5\\Batch 31550\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:33] Epoch 5\\Batch 31600\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:38] Epoch 5\\Batch 31650\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:43] Epoch 5\\Batch 31700\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:48] Epoch 5\\Batch 31750\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:53] Epoch 5\\Batch 31800\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:10:58] Epoch 5\\Batch 31850\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:03] Epoch 5\\Batch 31900\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:08] Epoch 5\\Batch 31950\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:13] Epoch 5\\Batch 32000\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:18] Epoch 5\\Batch 32050\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:23] Epoch 5\\Batch 32100\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:28] Epoch 5\\Batch 32150\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:33] Epoch 5\\Batch 32200\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:37] Epoch 5\\Batch 32250\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:42] Epoch 5\\Batch 32300\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:47] Epoch 5\\Batch 32350\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:52] Epoch 5\\Batch 32400\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:11:57] Epoch 5\\Batch 32450\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:02] Epoch 5\\Batch 32500\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:07] Epoch 5\\Batch 32550\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:12] Epoch 5\\Batch 32600\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:17] Epoch 5\\Batch 32650\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:22] Epoch 5\\Batch 32700\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:27] Epoch 5\\Batch 32750\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:32] Epoch 5\\Batch 32800\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:37] Epoch 5\\Batch 32850\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:42] Epoch 5\\Batch 32900\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:47] Epoch 5\\Batch 32950\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:12:52] Epoch 5\\Batch 33000\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5484.000001, 'TP': 1515.0000009999999, 'FP': 1688.0000009999999}\n",
      "[2019/03/18 00:13:11] Epoch 5/ Validation Loss:9.878/ F1_score:0.297/ Precision:0.473/ Recall:0.216\n",
      "[2019/03/18 00:13:14] Epoch 5\\Batch 33050\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:17] Epoch 5\\Batch 33100\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:21] Epoch 5\\Batch 33150\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:26] Epoch 5\\Batch 33200\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:31] Epoch 5\\Batch 33250\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:36] Epoch 5\\Batch 33300\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:41] Epoch 5\\Batch 33350\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:46] Epoch 5\\Batch 33400\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:51] Epoch 5\\Batch 33450\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:13:56] Epoch 5\\Batch 33500\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:01] Epoch 5\\Batch 33550\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:06] Epoch 5\\Batch 33600\\ Train Loss:10.333\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:14:11] Epoch 5\\Batch 33650\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:16] Epoch 5\\Batch 33700\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:21] Epoch 5\\Batch 33750\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:26] Epoch 5\\Batch 33800\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:31] Epoch 5\\Batch 33850\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:36] Epoch 5\\Batch 33900\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:41] Epoch 5\\Batch 33950\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:46] Epoch 5\\Batch 34000\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:51] Epoch 5\\Batch 34050\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:14:56] Epoch 5\\Batch 34100\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:01] Epoch 5\\Batch 34150\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:05] Epoch 5\\Batch 34200\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:10] Epoch 5\\Batch 34250\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:15] Epoch 5\\Batch 34300\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:21] Epoch 5\\Batch 34350\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:25] Epoch 5\\Batch 34400\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:30] Epoch 5\\Batch 34450\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:35] Epoch 5\\Batch 34500\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:40] Epoch 5\\Batch 34550\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:45] Epoch 5\\Batch 34600\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:50] Epoch 5\\Batch 34650\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:15:55] Epoch 5\\Batch 34700\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:01] Epoch 5\\Batch 34750\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:06] Epoch 5\\Batch 34800\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:10] Epoch 5\\Batch 34850\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:16] Epoch 5\\Batch 34900\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:20] Epoch 5\\Batch 34950\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:25] Epoch 5\\Batch 35000\\ Train Loss:10.334\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:30] Epoch 5\\Batch 35050\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:35] Epoch 5\\Batch 35100\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:40] Epoch 5\\Batch 35150\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:45] Epoch 5\\Batch 35200\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:50] Epoch 5\\Batch 35250\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:16:55] Epoch 5\\Batch 35300\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:00] Epoch 5\\Batch 35350\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:05] Epoch 5\\Batch 35400\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:10] Epoch 5\\Batch 35450\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:14] Epoch 5\\Batch 35500\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:19] Epoch 5\\Batch 35550\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:24] Epoch 5\\Batch 35600\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:29] Epoch 5\\Batch 35650\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:34] Epoch 5\\Batch 35700\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:39] Epoch 5\\Batch 35750\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:44] Epoch 5\\Batch 35800\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:49] Epoch 5\\Batch 35850\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:54] Epoch 5\\Batch 35900\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:17:59] Epoch 5\\Batch 35950\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:18:04] Epoch 5\\Batch 36000\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5481.000001, 'TP': 1518.0000009999999, 'FP': 1683.0000009999999}\n",
      "[2019/03/18 00:18:27] Epoch 5/ Validation Loss:9.862/ F1_score:0.298/ Precision:0.474/ Recall:0.217\n",
      "[2019/03/18 00:18:32] Epoch 5\\Batch 36050\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:18:37] Epoch 5\\Batch 36100\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:18:42] Epoch 5\\Batch 36150\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:18:47] Epoch 5\\Batch 36200\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:18:52] Epoch 5\\Batch 36250\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:18:57] Epoch 5\\Batch 36300\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:02] Epoch 5\\Batch 36350\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:07] Epoch 5\\Batch 36400\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:12] Epoch 5\\Batch 36450\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:17] Epoch 5\\Batch 36500\\ Train Loss:10.332\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:22] Epoch 5\\Batch 36550\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:27] Epoch 5\\Batch 36600\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:32] Epoch 5\\Batch 36650\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:37] Epoch 5\\Batch 36700\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:42] Epoch 5\\Batch 36750\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:47] Epoch 5\\Batch 36800\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:52] Epoch 5\\Batch 36850\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:19:57] Epoch 5\\Batch 36900\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:02] Epoch 5\\Batch 36950\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:06] Epoch 5\\Batch 37000\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:10] Epoch 5\\Batch 37050\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:13] Epoch 5\\Batch 37100\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:16] Epoch 5\\Batch 37150\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:20] Epoch 5\\Batch 37200\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:23] Epoch 5\\Batch 37250\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:26] Epoch 5\\Batch 37300\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:30] Epoch 5\\Batch 37350\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:34] Epoch 5\\Batch 37400\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:39] Epoch 5\\Batch 37450\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:44] Epoch 5\\Batch 37500\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:49] Epoch 5\\Batch 37550\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:54] Epoch 5\\Batch 37600\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:20:59] Epoch 5\\Batch 37650\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:03] Epoch 5\\Batch 37700\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:08] Epoch 5\\Batch 37750\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:14] Epoch 5\\Batch 37800\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:19] Epoch 5\\Batch 37850\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:24] Epoch 5\\Batch 37900\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:29] Epoch 5\\Batch 37950\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:34] Epoch 5\\Batch 38000\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:39] Epoch 5\\Batch 38050\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:44] Epoch 5\\Batch 38100\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:48] Epoch 5\\Batch 38150\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:53] Epoch 5\\Batch 38200\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:21:58] Epoch 5\\Batch 38250\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:03] Epoch 5\\Batch 38300\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:08] Epoch 5\\Batch 38350\\ Train Loss:10.331\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:22:13] Epoch 5\\Batch 38400\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:18] Epoch 5\\Batch 38450\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:23] Epoch 5\\Batch 38500\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:28] Epoch 5\\Batch 38550\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:33] Epoch 5\\Batch 38600\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:37] Epoch 5\\Batch 38650\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:42] Epoch 5\\Batch 38700\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:47] Epoch 5\\Batch 38750\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:52] Epoch 5\\Batch 38800\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:22:57] Epoch 5\\Batch 38850\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:23:02] Epoch 5\\Batch 38900\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:23:07] Epoch 5\\Batch 38950\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:23:12] Epoch 5\\Batch 39000\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5481.000001, 'TP': 1518.0000009999999, 'FP': 1677.0000009999999}\n",
      "[2019/03/18 00:23:35] Epoch 5/ Validation Loss:9.890/ F1_score:0.298/ Precision:0.475/ Recall:0.217\n",
      "[2019/03/18 00:23:40] Epoch 5\\Batch 39050\\ Train Loss:10.331\\ Learning rate:0.00030\n",
      "[2019/03/18 00:23:45] Epoch 5\\Batch 39100\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:23:50] Epoch 5\\Batch 39150\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:23:55] Epoch 5\\Batch 39200\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:00] Epoch 5\\Batch 39250\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:05] Epoch 5\\Batch 39300\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:10] Epoch 5\\Batch 39350\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:15] Epoch 5\\Batch 39400\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:20] Epoch 5\\Batch 39450\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:25] Epoch 5\\Batch 39500\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:30] Epoch 5\\Batch 39550\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:35] Epoch 5\\Batch 39600\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:40] Epoch 5\\Batch 39650\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:44] Epoch 5\\Batch 39700\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:49] Epoch 5\\Batch 39750\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:54] Epoch 5\\Batch 39800\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:24:59] Epoch 5\\Batch 39850\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:04] Epoch 5\\Batch 39900\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:09] Epoch 5\\Batch 39950\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:14] Epoch 5\\Batch 40000\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:19] Epoch 5\\Batch 40050\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:24] Epoch 5\\Batch 40100\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:29] Epoch 5\\Batch 40150\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:34] Epoch 5\\Batch 40200\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:39] Epoch 5\\Batch 40250\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:44] Epoch 5\\Batch 40300\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:49] Epoch 5\\Batch 40350\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:54] Epoch 5\\Batch 40400\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:25:59] Epoch 5\\Batch 40450\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:04] Epoch 5\\Batch 40500\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:09] Epoch 5\\Batch 40550\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:14] Epoch 5\\Batch 40600\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:19] Epoch 5\\Batch 40650\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:24] Epoch 5\\Batch 40700\\ Train Loss:10.330\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:29] Epoch 5\\Batch 40750\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:34] Epoch 5\\Batch 40800\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:39] Epoch 5\\Batch 40850\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:43] Epoch 5\\Batch 40900\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:48] Epoch 5\\Batch 40950\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:53] Epoch 5\\Batch 41000\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:26:58] Epoch 5\\Batch 41050\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:03] Epoch 5\\Batch 41100\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:08] Epoch 5\\Batch 41150\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:13] Epoch 5\\Batch 41200\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:17] Epoch 5\\Batch 41250\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:21] Epoch 5\\Batch 41300\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:24] Epoch 5\\Batch 41350\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:27] Epoch 5\\Batch 41400\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:30] Epoch 5\\Batch 41450\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:34] Epoch 5\\Batch 41500\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:37] Epoch 5\\Batch 41550\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:40] Epoch 5\\Batch 41600\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:45] Epoch 5\\Batch 41650\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:50] Epoch 5\\Batch 41700\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:27:55] Epoch 5\\Batch 41750\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:00] Epoch 5\\Batch 41800\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:05] Epoch 5\\Batch 41850\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:10] Epoch 5\\Batch 41900\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:15] Epoch 5\\Batch 41950\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:20] Epoch 5\\Batch 42000\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5470.000001, 'TP': 1529.0000009999999, 'FP': 1660.0000009999999}\n",
      "[2019/03/18 00:28:42] Epoch 5/ Validation Loss:9.873/ F1_score:0.300/ Precision:0.479/ Recall:0.218\n",
      "[2019/03/18 00:28:47] Epoch 5\\Batch 42050\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:52] Epoch 5\\Batch 42100\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:28:57] Epoch 5\\Batch 42150\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:02] Epoch 5\\Batch 42200\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:07] Epoch 5\\Batch 42250\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:12] Epoch 5\\Batch 42300\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:17] Epoch 5\\Batch 42350\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:22] Epoch 5\\Batch 42400\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:27] Epoch 5\\Batch 42450\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:32] Epoch 5\\Batch 42500\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:37] Epoch 5\\Batch 42550\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:41] Epoch 5\\Batch 42600\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:46] Epoch 5\\Batch 42650\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:51] Epoch 5\\Batch 42700\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:29:56] Epoch 5\\Batch 42750\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:01] Epoch 5\\Batch 42800\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:06] Epoch 5\\Batch 42850\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:11] Epoch 5\\Batch 42900\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:16] Epoch 5\\Batch 42950\\ Train Loss:10.329\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:30:21] Epoch 5\\Batch 43000\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:26] Epoch 5\\Batch 43050\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:31] Epoch 5\\Batch 43100\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:36] Epoch 5\\Batch 43150\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:41] Epoch 5\\Batch 43200\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:46] Epoch 5\\Batch 43250\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:51] Epoch 5\\Batch 43300\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:30:55] Epoch 5\\Batch 43350\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:00] Epoch 5\\Batch 43400\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:05] Epoch 5\\Batch 43450\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:10] Epoch 5\\Batch 43500\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:15] Epoch 5\\Batch 43550\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:20] Epoch 5\\Batch 43600\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:25] Epoch 5\\Batch 43650\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:30] Epoch 5\\Batch 43700\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:35] Epoch 5\\Batch 43750\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:40] Epoch 5\\Batch 43800\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:45] Epoch 5\\Batch 43850\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:50] Epoch 5\\Batch 43900\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:31:55] Epoch 5\\Batch 43950\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:00] Epoch 5\\Batch 44000\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:05] Epoch 5\\Batch 44050\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:10] Epoch 5\\Batch 44100\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:15] Epoch 5\\Batch 44150\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:19] Epoch 5\\Batch 44200\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:25] Epoch 5\\Batch 44250\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:29] Epoch 5\\Batch 44300\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:34] Epoch 5\\Batch 44350\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:39] Epoch 5\\Batch 44400\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:44] Epoch 5\\Batch 44450\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:49] Epoch 5\\Batch 44500\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:54] Epoch 5\\Batch 44550\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:32:59] Epoch 5\\Batch 44600\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:04] Epoch 5\\Batch 44650\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:09] Epoch 5\\Batch 44700\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:14] Epoch 5\\Batch 44750\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:19] Epoch 5\\Batch 44800\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:23] Epoch 5\\Batch 44850\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:28] Epoch 5\\Batch 44900\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:33] Epoch 5\\Batch 44950\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:33:38] Epoch 5\\Batch 45000\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5486.000001, 'TP': 1513.0000009999999, 'FP': 1666.0000009999999}\n",
      "[2019/03/18 00:34:01] Epoch 5/ Validation Loss:9.869/ F1_score:0.297/ Precision:0.476/ Recall:0.216\n",
      "[2019/03/18 00:34:06] Epoch 5\\Batch 45050\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:11] Epoch 5\\Batch 45100\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:15] Epoch 5\\Batch 45150\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:18] Epoch 5\\Batch 45200\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:22] Epoch 5\\Batch 45250\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:25] Epoch 5\\Batch 45300\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:28] Epoch 5\\Batch 45350\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:31] Epoch 5\\Batch 45400\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:35] Epoch 5\\Batch 45450\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:38] Epoch 5\\Batch 45500\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:43] Epoch 5\\Batch 45550\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:48] Epoch 5\\Batch 45600\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:53] Epoch 5\\Batch 45650\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:34:58] Epoch 5\\Batch 45700\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:03] Epoch 5\\Batch 45750\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:08] Epoch 5\\Batch 45800\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:13] Epoch 5\\Batch 45850\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:18] Epoch 5\\Batch 45900\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:22] Epoch 5\\Batch 45950\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:27] Epoch 5\\Batch 46000\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:32] Epoch 5\\Batch 46050\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:37] Epoch 5\\Batch 46100\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:42] Epoch 5\\Batch 46150\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:47] Epoch 5\\Batch 46200\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:35:57] Epoch 6\\Batch 50\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:02] Epoch 6\\Batch 100\\ Train Loss:10.373\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:07] Epoch 6\\Batch 150\\ Train Loss:10.410\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:12] Epoch 6\\Batch 200\\ Train Loss:10.360\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:17] Epoch 6\\Batch 250\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:22] Epoch 6\\Batch 300\\ Train Loss:10.367\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:27] Epoch 6\\Batch 350\\ Train Loss:10.371\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:31] Epoch 6\\Batch 400\\ Train Loss:10.369\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:36] Epoch 6\\Batch 450\\ Train Loss:10.352\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:41] Epoch 6\\Batch 500\\ Train Loss:10.340\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:46] Epoch 6\\Batch 550\\ Train Loss:10.322\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:51] Epoch 6\\Batch 600\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 00:36:56] Epoch 6\\Batch 650\\ Train Loss:10.325\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:01] Epoch 6\\Batch 700\\ Train Loss:10.329\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:06] Epoch 6\\Batch 750\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:11] Epoch 6\\Batch 800\\ Train Loss:10.324\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:16] Epoch 6\\Batch 850\\ Train Loss:10.327\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:21] Epoch 6\\Batch 900\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:26] Epoch 6\\Batch 950\\ Train Loss:10.324\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:31] Epoch 6\\Batch 1000\\ Train Loss:10.315\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:36] Epoch 6\\Batch 1050\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:41] Epoch 6\\Batch 1100\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:45] Epoch 6\\Batch 1150\\ Train Loss:10.316\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:50] Epoch 6\\Batch 1200\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:37:55] Epoch 6\\Batch 1250\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:00] Epoch 6\\Batch 1300\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:05] Epoch 6\\Batch 1350\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:10] Epoch 6\\Batch 1400\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:15] Epoch 6\\Batch 1450\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:20] Epoch 6\\Batch 1500\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:25] Epoch 6\\Batch 1550\\ Train Loss:10.301\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:38:30] Epoch 6\\Batch 1600\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:35] Epoch 6\\Batch 1650\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:40] Epoch 6\\Batch 1700\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:45] Epoch 6\\Batch 1750\\ Train Loss:10.298\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:50] Epoch 6\\Batch 1800\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 00:38:55] Epoch 6\\Batch 1850\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:00] Epoch 6\\Batch 1900\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:04] Epoch 6\\Batch 1950\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:09] Epoch 6\\Batch 2000\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:14] Epoch 6\\Batch 2050\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:19] Epoch 6\\Batch 2100\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:24] Epoch 6\\Batch 2150\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:29] Epoch 6\\Batch 2200\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:34] Epoch 6\\Batch 2250\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:39] Epoch 6\\Batch 2300\\ Train Loss:10.315\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:44] Epoch 6\\Batch 2350\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:49] Epoch 6\\Batch 2400\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:54] Epoch 6\\Batch 2450\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:39:59] Epoch 6\\Batch 2500\\ Train Loss:10.315\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:04] Epoch 6\\Batch 2550\\ Train Loss:10.315\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:09] Epoch 6\\Batch 2600\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:14] Epoch 6\\Batch 2650\\ Train Loss:10.316\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:19] Epoch 6\\Batch 2700\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:24] Epoch 6\\Batch 2750\\ Train Loss:10.319\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:29] Epoch 6\\Batch 2800\\ Train Loss:10.317\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:34] Epoch 6\\Batch 2850\\ Train Loss:10.318\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:38] Epoch 6\\Batch 2900\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:43] Epoch 6\\Batch 2950\\ Train Loss:10.316\\ Learning rate:0.00030\n",
      "[2019/03/18 00:40:48] Epoch 6\\Batch 3000\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5496.000001, 'TP': 1503.0000009999999, 'FP': 1669.0000009999999}\n",
      "[2019/03/18 00:41:11] Epoch 6/ Validation Loss:9.884/ F1_score:0.296/ Precision:0.474/ Recall:0.215\n",
      "[2019/03/18 00:41:16] Epoch 6\\Batch 3050\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:21] Epoch 6\\Batch 3100\\ Train Loss:10.317\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:25] Epoch 6\\Batch 3150\\ Train Loss:10.317\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:28] Epoch 6\\Batch 3200\\ Train Loss:10.315\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:32] Epoch 6\\Batch 3250\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:35] Epoch 6\\Batch 3300\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:38] Epoch 6\\Batch 3350\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:42] Epoch 6\\Batch 3400\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:45] Epoch 6\\Batch 3450\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:48] Epoch 6\\Batch 3500\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:53] Epoch 6\\Batch 3550\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:41:58] Epoch 6\\Batch 3600\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:03] Epoch 6\\Batch 3650\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:08] Epoch 6\\Batch 3700\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:13] Epoch 6\\Batch 3750\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:18] Epoch 6\\Batch 3800\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:23] Epoch 6\\Batch 3850\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:28] Epoch 6\\Batch 3900\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:33] Epoch 6\\Batch 3950\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:38] Epoch 6\\Batch 4000\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:43] Epoch 6\\Batch 4050\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:48] Epoch 6\\Batch 4100\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:53] Epoch 6\\Batch 4150\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:42:58] Epoch 6\\Batch 4200\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:03] Epoch 6\\Batch 4250\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:08] Epoch 6\\Batch 4300\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:14] Epoch 6\\Batch 4350\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:19] Epoch 6\\Batch 4400\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:24] Epoch 6\\Batch 4450\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:29] Epoch 6\\Batch 4500\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:35] Epoch 6\\Batch 4550\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:40] Epoch 6\\Batch 4600\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:45] Epoch 6\\Batch 4650\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:50] Epoch 6\\Batch 4700\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:43:56] Epoch 6\\Batch 4750\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:01] Epoch 6\\Batch 4800\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:06] Epoch 6\\Batch 4850\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:11] Epoch 6\\Batch 4900\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:17] Epoch 6\\Batch 4950\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:22] Epoch 6\\Batch 5000\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:27] Epoch 6\\Batch 5050\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:32] Epoch 6\\Batch 5100\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:37] Epoch 6\\Batch 5150\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:43] Epoch 6\\Batch 5200\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:48] Epoch 6\\Batch 5250\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:53] Epoch 6\\Batch 5300\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:44:58] Epoch 6\\Batch 5350\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:03] Epoch 6\\Batch 5400\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:08] Epoch 6\\Batch 5450\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:13] Epoch 6\\Batch 5500\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:18] Epoch 6\\Batch 5550\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:24] Epoch 6\\Batch 5600\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:29] Epoch 6\\Batch 5650\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:34] Epoch 6\\Batch 5700\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:39] Epoch 6\\Batch 5750\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:44] Epoch 6\\Batch 5800\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:50] Epoch 6\\Batch 5850\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:45:55] Epoch 6\\Batch 5900\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "[2019/03/18 00:46:00] Epoch 6\\Batch 5950\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "[2019/03/18 00:46:05] Epoch 6\\Batch 6000\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5475.000001, 'TP': 1524.0000009999999, 'FP': 1667.0000009999999}\n",
      "[2019/03/18 00:46:28] Epoch 6/ Validation Loss:9.859/ F1_score:0.299/ Precision:0.478/ Recall:0.218\n",
      "[2019/03/18 00:46:33] Epoch 6\\Batch 6050\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "[2019/03/18 00:46:38] Epoch 6\\Batch 6100\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:46:43] Epoch 6\\Batch 6150\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:46:48] Epoch 6\\Batch 6200\\ Train Loss:10.314\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:46:53] Epoch 6\\Batch 6250\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:46:58] Epoch 6\\Batch 6300\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:03] Epoch 6\\Batch 6350\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:08] Epoch 6\\Batch 6400\\ Train Loss:10.314\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:13] Epoch 6\\Batch 6450\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:18] Epoch 6\\Batch 6500\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:23] Epoch 6\\Batch 6550\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:28] Epoch 6\\Batch 6600\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:33] Epoch 6\\Batch 6650\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:38] Epoch 6\\Batch 6700\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:43] Epoch 6\\Batch 6750\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:48] Epoch 6\\Batch 6800\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:53] Epoch 6\\Batch 6850\\ Train Loss:10.312\\ Learning rate:0.00030\n",
      "[2019/03/18 00:47:58] Epoch 6\\Batch 6900\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:03] Epoch 6\\Batch 6950\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:08] Epoch 6\\Batch 7000\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:13] Epoch 6\\Batch 7050\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:18] Epoch 6\\Batch 7100\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:23] Epoch 6\\Batch 7150\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:27] Epoch 6\\Batch 7200\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:32] Epoch 6\\Batch 7250\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:37] Epoch 6\\Batch 7300\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:42] Epoch 6\\Batch 7350\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:46] Epoch 6\\Batch 7400\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:49] Epoch 6\\Batch 7450\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:52] Epoch 6\\Batch 7500\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:55] Epoch 6\\Batch 7550\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:48:59] Epoch 6\\Batch 7600\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:02] Epoch 6\\Batch 7650\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:05] Epoch 6\\Batch 7700\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:09] Epoch 6\\Batch 7750\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:14] Epoch 6\\Batch 7800\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:18] Epoch 6\\Batch 7850\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:23] Epoch 6\\Batch 7900\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:28] Epoch 6\\Batch 7950\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:33] Epoch 6\\Batch 8000\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:38] Epoch 6\\Batch 8050\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:44] Epoch 6\\Batch 8100\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:48] Epoch 6\\Batch 8150\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:53] Epoch 6\\Batch 8200\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:49:58] Epoch 6\\Batch 8250\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:03] Epoch 6\\Batch 8300\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:08] Epoch 6\\Batch 8350\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:13] Epoch 6\\Batch 8400\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:18] Epoch 6\\Batch 8450\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:23] Epoch 6\\Batch 8500\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:28] Epoch 6\\Batch 8550\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:33] Epoch 6\\Batch 8600\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:38] Epoch 6\\Batch 8650\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:43] Epoch 6\\Batch 8700\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:48] Epoch 6\\Batch 8750\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:53] Epoch 6\\Batch 8800\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:50:58] Epoch 6\\Batch 8850\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:03] Epoch 6\\Batch 8900\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:08] Epoch 6\\Batch 8950\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:13] Epoch 6\\Batch 9000\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5498.000001, 'TP': 1501.0000009999999, 'FP': 1680.0000009999999}\n",
      "[2019/03/18 00:51:36] Epoch 6/ Validation Loss:9.863/ F1_score:0.295/ Precision:0.472/ Recall:0.214\n",
      "[2019/03/18 00:51:41] Epoch 6\\Batch 9050\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:46] Epoch 6\\Batch 9100\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:50] Epoch 6\\Batch 9150\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:54] Epoch 6\\Batch 9200\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:51:57] Epoch 6\\Batch 9250\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:00] Epoch 6\\Batch 9300\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:03] Epoch 6\\Batch 9350\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:06] Epoch 6\\Batch 9400\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:10] Epoch 6\\Batch 9450\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:13] Epoch 6\\Batch 9500\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:16] Epoch 6\\Batch 9550\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:19] Epoch 6\\Batch 9600\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:22] Epoch 6\\Batch 9650\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:26] Epoch 6\\Batch 9700\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:29] Epoch 6\\Batch 9750\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:32] Epoch 6\\Batch 9800\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:35] Epoch 6\\Batch 9850\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:39] Epoch 6\\Batch 9900\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:44] Epoch 6\\Batch 9950\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:49] Epoch 6\\Batch 10000\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:54] Epoch 6\\Batch 10050\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:52:59] Epoch 6\\Batch 10100\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:03] Epoch 6\\Batch 10150\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:08] Epoch 6\\Batch 10200\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:13] Epoch 6\\Batch 10250\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:18] Epoch 6\\Batch 10300\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:23] Epoch 6\\Batch 10350\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:28] Epoch 6\\Batch 10400\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:33] Epoch 6\\Batch 10450\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:38] Epoch 6\\Batch 10500\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:43] Epoch 6\\Batch 10550\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:48] Epoch 6\\Batch 10600\\ Train Loss:10.310\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:53] Epoch 6\\Batch 10650\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:53:58] Epoch 6\\Batch 10700\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:03] Epoch 6\\Batch 10750\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:08] Epoch 6\\Batch 10800\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:13] Epoch 6\\Batch 10850\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:18] Epoch 6\\Batch 10900\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:23] Epoch 6\\Batch 10950\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:28] Epoch 6\\Batch 11000\\ Train Loss:10.309\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 00:54:33] Epoch 6\\Batch 11050\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:38] Epoch 6\\Batch 11100\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:43] Epoch 6\\Batch 11150\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:47] Epoch 6\\Batch 11200\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:52] Epoch 6\\Batch 11250\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:54:57] Epoch 6\\Batch 11300\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:02] Epoch 6\\Batch 11350\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:07] Epoch 6\\Batch 11400\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:12] Epoch 6\\Batch 11450\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:17] Epoch 6\\Batch 11500\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:22] Epoch 6\\Batch 11550\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:27] Epoch 6\\Batch 11600\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:32] Epoch 6\\Batch 11650\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:37] Epoch 6\\Batch 11700\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:41] Epoch 6\\Batch 11750\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:46] Epoch 6\\Batch 11800\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:51] Epoch 6\\Batch 11850\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:55:56] Epoch 6\\Batch 11900\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:01] Epoch 6\\Batch 11950\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:06] Epoch 6\\Batch 12000\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [34]\n",
      "{'FN': 5468.000001, 'TP': 1531.0000009999999, 'FP': 1679.0000009999999}\n",
      "[2019/03/18 00:56:29] Epoch 6/ Validation Loss:9.840/ F1_score:0.300/ Precision:0.477/ Recall:0.219\n",
      "[2019/03/18 00:56:34] Epoch 6\\Batch 12050\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:39] Epoch 6\\Batch 12100\\ Train Loss:10.308\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:44] Epoch 6\\Batch 12150\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:49] Epoch 6\\Batch 12200\\ Train Loss:10.309\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:54] Epoch 6\\Batch 12250\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:56:59] Epoch 6\\Batch 12300\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:04] Epoch 6\\Batch 12350\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:09] Epoch 6\\Batch 12400\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:14] Epoch 6\\Batch 12450\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:19] Epoch 6\\Batch 12500\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:24] Epoch 6\\Batch 12550\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:29] Epoch 6\\Batch 12600\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:34] Epoch 6\\Batch 12650\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:39] Epoch 6\\Batch 12700\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:44] Epoch 6\\Batch 12750\\ Train Loss:10.306\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:49] Epoch 6\\Batch 12800\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:54] Epoch 6\\Batch 12850\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 00:57:59] Epoch 6\\Batch 12900\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:04] Epoch 6\\Batch 12950\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:09] Epoch 6\\Batch 13000\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:14] Epoch 6\\Batch 13050\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:19] Epoch 6\\Batch 13100\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:24] Epoch 6\\Batch 13150\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:29] Epoch 6\\Batch 13200\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:34] Epoch 6\\Batch 13250\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:39] Epoch 6\\Batch 13300\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:44] Epoch 6\\Batch 13350\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:49] Epoch 6\\Batch 13400\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:54] Epoch 6\\Batch 13450\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:58:59] Epoch 6\\Batch 13500\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:04] Epoch 6\\Batch 13550\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:09] Epoch 6\\Batch 13600\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:13] Epoch 6\\Batch 13650\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:18] Epoch 6\\Batch 13700\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:23] Epoch 6\\Batch 13750\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:27] Epoch 6\\Batch 13800\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:30] Epoch 6\\Batch 13850\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:33] Epoch 6\\Batch 13900\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:37] Epoch 6\\Batch 13950\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:40] Epoch 6\\Batch 14000\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:43] Epoch 6\\Batch 14050\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:47] Epoch 6\\Batch 14100\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:51] Epoch 6\\Batch 14150\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 00:59:56] Epoch 6\\Batch 14200\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:01] Epoch 6\\Batch 14250\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:07] Epoch 6\\Batch 14300\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:12] Epoch 6\\Batch 14350\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:17] Epoch 6\\Batch 14400\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:22] Epoch 6\\Batch 14450\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:27] Epoch 6\\Batch 14500\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:32] Epoch 6\\Batch 14550\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:37] Epoch 6\\Batch 14600\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:42] Epoch 6\\Batch 14650\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:47] Epoch 6\\Batch 14700\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:53] Epoch 6\\Batch 14750\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:00:58] Epoch 6\\Batch 14800\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:01:03] Epoch 6\\Batch 14850\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:01:08] Epoch 6\\Batch 14900\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:01:13] Epoch 6\\Batch 14950\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:01:18] Epoch 6\\Batch 15000\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5471.000001, 'TP': 1528.0000009999999, 'FP': 1651.0000009999999}\n",
      "[2019/03/18 01:01:41] Epoch 6/ Validation Loss:9.846/ F1_score:0.300/ Precision:0.481/ Recall:0.218\n",
      "[2019/03/18 01:01:46] Epoch 6\\Batch 15050\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:01:52] Epoch 6\\Batch 15100\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:01:57] Epoch 6\\Batch 15150\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:02] Epoch 6\\Batch 15200\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:07] Epoch 6\\Batch 15250\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:12] Epoch 6\\Batch 15300\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:18] Epoch 6\\Batch 15350\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:23] Epoch 6\\Batch 15400\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:28] Epoch 6\\Batch 15450\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:33] Epoch 6\\Batch 15500\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:38] Epoch 6\\Batch 15550\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:43] Epoch 6\\Batch 15600\\ Train Loss:10.305\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:02:48] Epoch 6\\Batch 15650\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:54] Epoch 6\\Batch 15700\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:02:59] Epoch 6\\Batch 15750\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:04] Epoch 6\\Batch 15800\\ Train Loss:10.305\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:09] Epoch 6\\Batch 15850\\ Train Loss:10.304\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:14] Epoch 6\\Batch 15900\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:19] Epoch 6\\Batch 15950\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:24] Epoch 6\\Batch 16000\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:29] Epoch 6\\Batch 16050\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:34] Epoch 6\\Batch 16100\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:38] Epoch 6\\Batch 16150\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:44] Epoch 6\\Batch 16200\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:49] Epoch 6\\Batch 16250\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:54] Epoch 6\\Batch 16300\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:03:59] Epoch 6\\Batch 16350\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:04] Epoch 6\\Batch 16400\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:09] Epoch 6\\Batch 16450\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:14] Epoch 6\\Batch 16500\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:19] Epoch 6\\Batch 16550\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:25] Epoch 6\\Batch 16600\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:30] Epoch 6\\Batch 16650\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:35] Epoch 6\\Batch 16700\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:40] Epoch 6\\Batch 16750\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:45] Epoch 6\\Batch 16800\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:50] Epoch 6\\Batch 16850\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:04:55] Epoch 6\\Batch 16900\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:00] Epoch 6\\Batch 16950\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:06] Epoch 6\\Batch 17000\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:11] Epoch 6\\Batch 17050\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:16] Epoch 6\\Batch 17100\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:21] Epoch 6\\Batch 17150\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:26] Epoch 6\\Batch 17200\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:30] Epoch 6\\Batch 17250\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:35] Epoch 6\\Batch 17300\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:40] Epoch 6\\Batch 17350\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:45] Epoch 6\\Batch 17400\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:50] Epoch 6\\Batch 17450\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:05:55] Epoch 6\\Batch 17500\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:00] Epoch 6\\Batch 17550\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:05] Epoch 6\\Batch 17600\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:10] Epoch 6\\Batch 17650\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:15] Epoch 6\\Batch 17700\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:20] Epoch 6\\Batch 17750\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:26] Epoch 6\\Batch 17800\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:31] Epoch 6\\Batch 17850\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:36] Epoch 6\\Batch 17900\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:40] Epoch 6\\Batch 17950\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:06:43] Epoch 6\\Batch 18000\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5476.000001, 'TP': 1523.0000009999999, 'FP': 1680.0000009999999}\n",
      "[2019/03/18 01:07:01] Epoch 6/ Validation Loss:9.842/ F1_score:0.299/ Precision:0.475/ Recall:0.218\n",
      "[2019/03/18 01:07:06] Epoch 6\\Batch 18050\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:11] Epoch 6\\Batch 18100\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:16] Epoch 6\\Batch 18150\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:21] Epoch 6\\Batch 18200\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:26] Epoch 6\\Batch 18250\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:31] Epoch 6\\Batch 18300\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:36] Epoch 6\\Batch 18350\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:42] Epoch 6\\Batch 18400\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:47] Epoch 6\\Batch 18450\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:52] Epoch 6\\Batch 18500\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:07:57] Epoch 6\\Batch 18550\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:02] Epoch 6\\Batch 18600\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:08] Epoch 6\\Batch 18650\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:13] Epoch 6\\Batch 18700\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:18] Epoch 6\\Batch 18750\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:23] Epoch 6\\Batch 18800\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:28] Epoch 6\\Batch 18850\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:33] Epoch 6\\Batch 18900\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:38] Epoch 6\\Batch 18950\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:43] Epoch 6\\Batch 19000\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:48] Epoch 6\\Batch 19050\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:53] Epoch 6\\Batch 19100\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:08:58] Epoch 6\\Batch 19150\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:03] Epoch 6\\Batch 19200\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:08] Epoch 6\\Batch 19250\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:13] Epoch 6\\Batch 19300\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:18] Epoch 6\\Batch 19350\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:23] Epoch 6\\Batch 19400\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:28] Epoch 6\\Batch 19450\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:33] Epoch 6\\Batch 19500\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:38] Epoch 6\\Batch 19550\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:43] Epoch 6\\Batch 19600\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:48] Epoch 6\\Batch 19650\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:53] Epoch 6\\Batch 19700\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:09:58] Epoch 6\\Batch 19750\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:03] Epoch 6\\Batch 19800\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:08] Epoch 6\\Batch 19850\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:13] Epoch 6\\Batch 19900\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:18] Epoch 6\\Batch 19950\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:23] Epoch 6\\Batch 20000\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:28] Epoch 6\\Batch 20050\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:33] Epoch 6\\Batch 20100\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:38] Epoch 6\\Batch 20150\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:43] Epoch 6\\Batch 20200\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:48] Epoch 6\\Batch 20250\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:53] Epoch 6\\Batch 20300\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:10:58] Epoch 6\\Batch 20350\\ Train Loss:10.303\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:11:03] Epoch 6\\Batch 20400\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:08] Epoch 6\\Batch 20450\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:13] Epoch 6\\Batch 20500\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:18] Epoch 6\\Batch 20550\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:23] Epoch 6\\Batch 20600\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:27] Epoch 6\\Batch 20650\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:32] Epoch 6\\Batch 20700\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:38] Epoch 6\\Batch 20750\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:42] Epoch 6\\Batch 20800\\ Train Loss:10.303\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:47] Epoch 6\\Batch 20850\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:52] Epoch 6\\Batch 20900\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:11:57] Epoch 6\\Batch 20950\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:12:02] Epoch 6\\Batch 21000\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5441.000001, 'TP': 1558.0000009999999, 'FP': 1661.0000009999999}\n",
      "[2019/03/18 01:12:25] Epoch 6/ Validation Loss:9.852/ F1_score:0.305/ Precision:0.484/ Recall:0.223\n",
      "[2019/03/18 01:12:30] Epoch 6\\Batch 21050\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:12:35] Epoch 6\\Batch 21100\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:12:40] Epoch 6\\Batch 21150\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:12:45] Epoch 6\\Batch 21200\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:12:50] Epoch 6\\Batch 21250\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:12:55] Epoch 6\\Batch 21300\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:00] Epoch 6\\Batch 21350\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:05] Epoch 6\\Batch 21400\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:10] Epoch 6\\Batch 21450\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:15] Epoch 6\\Batch 21500\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:20] Epoch 6\\Batch 21550\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:25] Epoch 6\\Batch 21600\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:30] Epoch 6\\Batch 21650\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:35] Epoch 6\\Batch 21700\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:40] Epoch 6\\Batch 21750\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:44] Epoch 6\\Batch 21800\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:47] Epoch 6\\Batch 21850\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:50] Epoch 6\\Batch 21900\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:54] Epoch 6\\Batch 21950\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:13:57] Epoch 6\\Batch 22000\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:01] Epoch 6\\Batch 22050\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:04] Epoch 6\\Batch 22100\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:09] Epoch 6\\Batch 22150\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:14] Epoch 6\\Batch 22200\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:19] Epoch 6\\Batch 22250\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:24] Epoch 6\\Batch 22300\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:29] Epoch 6\\Batch 22350\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:34] Epoch 6\\Batch 22400\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:39] Epoch 6\\Batch 22450\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:44] Epoch 6\\Batch 22500\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:49] Epoch 6\\Batch 22550\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:54] Epoch 6\\Batch 22600\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:14:59] Epoch 6\\Batch 22650\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:04] Epoch 6\\Batch 22700\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:09] Epoch 6\\Batch 22750\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:14] Epoch 6\\Batch 22800\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:19] Epoch 6\\Batch 22850\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:24] Epoch 6\\Batch 22900\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:29] Epoch 6\\Batch 22950\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:35] Epoch 6\\Batch 23000\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:40] Epoch 6\\Batch 23050\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:45] Epoch 6\\Batch 23100\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:50] Epoch 6\\Batch 23150\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:15:55] Epoch 6\\Batch 23200\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:00] Epoch 6\\Batch 23250\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:05] Epoch 6\\Batch 23300\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:11] Epoch 6\\Batch 23350\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:16] Epoch 6\\Batch 23400\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:21] Epoch 6\\Batch 23450\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:26] Epoch 6\\Batch 23500\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:31] Epoch 6\\Batch 23550\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:36] Epoch 6\\Batch 23600\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:41] Epoch 6\\Batch 23650\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:46] Epoch 6\\Batch 23700\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:51] Epoch 6\\Batch 23750\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:16:56] Epoch 6\\Batch 23800\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:01] Epoch 6\\Batch 23850\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:05] Epoch 6\\Batch 23900\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:10] Epoch 6\\Batch 23950\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:16] Epoch 6\\Batch 24000\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5462.000001, 'TP': 1537.0000009999999, 'FP': 1668.0000009999999}\n",
      "[2019/03/18 01:17:38] Epoch 6/ Validation Loss:9.833/ F1_score:0.301/ Precision:0.480/ Recall:0.220\n",
      "[2019/03/18 01:17:44] Epoch 6\\Batch 24050\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:49] Epoch 6\\Batch 24100\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:54] Epoch 6\\Batch 24150\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:17:59] Epoch 6\\Batch 24200\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:04] Epoch 6\\Batch 24250\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:09] Epoch 6\\Batch 24300\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:14] Epoch 6\\Batch 24350\\ Train Loss:10.298\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:19] Epoch 6\\Batch 24400\\ Train Loss:10.298\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:24] Epoch 6\\Batch 24450\\ Train Loss:10.298\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:29] Epoch 6\\Batch 24500\\ Train Loss:10.298\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:34] Epoch 6\\Batch 24550\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:39] Epoch 6\\Batch 24600\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:44] Epoch 6\\Batch 24650\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:49] Epoch 6\\Batch 24700\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:54] Epoch 6\\Batch 24750\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 01:18:59] Epoch 6\\Batch 24800\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:05] Epoch 6\\Batch 24850\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:10] Epoch 6\\Batch 24900\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:15] Epoch 6\\Batch 24950\\ Train Loss:10.296\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:19:20] Epoch 6\\Batch 25000\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:25] Epoch 6\\Batch 25050\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:30] Epoch 6\\Batch 25100\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:35] Epoch 6\\Batch 25150\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:40] Epoch 6\\Batch 25200\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:45] Epoch 6\\Batch 25250\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:50] Epoch 6\\Batch 25300\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:19:55] Epoch 6\\Batch 25350\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:00] Epoch 6\\Batch 25400\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:05] Epoch 6\\Batch 25450\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:10] Epoch 6\\Batch 25500\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:15] Epoch 6\\Batch 25550\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:20] Epoch 6\\Batch 25600\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:25] Epoch 6\\Batch 25650\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:30] Epoch 6\\Batch 25700\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:35] Epoch 6\\Batch 25750\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:40] Epoch 6\\Batch 25800\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:45] Epoch 6\\Batch 25850\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:50] Epoch 6\\Batch 25900\\ Train Loss:10.295\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:54] Epoch 6\\Batch 25950\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:20:57] Epoch 6\\Batch 26000\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:01] Epoch 6\\Batch 26050\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:04] Epoch 6\\Batch 26100\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:08] Epoch 6\\Batch 26150\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:11] Epoch 6\\Batch 26200\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:14] Epoch 6\\Batch 26250\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:18] Epoch 6\\Batch 26300\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:23] Epoch 6\\Batch 26350\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:28] Epoch 6\\Batch 26400\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:33] Epoch 6\\Batch 26450\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:38] Epoch 6\\Batch 26500\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:43] Epoch 6\\Batch 26550\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:48] Epoch 6\\Batch 26600\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:53] Epoch 6\\Batch 26650\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:21:58] Epoch 6\\Batch 26700\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:22:03] Epoch 6\\Batch 26750\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:22:08] Epoch 6\\Batch 26800\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:22:13] Epoch 6\\Batch 26850\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:22:18] Epoch 6\\Batch 26900\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:22:23] Epoch 6\\Batch 26950\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:22:28] Epoch 6\\Batch 27000\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5468.000001, 'TP': 1531.0000009999999, 'FP': 1672.0000009999999}\n",
      "[2019/03/18 01:22:51] Epoch 6/ Validation Loss:9.836/ F1_score:0.300/ Precision:0.478/ Recall:0.219\n",
      "[2019/03/18 01:22:56] Epoch 6\\Batch 27050\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:01] Epoch 6\\Batch 27100\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:06] Epoch 6\\Batch 27150\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:11] Epoch 6\\Batch 27200\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:16] Epoch 6\\Batch 27250\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:21] Epoch 6\\Batch 27300\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:27] Epoch 6\\Batch 27350\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:32] Epoch 6\\Batch 27400\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:37] Epoch 6\\Batch 27450\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:42] Epoch 6\\Batch 27500\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:47] Epoch 6\\Batch 27550\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:52] Epoch 6\\Batch 27600\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:23:57] Epoch 6\\Batch 27650\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:02] Epoch 6\\Batch 27700\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:07] Epoch 6\\Batch 27750\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:12] Epoch 6\\Batch 27800\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:17] Epoch 6\\Batch 27850\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:22] Epoch 6\\Batch 27900\\ Train Loss:10.294\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:27] Epoch 6\\Batch 27950\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:33] Epoch 6\\Batch 28000\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:38] Epoch 6\\Batch 28050\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:43] Epoch 6\\Batch 28100\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:48] Epoch 6\\Batch 28150\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:53] Epoch 6\\Batch 28200\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:24:58] Epoch 6\\Batch 28250\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:03] Epoch 6\\Batch 28300\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:08] Epoch 6\\Batch 28350\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:13] Epoch 6\\Batch 28400\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:18] Epoch 6\\Batch 28450\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:23] Epoch 6\\Batch 28500\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:28] Epoch 6\\Batch 28550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:33] Epoch 6\\Batch 28600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:38] Epoch 6\\Batch 28650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:43] Epoch 6\\Batch 28700\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:48] Epoch 6\\Batch 28750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:53] Epoch 6\\Batch 28800\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:25:58] Epoch 6\\Batch 28850\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:03] Epoch 6\\Batch 28900\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:08] Epoch 6\\Batch 28950\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:13] Epoch 6\\Batch 29000\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:18] Epoch 6\\Batch 29050\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:23] Epoch 6\\Batch 29100\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:28] Epoch 6\\Batch 29150\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:33] Epoch 6\\Batch 29200\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:38] Epoch 6\\Batch 29250\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:43] Epoch 6\\Batch 29300\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:48] Epoch 6\\Batch 29350\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:53] Epoch 6\\Batch 29400\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:26:58] Epoch 6\\Batch 29450\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:03] Epoch 6\\Batch 29500\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:08] Epoch 6\\Batch 29550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:13] Epoch 6\\Batch 29600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:18] Epoch 6\\Batch 29650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:23] Epoch 6\\Batch 29700\\ Train Loss:10.292\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:27:28] Epoch 6\\Batch 29750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:33] Epoch 6\\Batch 29800\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:38] Epoch 6\\Batch 29850\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:43] Epoch 6\\Batch 29900\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:48] Epoch 6\\Batch 29950\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:27:53] Epoch 6\\Batch 30000\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5484.000001, 'TP': 1515.0000009999999, 'FP': 1679.0000009999999}\n",
      "[2019/03/18 01:28:11] Epoch 6/ Validation Loss:9.815/ F1_score:0.297/ Precision:0.474/ Recall:0.216\n",
      "[2019/03/18 01:28:15] Epoch 6\\Batch 30050\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:18] Epoch 6\\Batch 30100\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:23] Epoch 6\\Batch 30150\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:28] Epoch 6\\Batch 30200\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:33] Epoch 6\\Batch 30250\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:38] Epoch 6\\Batch 30300\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:43] Epoch 6\\Batch 30350\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:47] Epoch 6\\Batch 30400\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:53] Epoch 6\\Batch 30450\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:28:57] Epoch 6\\Batch 30500\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:02] Epoch 6\\Batch 30550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:07] Epoch 6\\Batch 30600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:12] Epoch 6\\Batch 30650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:17] Epoch 6\\Batch 30700\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:22] Epoch 6\\Batch 30750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:27] Epoch 6\\Batch 30800\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:32] Epoch 6\\Batch 30850\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:37] Epoch 6\\Batch 30900\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:42] Epoch 6\\Batch 30950\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:47] Epoch 6\\Batch 31000\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:52] Epoch 6\\Batch 31050\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:29:57] Epoch 6\\Batch 31100\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:02] Epoch 6\\Batch 31150\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:07] Epoch 6\\Batch 31200\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:12] Epoch 6\\Batch 31250\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:17] Epoch 6\\Batch 31300\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:22] Epoch 6\\Batch 31350\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:27] Epoch 6\\Batch 31400\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:32] Epoch 6\\Batch 31450\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:37] Epoch 6\\Batch 31500\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:42] Epoch 6\\Batch 31550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:47] Epoch 6\\Batch 31600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:51] Epoch 6\\Batch 31650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:30:56] Epoch 6\\Batch 31700\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:01] Epoch 6\\Batch 31750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:06] Epoch 6\\Batch 31800\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:11] Epoch 6\\Batch 31850\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:16] Epoch 6\\Batch 31900\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:21] Epoch 6\\Batch 31950\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:26] Epoch 6\\Batch 32000\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:31] Epoch 6\\Batch 32050\\ Train Loss:10.293\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:36] Epoch 6\\Batch 32100\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:41] Epoch 6\\Batch 32150\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:46] Epoch 6\\Batch 32200\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:51] Epoch 6\\Batch 32250\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:31:56] Epoch 6\\Batch 32300\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:01] Epoch 6\\Batch 32350\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:06] Epoch 6\\Batch 32400\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:11] Epoch 6\\Batch 32450\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:16] Epoch 6\\Batch 32500\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:21] Epoch 6\\Batch 32550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:26] Epoch 6\\Batch 32600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:31] Epoch 6\\Batch 32650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:36] Epoch 6\\Batch 32700\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:41] Epoch 6\\Batch 32750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:46] Epoch 6\\Batch 32800\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:50] Epoch 6\\Batch 32850\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:32:55] Epoch 6\\Batch 32900\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:00] Epoch 6\\Batch 32950\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:05] Epoch 6\\Batch 33000\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5478.000001, 'TP': 1521.0000009999999, 'FP': 1685.0000009999999}\n",
      "[2019/03/18 01:33:28] Epoch 6/ Validation Loss:9.836/ F1_score:0.298/ Precision:0.474/ Recall:0.217\n",
      "[2019/03/18 01:33:33] Epoch 6\\Batch 33050\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:38] Epoch 6\\Batch 33100\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:43] Epoch 6\\Batch 33150\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:48] Epoch 6\\Batch 33200\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:53] Epoch 6\\Batch 33250\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:33:58] Epoch 6\\Batch 33300\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:03] Epoch 6\\Batch 33350\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:08] Epoch 6\\Batch 33400\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:12] Epoch 6\\Batch 33450\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:17] Epoch 6\\Batch 33500\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:22] Epoch 6\\Batch 33550\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:27] Epoch 6\\Batch 33600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:32] Epoch 6\\Batch 33650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:37] Epoch 6\\Batch 33700\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:42] Epoch 6\\Batch 33750\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:47] Epoch 6\\Batch 33800\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:52] Epoch 6\\Batch 33850\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:34:57] Epoch 6\\Batch 33900\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:01] Epoch 6\\Batch 33950\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:05] Epoch 6\\Batch 34000\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:08] Epoch 6\\Batch 34050\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:11] Epoch 6\\Batch 34100\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:14] Epoch 6\\Batch 34150\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:18] Epoch 6\\Batch 34200\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:21] Epoch 6\\Batch 34250\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:24] Epoch 6\\Batch 34300\\ Train Loss:10.291\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:35:29] Epoch 6\\Batch 34350\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:34] Epoch 6\\Batch 34400\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:39] Epoch 6\\Batch 34450\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:44] Epoch 6\\Batch 34500\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:49] Epoch 6\\Batch 34550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:54] Epoch 6\\Batch 34600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:35:59] Epoch 6\\Batch 34650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:03] Epoch 6\\Batch 34700\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:08] Epoch 6\\Batch 34750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:13] Epoch 6\\Batch 34800\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:18] Epoch 6\\Batch 34850\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:23] Epoch 6\\Batch 34900\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:28] Epoch 6\\Batch 34950\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:33] Epoch 6\\Batch 35000\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:38] Epoch 6\\Batch 35050\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:43] Epoch 6\\Batch 35100\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:48] Epoch 6\\Batch 35150\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:53] Epoch 6\\Batch 35200\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:36:58] Epoch 6\\Batch 35250\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:02] Epoch 6\\Batch 35300\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:07] Epoch 6\\Batch 35350\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:12] Epoch 6\\Batch 35400\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:17] Epoch 6\\Batch 35450\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:22] Epoch 6\\Batch 35500\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:27] Epoch 6\\Batch 35550\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:32] Epoch 6\\Batch 35600\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:37] Epoch 6\\Batch 35650\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:42] Epoch 6\\Batch 35700\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:47] Epoch 6\\Batch 35750\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:52] Epoch 6\\Batch 35800\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:37:57] Epoch 6\\Batch 35850\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:02] Epoch 6\\Batch 35900\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:07] Epoch 6\\Batch 35950\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:12] Epoch 6\\Batch 36000\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5476.000001, 'TP': 1523.0000009999999, 'FP': 1665.0000009999999}\n",
      "[2019/03/18 01:38:34] Epoch 6/ Validation Loss:9.828/ F1_score:0.299/ Precision:0.478/ Recall:0.218\n",
      "[2019/03/18 01:38:39] Epoch 6\\Batch 36050\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:44] Epoch 6\\Batch 36100\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:49] Epoch 6\\Batch 36150\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:54] Epoch 6\\Batch 36200\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:38:59] Epoch 6\\Batch 36250\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:04] Epoch 6\\Batch 36300\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:09] Epoch 6\\Batch 36350\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:14] Epoch 6\\Batch 36400\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:19] Epoch 6\\Batch 36450\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:24] Epoch 6\\Batch 36500\\ Train Loss:10.291\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:29] Epoch 6\\Batch 36550\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:34] Epoch 6\\Batch 36600\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:38] Epoch 6\\Batch 36650\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:43] Epoch 6\\Batch 36700\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:48] Epoch 6\\Batch 36750\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:53] Epoch 6\\Batch 36800\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:39:58] Epoch 6\\Batch 36850\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:03] Epoch 6\\Batch 36900\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:08] Epoch 6\\Batch 36950\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:13] Epoch 6\\Batch 37000\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:18] Epoch 6\\Batch 37050\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:23] Epoch 6\\Batch 37100\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:28] Epoch 6\\Batch 37150\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:32] Epoch 6\\Batch 37200\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:38] Epoch 6\\Batch 37250\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:42] Epoch 6\\Batch 37300\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:47] Epoch 6\\Batch 37350\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:52] Epoch 6\\Batch 37400\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:40:57] Epoch 6\\Batch 37450\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:02] Epoch 6\\Batch 37500\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:07] Epoch 6\\Batch 37550\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:12] Epoch 6\\Batch 37600\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:17] Epoch 6\\Batch 37650\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:22] Epoch 6\\Batch 37700\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:27] Epoch 6\\Batch 37750\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:32] Epoch 6\\Batch 37800\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:37] Epoch 6\\Batch 37850\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:42] Epoch 6\\Batch 37900\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:47] Epoch 6\\Batch 37950\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:52] Epoch 6\\Batch 38000\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:41:57] Epoch 6\\Batch 38050\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:02] Epoch 6\\Batch 38100\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:07] Epoch 6\\Batch 38150\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:11] Epoch 6\\Batch 38200\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:14] Epoch 6\\Batch 38250\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:18] Epoch 6\\Batch 38300\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:21] Epoch 6\\Batch 38350\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:24] Epoch 6\\Batch 38400\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:28] Epoch 6\\Batch 38450\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:31] Epoch 6\\Batch 38500\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:36] Epoch 6\\Batch 38550\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:41] Epoch 6\\Batch 38600\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:46] Epoch 6\\Batch 38650\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:51] Epoch 6\\Batch 38700\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:42:56] Epoch 6\\Batch 38750\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:01] Epoch 6\\Batch 38800\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:06] Epoch 6\\Batch 38850\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:11] Epoch 6\\Batch 38900\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:16] Epoch 6\\Batch 38950\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:21] Epoch 6\\Batch 39000\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5468.000001, 'TP': 1531.0000009999999, 'FP': 1675.0000009999999}\n",
      "[2019/03/18 01:43:44] Epoch 6/ Validation Loss:9.838/ F1_score:0.300/ Precision:0.478/ Recall:0.219\n",
      "[2019/03/18 01:43:49] Epoch 6\\Batch 39050\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:54] Epoch 6\\Batch 39100\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:43:59] Epoch 6\\Batch 39150\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:04] Epoch 6\\Batch 39200\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:09] Epoch 6\\Batch 39250\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:14] Epoch 6\\Batch 39300\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:19] Epoch 6\\Batch 39350\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:24] Epoch 6\\Batch 39400\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:29] Epoch 6\\Batch 39450\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:34] Epoch 6\\Batch 39500\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:39] Epoch 6\\Batch 39550\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:44] Epoch 6\\Batch 39600\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:49] Epoch 6\\Batch 39650\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:54] Epoch 6\\Batch 39700\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:44:59] Epoch 6\\Batch 39750\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:04] Epoch 6\\Batch 39800\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:09] Epoch 6\\Batch 39850\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:14] Epoch 6\\Batch 39900\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:19] Epoch 6\\Batch 39950\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:24] Epoch 6\\Batch 40000\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:29] Epoch 6\\Batch 40050\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:33] Epoch 6\\Batch 40100\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:38] Epoch 6\\Batch 40150\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:43] Epoch 6\\Batch 40200\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:48] Epoch 6\\Batch 40250\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:53] Epoch 6\\Batch 40300\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:45:58] Epoch 6\\Batch 40350\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:03] Epoch 6\\Batch 40400\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:08] Epoch 6\\Batch 40450\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:13] Epoch 6\\Batch 40500\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:18] Epoch 6\\Batch 40550\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:23] Epoch 6\\Batch 40600\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:28] Epoch 6\\Batch 40650\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:33] Epoch 6\\Batch 40700\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:37] Epoch 6\\Batch 40750\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:42] Epoch 6\\Batch 40800\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:48] Epoch 6\\Batch 40850\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:52] Epoch 6\\Batch 40900\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:46:57] Epoch 6\\Batch 40950\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:02] Epoch 6\\Batch 41000\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:07] Epoch 6\\Batch 41050\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:12] Epoch 6\\Batch 41100\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:17] Epoch 6\\Batch 41150\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:22] Epoch 6\\Batch 41200\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:27] Epoch 6\\Batch 41250\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:31] Epoch 6\\Batch 41300\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:36] Epoch 6\\Batch 41350\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:41] Epoch 6\\Batch 41400\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:46] Epoch 6\\Batch 41450\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:51] Epoch 6\\Batch 41500\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:47:56] Epoch 6\\Batch 41550\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:01] Epoch 6\\Batch 41600\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:06] Epoch 6\\Batch 41650\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:11] Epoch 6\\Batch 41700\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:16] Epoch 6\\Batch 41750\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:21] Epoch 6\\Batch 41800\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:26] Epoch 6\\Batch 41850\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:30] Epoch 6\\Batch 41900\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:35] Epoch 6\\Batch 41950\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:48:40] Epoch 6\\Batch 42000\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5448.000001, 'TP': 1551.0000009999999, 'FP': 1630.0000009999999}\n",
      "[2019/03/18 01:49:03] Epoch 6/ Validation Loss:9.808/ F1_score:0.305/ Precision:0.488/ Recall:0.222\n",
      "[2019/03/18 01:49:07] Epoch 6\\Batch 42050\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:10] Epoch 6\\Batch 42100\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:14] Epoch 6\\Batch 42150\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:17] Epoch 6\\Batch 42200\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:20] Epoch 6\\Batch 42250\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:23] Epoch 6\\Batch 42300\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:27] Epoch 6\\Batch 42350\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:31] Epoch 6\\Batch 42400\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:35] Epoch 6\\Batch 42450\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:40] Epoch 6\\Batch 42500\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:45] Epoch 6\\Batch 42550\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:50] Epoch 6\\Batch 42600\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:49:55] Epoch 6\\Batch 42650\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:00] Epoch 6\\Batch 42700\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:05] Epoch 6\\Batch 42750\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:10] Epoch 6\\Batch 42800\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:15] Epoch 6\\Batch 42850\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:20] Epoch 6\\Batch 42900\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:25] Epoch 6\\Batch 42950\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:30] Epoch 6\\Batch 43000\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:35] Epoch 6\\Batch 43050\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:40] Epoch 6\\Batch 43100\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:45] Epoch 6\\Batch 43150\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:50] Epoch 6\\Batch 43200\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:50:55] Epoch 6\\Batch 43250\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:00] Epoch 6\\Batch 43300\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:05] Epoch 6\\Batch 43350\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:10] Epoch 6\\Batch 43400\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:15] Epoch 6\\Batch 43450\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:20] Epoch 6\\Batch 43500\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:25] Epoch 6\\Batch 43550\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:30] Epoch 6\\Batch 43600\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:35] Epoch 6\\Batch 43650\\ Train Loss:10.287\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:51:40] Epoch 6\\Batch 43700\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:45] Epoch 6\\Batch 43750\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:50] Epoch 6\\Batch 43800\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:51:55] Epoch 6\\Batch 43850\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:00] Epoch 6\\Batch 43900\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:05] Epoch 6\\Batch 43950\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:10] Epoch 6\\Batch 44000\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:15] Epoch 6\\Batch 44050\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:20] Epoch 6\\Batch 44100\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:25] Epoch 6\\Batch 44150\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:29] Epoch 6\\Batch 44200\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:34] Epoch 6\\Batch 44250\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:40] Epoch 6\\Batch 44300\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:44] Epoch 6\\Batch 44350\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:49] Epoch 6\\Batch 44400\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:54] Epoch 6\\Batch 44450\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:52:59] Epoch 6\\Batch 44500\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:04] Epoch 6\\Batch 44550\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:09] Epoch 6\\Batch 44600\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:14] Epoch 6\\Batch 44650\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:19] Epoch 6\\Batch 44700\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:24] Epoch 6\\Batch 44750\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:29] Epoch 6\\Batch 44800\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:34] Epoch 6\\Batch 44850\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:39] Epoch 6\\Batch 44900\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:44] Epoch 6\\Batch 44950\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:53:49] Epoch 6\\Batch 45000\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5462.000001, 'TP': 1537.0000009999999, 'FP': 1642.0000009999999}\n",
      "[2019/03/18 01:54:11] Epoch 6/ Validation Loss:9.822/ F1_score:0.302/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 01:54:17] Epoch 6\\Batch 45050\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:22] Epoch 6\\Batch 45100\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:27] Epoch 6\\Batch 45150\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:32] Epoch 6\\Batch 45200\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:37] Epoch 6\\Batch 45250\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:42] Epoch 6\\Batch 45300\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:47] Epoch 6\\Batch 45350\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:52] Epoch 6\\Batch 45400\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:54:57] Epoch 6\\Batch 45450\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:02] Epoch 6\\Batch 45500\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:07] Epoch 6\\Batch 45550\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:12] Epoch 6\\Batch 45600\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:17] Epoch 6\\Batch 45650\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:22] Epoch 6\\Batch 45700\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:27] Epoch 6\\Batch 45750\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:32] Epoch 6\\Batch 45800\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:37] Epoch 6\\Batch 45850\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:42] Epoch 6\\Batch 45900\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:47] Epoch 6\\Batch 45950\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:52] Epoch 6\\Batch 46000\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:55:57] Epoch 6\\Batch 46050\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:02] Epoch 6\\Batch 46100\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:06] Epoch 6\\Batch 46150\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:11] Epoch 6\\Batch 46200\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:19] Epoch 7\\Batch 50\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:22] Epoch 7\\Batch 100\\ Train Loss:10.338\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:25] Epoch 7\\Batch 150\\ Train Loss:10.369\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:29] Epoch 7\\Batch 200\\ Train Loss:10.333\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:32] Epoch 7\\Batch 250\\ Train Loss:10.323\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:35] Epoch 7\\Batch 300\\ Train Loss:10.326\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:39] Epoch 7\\Batch 350\\ Train Loss:10.341\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:44] Epoch 7\\Batch 400\\ Train Loss:10.346\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:49] Epoch 7\\Batch 450\\ Train Loss:10.328\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:54] Epoch 7\\Batch 500\\ Train Loss:10.316\\ Learning rate:0.00030\n",
      "[2019/03/18 01:56:59] Epoch 7\\Batch 550\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:04] Epoch 7\\Batch 600\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:08] Epoch 7\\Batch 650\\ Train Loss:10.296\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:14] Epoch 7\\Batch 700\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:19] Epoch 7\\Batch 750\\ Train Loss:10.302\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:24] Epoch 7\\Batch 800\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:29] Epoch 7\\Batch 850\\ Train Loss:10.300\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:34] Epoch 7\\Batch 900\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:39] Epoch 7\\Batch 950\\ Train Loss:10.299\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:44] Epoch 7\\Batch 1000\\ Train Loss:10.289\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:49] Epoch 7\\Batch 1050\\ Train Loss:10.288\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:53] Epoch 7\\Batch 1100\\ Train Loss:10.283\\ Learning rate:0.00030\n",
      "[2019/03/18 01:57:58] Epoch 7\\Batch 1150\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:03] Epoch 7\\Batch 1200\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:08] Epoch 7\\Batch 1250\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:13] Epoch 7\\Batch 1300\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:18] Epoch 7\\Batch 1350\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:23] Epoch 7\\Batch 1400\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:28] Epoch 7\\Batch 1450\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:33] Epoch 7\\Batch 1500\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:38] Epoch 7\\Batch 1550\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:43] Epoch 7\\Batch 1600\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:48] Epoch 7\\Batch 1650\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:53] Epoch 7\\Batch 1700\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 01:58:58] Epoch 7\\Batch 1750\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:03] Epoch 7\\Batch 1800\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:08] Epoch 7\\Batch 1850\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:13] Epoch 7\\Batch 1900\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:18] Epoch 7\\Batch 1950\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:23] Epoch 7\\Batch 2000\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:28] Epoch 7\\Batch 2050\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:33] Epoch 7\\Batch 2100\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:38] Epoch 7\\Batch 2150\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:42] Epoch 7\\Batch 2200\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:47] Epoch 7\\Batch 2250\\ Train Loss:10.284\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 01:59:52] Epoch 7\\Batch 2300\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 01:59:57] Epoch 7\\Batch 2350\\ Train Loss:10.284\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:02] Epoch 7\\Batch 2400\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:08] Epoch 7\\Batch 2450\\ Train Loss:10.284\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:13] Epoch 7\\Batch 2500\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:17] Epoch 7\\Batch 2550\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:22] Epoch 7\\Batch 2600\\ Train Loss:10.284\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:27] Epoch 7\\Batch 2650\\ Train Loss:10.286\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:32] Epoch 7\\Batch 2700\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:37] Epoch 7\\Batch 2750\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:42] Epoch 7\\Batch 2800\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:47] Epoch 7\\Batch 2850\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:52] Epoch 7\\Batch 2900\\ Train Loss:10.283\\ Learning rate:0.00030\n",
      "[2019/03/18 02:00:57] Epoch 7\\Batch 2950\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:02] Epoch 7\\Batch 3000\\ Train Loss:10.282\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5474.000001, 'TP': 1525.0000009999999, 'FP': 1663.0000009999999}\n",
      "[2019/03/18 02:01:24] Epoch 7/ Validation Loss:9.838/ F1_score:0.299/ Precision:0.478/ Recall:0.218\n",
      "[2019/03/18 02:01:29] Epoch 7\\Batch 3050\\ Train Loss:10.283\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:34] Epoch 7\\Batch 3100\\ Train Loss:10.287\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:39] Epoch 7\\Batch 3150\\ Train Loss:10.285\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:44] Epoch 7\\Batch 3200\\ Train Loss:10.283\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:49] Epoch 7\\Batch 3250\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:54] Epoch 7\\Batch 3300\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:01:59] Epoch 7\\Batch 3350\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:04] Epoch 7\\Batch 3400\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:09] Epoch 7\\Batch 3450\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:14] Epoch 7\\Batch 3500\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:19] Epoch 7\\Batch 3550\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:24] Epoch 7\\Batch 3600\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:29] Epoch 7\\Batch 3650\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:34] Epoch 7\\Batch 3700\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:39] Epoch 7\\Batch 3750\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:44] Epoch 7\\Batch 3800\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:49] Epoch 7\\Batch 3850\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:54] Epoch 7\\Batch 3900\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:02:59] Epoch 7\\Batch 3950\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:04] Epoch 7\\Batch 4000\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:09] Epoch 7\\Batch 4050\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:14] Epoch 7\\Batch 4100\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:19] Epoch 7\\Batch 4150\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:23] Epoch 7\\Batch 4200\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:26] Epoch 7\\Batch 4250\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:30] Epoch 7\\Batch 4300\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:33] Epoch 7\\Batch 4350\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:36] Epoch 7\\Batch 4400\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:40] Epoch 7\\Batch 4450\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:43] Epoch 7\\Batch 4500\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:46] Epoch 7\\Batch 4550\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:51] Epoch 7\\Batch 4600\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:03:55] Epoch 7\\Batch 4650\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:00] Epoch 7\\Batch 4700\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:05] Epoch 7\\Batch 4750\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:10] Epoch 7\\Batch 4800\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:15] Epoch 7\\Batch 4850\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:20] Epoch 7\\Batch 4900\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:25] Epoch 7\\Batch 4950\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:30] Epoch 7\\Batch 5000\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:35] Epoch 7\\Batch 5050\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:40] Epoch 7\\Batch 5100\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:45] Epoch 7\\Batch 5150\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:50] Epoch 7\\Batch 5200\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:04:55] Epoch 7\\Batch 5250\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:00] Epoch 7\\Batch 5300\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:05] Epoch 7\\Batch 5350\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:09] Epoch 7\\Batch 5400\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:14] Epoch 7\\Batch 5450\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:19] Epoch 7\\Batch 5500\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:24] Epoch 7\\Batch 5550\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:29] Epoch 7\\Batch 5600\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:34] Epoch 7\\Batch 5650\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:39] Epoch 7\\Batch 5700\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:44] Epoch 7\\Batch 5750\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:49] Epoch 7\\Batch 5800\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:54] Epoch 7\\Batch 5850\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:05:59] Epoch 7\\Batch 5900\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:06:04] Epoch 7\\Batch 5950\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:06:09] Epoch 7\\Batch 6000\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5466.000001, 'TP': 1533.0000009999999, 'FP': 1652.0000009999999}\n",
      "[2019/03/18 02:06:32] Epoch 7/ Validation Loss:9.815/ F1_score:0.301/ Precision:0.481/ Recall:0.219\n",
      "[2019/03/18 02:06:37] Epoch 7\\Batch 6050\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:06:42] Epoch 7\\Batch 6100\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:06:47] Epoch 7\\Batch 6150\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:06:52] Epoch 7\\Batch 6200\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:06:57] Epoch 7\\Batch 6250\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:01] Epoch 7\\Batch 6300\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:06] Epoch 7\\Batch 6350\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:11] Epoch 7\\Batch 6400\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:16] Epoch 7\\Batch 6450\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:21] Epoch 7\\Batch 6500\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:26] Epoch 7\\Batch 6550\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:31] Epoch 7\\Batch 6600\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:36] Epoch 7\\Batch 6650\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:41] Epoch 7\\Batch 6700\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:46] Epoch 7\\Batch 6750\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:51] Epoch 7\\Batch 6800\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:07:56] Epoch 7\\Batch 6850\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:01] Epoch 7\\Batch 6900\\ Train Loss:10.278\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 02:08:06] Epoch 7\\Batch 6950\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:11] Epoch 7\\Batch 7000\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:16] Epoch 7\\Batch 7050\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:21] Epoch 7\\Batch 7100\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:26] Epoch 7\\Batch 7150\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:31] Epoch 7\\Batch 7200\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:36] Epoch 7\\Batch 7250\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:41] Epoch 7\\Batch 7300\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:45] Epoch 7\\Batch 7350\\ Train Loss:10.278\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:50] Epoch 7\\Batch 7400\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:08:55] Epoch 7\\Batch 7450\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:00] Epoch 7\\Batch 7500\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:05] Epoch 7\\Batch 7550\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:10] Epoch 7\\Batch 7600\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:15] Epoch 7\\Batch 7650\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:20] Epoch 7\\Batch 7700\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:25] Epoch 7\\Batch 7750\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:30] Epoch 7\\Batch 7800\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:35] Epoch 7\\Batch 7850\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:40] Epoch 7\\Batch 7900\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:45] Epoch 7\\Batch 7950\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:50] Epoch 7\\Batch 8000\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:09:55] Epoch 7\\Batch 8050\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:00] Epoch 7\\Batch 8100\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:04] Epoch 7\\Batch 8150\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:09] Epoch 7\\Batch 8200\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:14] Epoch 7\\Batch 8250\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:19] Epoch 7\\Batch 8300\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:24] Epoch 7\\Batch 8350\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:29] Epoch 7\\Batch 8400\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:33] Epoch 7\\Batch 8450\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:36] Epoch 7\\Batch 8500\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:39] Epoch 7\\Batch 8550\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:42] Epoch 7\\Batch 8600\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:46] Epoch 7\\Batch 8650\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:49] Epoch 7\\Batch 8700\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:52] Epoch 7\\Batch 8750\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:10:56] Epoch 7\\Batch 8800\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:01] Epoch 7\\Batch 8850\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:06] Epoch 7\\Batch 8900\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:11] Epoch 7\\Batch 8950\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:16] Epoch 7\\Batch 9000\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5468.000001, 'TP': 1531.0000009999999, 'FP': 1676.0000009999999}\n",
      "[2019/03/18 02:11:39] Epoch 7/ Validation Loss:9.824/ F1_score:0.300/ Precision:0.477/ Recall:0.219\n",
      "[2019/03/18 02:11:44] Epoch 7\\Batch 9050\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:49] Epoch 7\\Batch 9100\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:54] Epoch 7\\Batch 9150\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:11:59] Epoch 7\\Batch 9200\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:04] Epoch 7\\Batch 9250\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:09] Epoch 7\\Batch 9300\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:13] Epoch 7\\Batch 9350\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:18] Epoch 7\\Batch 9400\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:23] Epoch 7\\Batch 9450\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:28] Epoch 7\\Batch 9500\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:33] Epoch 7\\Batch 9550\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:39] Epoch 7\\Batch 9600\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:44] Epoch 7\\Batch 9650\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:49] Epoch 7\\Batch 9700\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:54] Epoch 7\\Batch 9750\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:12:58] Epoch 7\\Batch 9800\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:03] Epoch 7\\Batch 9850\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:08] Epoch 7\\Batch 9900\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:13] Epoch 7\\Batch 9950\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:18] Epoch 7\\Batch 10000\\ Train Loss:10.277\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:23] Epoch 7\\Batch 10050\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:28] Epoch 7\\Batch 10100\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:33] Epoch 7\\Batch 10150\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:38] Epoch 7\\Batch 10200\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:43] Epoch 7\\Batch 10250\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:48] Epoch 7\\Batch 10300\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:53] Epoch 7\\Batch 10350\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:13:58] Epoch 7\\Batch 10400\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:03] Epoch 7\\Batch 10450\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:09] Epoch 7\\Batch 10500\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:14] Epoch 7\\Batch 10550\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:19] Epoch 7\\Batch 10600\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:24] Epoch 7\\Batch 10650\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:29] Epoch 7\\Batch 10700\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:34] Epoch 7\\Batch 10750\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:39] Epoch 7\\Batch 10800\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:44] Epoch 7\\Batch 10850\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:50] Epoch 7\\Batch 10900\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:14:54] Epoch 7\\Batch 10950\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:00] Epoch 7\\Batch 11000\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:05] Epoch 7\\Batch 11050\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:10] Epoch 7\\Batch 11100\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:15] Epoch 7\\Batch 11150\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:20] Epoch 7\\Batch 11200\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:25] Epoch 7\\Batch 11250\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:30] Epoch 7\\Batch 11300\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:35] Epoch 7\\Batch 11350\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:40] Epoch 7\\Batch 11400\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:45] Epoch 7\\Batch 11450\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:50] Epoch 7\\Batch 11500\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:15:55] Epoch 7\\Batch 11550\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:00] Epoch 7\\Batch 11600\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:05] Epoch 7\\Batch 11650\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:10] Epoch 7\\Batch 11700\\ Train Loss:10.274\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 02:16:15] Epoch 7\\Batch 11750\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:20] Epoch 7\\Batch 11800\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:25] Epoch 7\\Batch 11850\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:30] Epoch 7\\Batch 11900\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:35] Epoch 7\\Batch 11950\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:16:40] Epoch 7\\Batch 12000\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5470.000001, 'TP': 1529.0000009999999, 'FP': 1682.0000009999999}\n",
      "[2019/03/18 02:17:03] Epoch 7/ Validation Loss:9.823/ F1_score:0.300/ Precision:0.476/ Recall:0.218\n",
      "[2019/03/18 02:17:08] Epoch 7\\Batch 12050\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:12] Epoch 7\\Batch 12100\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:17] Epoch 7\\Batch 12150\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:22] Epoch 7\\Batch 12200\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:27] Epoch 7\\Batch 12250\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:30] Epoch 7\\Batch 12300\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:34] Epoch 7\\Batch 12350\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:37] Epoch 7\\Batch 12400\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:40] Epoch 7\\Batch 12450\\ Train Loss:10.273\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:43] Epoch 7\\Batch 12500\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:47] Epoch 7\\Batch 12550\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:50] Epoch 7\\Batch 12600\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:17:55] Epoch 7\\Batch 12650\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:00] Epoch 7\\Batch 12700\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:05] Epoch 7\\Batch 12750\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:10] Epoch 7\\Batch 12800\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:14] Epoch 7\\Batch 12850\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:19] Epoch 7\\Batch 12900\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:24] Epoch 7\\Batch 12950\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:29] Epoch 7\\Batch 13000\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:34] Epoch 7\\Batch 13050\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:39] Epoch 7\\Batch 13100\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:44] Epoch 7\\Batch 13150\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:49] Epoch 7\\Batch 13200\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:54] Epoch 7\\Batch 13250\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:18:59] Epoch 7\\Batch 13300\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:04] Epoch 7\\Batch 13350\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:09] Epoch 7\\Batch 13400\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:14] Epoch 7\\Batch 13450\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:19] Epoch 7\\Batch 13500\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:24] Epoch 7\\Batch 13550\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:28] Epoch 7\\Batch 13600\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:33] Epoch 7\\Batch 13650\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:38] Epoch 7\\Batch 13700\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:43] Epoch 7\\Batch 13750\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:48] Epoch 7\\Batch 13800\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:53] Epoch 7\\Batch 13850\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:19:58] Epoch 7\\Batch 13900\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:03] Epoch 7\\Batch 13950\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:07] Epoch 7\\Batch 14000\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:12] Epoch 7\\Batch 14050\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:17] Epoch 7\\Batch 14100\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:22] Epoch 7\\Batch 14150\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:27] Epoch 7\\Batch 14200\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:32] Epoch 7\\Batch 14250\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:37] Epoch 7\\Batch 14300\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:42] Epoch 7\\Batch 14350\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:47] Epoch 7\\Batch 14400\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:51] Epoch 7\\Batch 14450\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:20:56] Epoch 7\\Batch 14500\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:01] Epoch 7\\Batch 14550\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:06] Epoch 7\\Batch 14600\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:11] Epoch 7\\Batch 14650\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:16] Epoch 7\\Batch 14700\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:21] Epoch 7\\Batch 14750\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:26] Epoch 7\\Batch 14800\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:31] Epoch 7\\Batch 14850\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:36] Epoch 7\\Batch 14900\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:41] Epoch 7\\Batch 14950\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:21:46] Epoch 7\\Batch 15000\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5477.000001, 'TP': 1522.0000009999999, 'FP': 1661.0000009999999}\n",
      "[2019/03/18 02:22:08] Epoch 7/ Validation Loss:9.825/ F1_score:0.299/ Precision:0.478/ Recall:0.217\n",
      "[2019/03/18 02:22:13] Epoch 7\\Batch 15050\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:18] Epoch 7\\Batch 15100\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:23] Epoch 7\\Batch 15150\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:28] Epoch 7\\Batch 15200\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:33] Epoch 7\\Batch 15250\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:38] Epoch 7\\Batch 15300\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:43] Epoch 7\\Batch 15350\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:48] Epoch 7\\Batch 15400\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:53] Epoch 7\\Batch 15450\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:22:58] Epoch 7\\Batch 15500\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:03] Epoch 7\\Batch 15550\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:08] Epoch 7\\Batch 15600\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:13] Epoch 7\\Batch 15650\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:18] Epoch 7\\Batch 15700\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:23] Epoch 7\\Batch 15750\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:28] Epoch 7\\Batch 15800\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:33] Epoch 7\\Batch 15850\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:38] Epoch 7\\Batch 15900\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:43] Epoch 7\\Batch 15950\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:48] Epoch 7\\Batch 16000\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:52] Epoch 7\\Batch 16050\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:23:58] Epoch 7\\Batch 16100\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:02] Epoch 7\\Batch 16150\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:07] Epoch 7\\Batch 16200\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:12] Epoch 7\\Batch 16250\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:17] Epoch 7\\Batch 16300\\ Train Loss:10.270\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 02:24:22] Epoch 7\\Batch 16350\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:27] Epoch 7\\Batch 16400\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:32] Epoch 7\\Batch 16450\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:35] Epoch 7\\Batch 16500\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:39] Epoch 7\\Batch 16550\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:42] Epoch 7\\Batch 16600\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:45] Epoch 7\\Batch 16650\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:48] Epoch 7\\Batch 16700\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:52] Epoch 7\\Batch 16750\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:55] Epoch 7\\Batch 16800\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:24:59] Epoch 7\\Batch 16850\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:04] Epoch 7\\Batch 16900\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:09] Epoch 7\\Batch 16950\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:14] Epoch 7\\Batch 17000\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:18] Epoch 7\\Batch 17050\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:23] Epoch 7\\Batch 17100\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:28] Epoch 7\\Batch 17150\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:33] Epoch 7\\Batch 17200\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:38] Epoch 7\\Batch 17250\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:43] Epoch 7\\Batch 17300\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:48] Epoch 7\\Batch 17350\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:53] Epoch 7\\Batch 17400\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:25:58] Epoch 7\\Batch 17450\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:03] Epoch 7\\Batch 17500\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:08] Epoch 7\\Batch 17550\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:13] Epoch 7\\Batch 17600\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:18] Epoch 7\\Batch 17650\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:22] Epoch 7\\Batch 17700\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:27] Epoch 7\\Batch 17750\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:32] Epoch 7\\Batch 17800\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:37] Epoch 7\\Batch 17850\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:42] Epoch 7\\Batch 17900\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:47] Epoch 7\\Batch 17950\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:26:53] Epoch 7\\Batch 18000\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1653.0000009999999}\n",
      "[2019/03/18 02:27:15] Epoch 7/ Validation Loss:9.814/ F1_score:0.303/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 02:27:20] Epoch 7\\Batch 18050\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:26] Epoch 7\\Batch 18100\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:31] Epoch 7\\Batch 18150\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:35] Epoch 7\\Batch 18200\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:40] Epoch 7\\Batch 18250\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:45] Epoch 7\\Batch 18300\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:50] Epoch 7\\Batch 18350\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:27:55] Epoch 7\\Batch 18400\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:00] Epoch 7\\Batch 18450\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:05] Epoch 7\\Batch 18500\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:10] Epoch 7\\Batch 18550\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:15] Epoch 7\\Batch 18600\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:20] Epoch 7\\Batch 18650\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:25] Epoch 7\\Batch 18700\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:30] Epoch 7\\Batch 18750\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:35] Epoch 7\\Batch 18800\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:40] Epoch 7\\Batch 18850\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:45] Epoch 7\\Batch 18900\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:50] Epoch 7\\Batch 18950\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:28:55] Epoch 7\\Batch 19000\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:00] Epoch 7\\Batch 19050\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:05] Epoch 7\\Batch 19100\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:10] Epoch 7\\Batch 19150\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:15] Epoch 7\\Batch 19200\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:19] Epoch 7\\Batch 19250\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:24] Epoch 7\\Batch 19300\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:29] Epoch 7\\Batch 19350\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:34] Epoch 7\\Batch 19400\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:39] Epoch 7\\Batch 19450\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:44] Epoch 7\\Batch 19500\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:49] Epoch 7\\Batch 19550\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:54] Epoch 7\\Batch 19600\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:29:59] Epoch 7\\Batch 19650\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:04] Epoch 7\\Batch 19700\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:09] Epoch 7\\Batch 19750\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:14] Epoch 7\\Batch 19800\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:18] Epoch 7\\Batch 19850\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:23] Epoch 7\\Batch 19900\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:28] Epoch 7\\Batch 19950\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:33] Epoch 7\\Batch 20000\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:38] Epoch 7\\Batch 20050\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:43] Epoch 7\\Batch 20100\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:48] Epoch 7\\Batch 20150\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:53] Epoch 7\\Batch 20200\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:30:58] Epoch 7\\Batch 20250\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:03] Epoch 7\\Batch 20300\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:08] Epoch 7\\Batch 20350\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:13] Epoch 7\\Batch 20400\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:18] Epoch 7\\Batch 20450\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:23] Epoch 7\\Batch 20500\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:28] Epoch 7\\Batch 20550\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:33] Epoch 7\\Batch 20600\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:38] Epoch 7\\Batch 20650\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:43] Epoch 7\\Batch 20700\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:46] Epoch 7\\Batch 20750\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:50] Epoch 7\\Batch 20800\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:53] Epoch 7\\Batch 20850\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:56] Epoch 7\\Batch 20900\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:31:59] Epoch 7\\Batch 20950\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:03] Epoch 7\\Batch 21000\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5441.000001, 'TP': 1558.0000009999999, 'FP': 1644.0000009999999}\n",
      "[2019/03/18 02:32:24] Epoch 7/ Validation Loss:9.821/ F1_score:0.305/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 02:32:29] Epoch 7\\Batch 21050\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:34] Epoch 7\\Batch 21100\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:39] Epoch 7\\Batch 21150\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:44] Epoch 7\\Batch 21200\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:49] Epoch 7\\Batch 21250\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:54] Epoch 7\\Batch 21300\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:32:59] Epoch 7\\Batch 21350\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:04] Epoch 7\\Batch 21400\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:09] Epoch 7\\Batch 21450\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:14] Epoch 7\\Batch 21500\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:19] Epoch 7\\Batch 21550\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:24] Epoch 7\\Batch 21600\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:29] Epoch 7\\Batch 21650\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:34] Epoch 7\\Batch 21700\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:39] Epoch 7\\Batch 21750\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:44] Epoch 7\\Batch 21800\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:49] Epoch 7\\Batch 21850\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:54] Epoch 7\\Batch 21900\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:33:59] Epoch 7\\Batch 21950\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:04] Epoch 7\\Batch 22000\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:09] Epoch 7\\Batch 22050\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:14] Epoch 7\\Batch 22100\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:19] Epoch 7\\Batch 22150\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:24] Epoch 7\\Batch 22200\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:29] Epoch 7\\Batch 22250\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:34] Epoch 7\\Batch 22300\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:38] Epoch 7\\Batch 22350\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:43] Epoch 7\\Batch 22400\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:48] Epoch 7\\Batch 22450\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:53] Epoch 7\\Batch 22500\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:34:58] Epoch 7\\Batch 22550\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:03] Epoch 7\\Batch 22600\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:08] Epoch 7\\Batch 22650\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:13] Epoch 7\\Batch 22700\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:18] Epoch 7\\Batch 22750\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:23] Epoch 7\\Batch 22800\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:28] Epoch 7\\Batch 22850\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:33] Epoch 7\\Batch 22900\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:38] Epoch 7\\Batch 22950\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:43] Epoch 7\\Batch 23000\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:47] Epoch 7\\Batch 23050\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:52] Epoch 7\\Batch 23100\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:35:57] Epoch 7\\Batch 23150\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:02] Epoch 7\\Batch 23200\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:07] Epoch 7\\Batch 23250\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:12] Epoch 7\\Batch 23300\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:17] Epoch 7\\Batch 23350\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:22] Epoch 7\\Batch 23400\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:27] Epoch 7\\Batch 23450\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:32] Epoch 7\\Batch 23500\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:37] Epoch 7\\Batch 23550\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:42] Epoch 7\\Batch 23600\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:47] Epoch 7\\Batch 23650\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:52] Epoch 7\\Batch 23700\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:36:57] Epoch 7\\Batch 23750\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:37:02] Epoch 7\\Batch 23800\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:37:07] Epoch 7\\Batch 23850\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:37:12] Epoch 7\\Batch 23900\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:37:17] Epoch 7\\Batch 23950\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:37:22] Epoch 7\\Batch 24000\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5444.000001, 'TP': 1555.0000009999999, 'FP': 1658.0000009999999}\n",
      "[2019/03/18 02:37:45] Epoch 7/ Validation Loss:9.778/ F1_score:0.305/ Precision:0.484/ Recall:0.222\n",
      "[2019/03/18 02:37:50] Epoch 7\\Batch 24050\\ Train Loss:10.267\\ Learning rate:0.00030\n",
      "[2019/03/18 02:37:55] Epoch 7\\Batch 24100\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:00] Epoch 7\\Batch 24150\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:05] Epoch 7\\Batch 24200\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:10] Epoch 7\\Batch 24250\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:14] Epoch 7\\Batch 24300\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:19] Epoch 7\\Batch 24350\\ Train Loss:10.265\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:24] Epoch 7\\Batch 24400\\ Train Loss:10.265\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:29] Epoch 7\\Batch 24450\\ Train Loss:10.265\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:34] Epoch 7\\Batch 24500\\ Train Loss:10.265\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:39] Epoch 7\\Batch 24550\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:44] Epoch 7\\Batch 24600\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:47] Epoch 7\\Batch 24650\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:50] Epoch 7\\Batch 24700\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:54] Epoch 7\\Batch 24750\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 02:38:57] Epoch 7\\Batch 24800\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:00] Epoch 7\\Batch 24850\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:03] Epoch 7\\Batch 24900\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:07] Epoch 7\\Batch 24950\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:11] Epoch 7\\Batch 25000\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:16] Epoch 7\\Batch 25050\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:21] Epoch 7\\Batch 25100\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:26] Epoch 7\\Batch 25150\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:31] Epoch 7\\Batch 25200\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:36] Epoch 7\\Batch 25250\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:41] Epoch 7\\Batch 25300\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:46] Epoch 7\\Batch 25350\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:51] Epoch 7\\Batch 25400\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 02:39:56] Epoch 7\\Batch 25450\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:00] Epoch 7\\Batch 25500\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:06] Epoch 7\\Batch 25550\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:11] Epoch 7\\Batch 25600\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:16] Epoch 7\\Batch 25650\\ Train Loss:10.262\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 02:40:20] Epoch 7\\Batch 25700\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:26] Epoch 7\\Batch 25750\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:31] Epoch 7\\Batch 25800\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:36] Epoch 7\\Batch 25850\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:41] Epoch 7\\Batch 25900\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:46] Epoch 7\\Batch 25950\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:51] Epoch 7\\Batch 26000\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:40:56] Epoch 7\\Batch 26050\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:00] Epoch 7\\Batch 26100\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:05] Epoch 7\\Batch 26150\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:10] Epoch 7\\Batch 26200\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:15] Epoch 7\\Batch 26250\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:20] Epoch 7\\Batch 26300\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:25] Epoch 7\\Batch 26350\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:30] Epoch 7\\Batch 26400\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:35] Epoch 7\\Batch 26450\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:40] Epoch 7\\Batch 26500\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:45] Epoch 7\\Batch 26550\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:50] Epoch 7\\Batch 26600\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:41:55] Epoch 7\\Batch 26650\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:00] Epoch 7\\Batch 26700\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:03] Epoch 7\\Batch 26750\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:06] Epoch 7\\Batch 26800\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:09] Epoch 7\\Batch 26850\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:13] Epoch 7\\Batch 26900\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:16] Epoch 7\\Batch 26950\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:19] Epoch 7\\Batch 27000\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5484.000001, 'TP': 1515.0000009999999, 'FP': 1674.0000009999999}\n",
      "[2019/03/18 02:42:36] Epoch 7/ Validation Loss:9.797/ F1_score:0.297/ Precision:0.475/ Recall:0.216\n",
      "[2019/03/18 02:42:39] Epoch 7\\Batch 27050\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:43] Epoch 7\\Batch 27100\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:48] Epoch 7\\Batch 27150\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:53] Epoch 7\\Batch 27200\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:42:58] Epoch 7\\Batch 27250\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:03] Epoch 7\\Batch 27300\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:08] Epoch 7\\Batch 27350\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:13] Epoch 7\\Batch 27400\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:18] Epoch 7\\Batch 27450\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:23] Epoch 7\\Batch 27500\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:28] Epoch 7\\Batch 27550\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:33] Epoch 7\\Batch 27600\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:38] Epoch 7\\Batch 27650\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:43] Epoch 7\\Batch 27700\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:48] Epoch 7\\Batch 27750\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:53] Epoch 7\\Batch 27800\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:43:58] Epoch 7\\Batch 27850\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:02] Epoch 7\\Batch 27900\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:07] Epoch 7\\Batch 27950\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:12] Epoch 7\\Batch 28000\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:17] Epoch 7\\Batch 28050\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:22] Epoch 7\\Batch 28100\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:27] Epoch 7\\Batch 28150\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:32] Epoch 7\\Batch 28200\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:37] Epoch 7\\Batch 28250\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:42] Epoch 7\\Batch 28300\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:47] Epoch 7\\Batch 28350\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:52] Epoch 7\\Batch 28400\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:44:57] Epoch 7\\Batch 28450\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:02] Epoch 7\\Batch 28500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:07] Epoch 7\\Batch 28550\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:12] Epoch 7\\Batch 28600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:16] Epoch 7\\Batch 28650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:21] Epoch 7\\Batch 28700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:26] Epoch 7\\Batch 28750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:31] Epoch 7\\Batch 28800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:36] Epoch 7\\Batch 28850\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:41] Epoch 7\\Batch 28900\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:46] Epoch 7\\Batch 28950\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:51] Epoch 7\\Batch 29000\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:45:56] Epoch 7\\Batch 29050\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:01] Epoch 7\\Batch 29100\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:06] Epoch 7\\Batch 29150\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:10] Epoch 7\\Batch 29200\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:15] Epoch 7\\Batch 29250\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:20] Epoch 7\\Batch 29300\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:25] Epoch 7\\Batch 29350\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:30] Epoch 7\\Batch 29400\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:35] Epoch 7\\Batch 29450\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:40] Epoch 7\\Batch 29500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:45] Epoch 7\\Batch 29550\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:50] Epoch 7\\Batch 29600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:46:55] Epoch 7\\Batch 29650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:00] Epoch 7\\Batch 29700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:05] Epoch 7\\Batch 29750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:10] Epoch 7\\Batch 29800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:15] Epoch 7\\Batch 29850\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:20] Epoch 7\\Batch 29900\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:25] Epoch 7\\Batch 29950\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:47:30] Epoch 7\\Batch 30000\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5479.000001, 'TP': 1520.0000009999999, 'FP': 1665.0000009999999}\n",
      "[2019/03/18 02:47:52] Epoch 7/ Validation Loss:9.784/ F1_score:0.299/ Precision:0.477/ Recall:0.217\n",
      "[2019/03/18 02:47:57] Epoch 7\\Batch 30050\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:02] Epoch 7\\Batch 30100\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:07] Epoch 7\\Batch 30150\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:12] Epoch 7\\Batch 30200\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:17] Epoch 7\\Batch 30250\\ Train Loss:10.260\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 02:48:22] Epoch 7\\Batch 30300\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:27] Epoch 7\\Batch 30350\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:32] Epoch 7\\Batch 30400\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:37] Epoch 7\\Batch 30450\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:42] Epoch 7\\Batch 30500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:47] Epoch 7\\Batch 30550\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:52] Epoch 7\\Batch 30600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:48:57] Epoch 7\\Batch 30650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:02] Epoch 7\\Batch 30700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:07] Epoch 7\\Batch 30750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:11] Epoch 7\\Batch 30800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:16] Epoch 7\\Batch 30850\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:21] Epoch 7\\Batch 30900\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:25] Epoch 7\\Batch 30950\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:29] Epoch 7\\Batch 31000\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:32] Epoch 7\\Batch 31050\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:35] Epoch 7\\Batch 31100\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:38] Epoch 7\\Batch 31150\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:42] Epoch 7\\Batch 31200\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:45] Epoch 7\\Batch 31250\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:48] Epoch 7\\Batch 31300\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:53] Epoch 7\\Batch 31350\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:49:58] Epoch 7\\Batch 31400\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:03] Epoch 7\\Batch 31450\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:08] Epoch 7\\Batch 31500\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:13] Epoch 7\\Batch 31550\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:18] Epoch 7\\Batch 31600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:23] Epoch 7\\Batch 31650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:28] Epoch 7\\Batch 31700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:33] Epoch 7\\Batch 31750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:38] Epoch 7\\Batch 31800\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:43] Epoch 7\\Batch 31850\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:48] Epoch 7\\Batch 31900\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:53] Epoch 7\\Batch 31950\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:50:57] Epoch 7\\Batch 32000\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:02] Epoch 7\\Batch 32050\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:07] Epoch 7\\Batch 32100\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:12] Epoch 7\\Batch 32150\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:17] Epoch 7\\Batch 32200\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:22] Epoch 7\\Batch 32250\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:27] Epoch 7\\Batch 32300\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:32] Epoch 7\\Batch 32350\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:37] Epoch 7\\Batch 32400\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:42] Epoch 7\\Batch 32450\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:47] Epoch 7\\Batch 32500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:52] Epoch 7\\Batch 32550\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:51:57] Epoch 7\\Batch 32600\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:02] Epoch 7\\Batch 32650\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:07] Epoch 7\\Batch 32700\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:12] Epoch 7\\Batch 32750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:17] Epoch 7\\Batch 32800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:22] Epoch 7\\Batch 32850\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:27] Epoch 7\\Batch 32900\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:32] Epoch 7\\Batch 32950\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:52:37] Epoch 7\\Batch 33000\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5455.000001, 'TP': 1544.0000009999999, 'FP': 1658.0000009999999}\n",
      "[2019/03/18 02:53:00] Epoch 7/ Validation Loss:9.782/ F1_score:0.303/ Precision:0.482/ Recall:0.221\n",
      "[2019/03/18 02:53:04] Epoch 7\\Batch 33050\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:10] Epoch 7\\Batch 33100\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:14] Epoch 7\\Batch 33150\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:19] Epoch 7\\Batch 33200\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:24] Epoch 7\\Batch 33250\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:29] Epoch 7\\Batch 33300\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:34] Epoch 7\\Batch 33350\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:39] Epoch 7\\Batch 33400\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:44] Epoch 7\\Batch 33450\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:49] Epoch 7\\Batch 33500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:54] Epoch 7\\Batch 33550\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:53:59] Epoch 7\\Batch 33600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:04] Epoch 7\\Batch 33650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:09] Epoch 7\\Batch 33700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:14] Epoch 7\\Batch 33750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:19] Epoch 7\\Batch 33800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:24] Epoch 7\\Batch 33850\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:29] Epoch 7\\Batch 33900\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:34] Epoch 7\\Batch 33950\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:39] Epoch 7\\Batch 34000\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:43] Epoch 7\\Batch 34050\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:48] Epoch 7\\Batch 34100\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:53] Epoch 7\\Batch 34150\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:54:58] Epoch 7\\Batch 34200\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:03] Epoch 7\\Batch 34250\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:08] Epoch 7\\Batch 34300\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:13] Epoch 7\\Batch 34350\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:18] Epoch 7\\Batch 34400\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:23] Epoch 7\\Batch 34450\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:28] Epoch 7\\Batch 34500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:33] Epoch 7\\Batch 34550\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:38] Epoch 7\\Batch 34600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:43] Epoch 7\\Batch 34650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:48] Epoch 7\\Batch 34700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:53] Epoch 7\\Batch 34750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:55:58] Epoch 7\\Batch 34800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:02] Epoch 7\\Batch 34850\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:07] Epoch 7\\Batch 34900\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:12] Epoch 7\\Batch 34950\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:17] Epoch 7\\Batch 35000\\ Train Loss:10.260\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 02:56:22] Epoch 7\\Batch 35050\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:27] Epoch 7\\Batch 35100\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:31] Epoch 7\\Batch 35150\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:35] Epoch 7\\Batch 35200\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:38] Epoch 7\\Batch 35250\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:41] Epoch 7\\Batch 35300\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:45] Epoch 7\\Batch 35350\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:48] Epoch 7\\Batch 35400\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:51] Epoch 7\\Batch 35450\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:54] Epoch 7\\Batch 35500\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:56:59] Epoch 7\\Batch 35550\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:04] Epoch 7\\Batch 35600\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:09] Epoch 7\\Batch 35650\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:14] Epoch 7\\Batch 35700\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:19] Epoch 7\\Batch 35750\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:24] Epoch 7\\Batch 35800\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:29] Epoch 7\\Batch 35850\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:34] Epoch 7\\Batch 35900\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:39] Epoch 7\\Batch 35950\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:57:44] Epoch 7\\Batch 36000\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5436.000001, 'TP': 1563.0000009999999, 'FP': 1621.0000009999999}\n",
      "[2019/03/18 02:58:06] Epoch 7/ Validation Loss:9.808/ F1_score:0.307/ Precision:0.491/ Recall:0.223\n",
      "[2019/03/18 02:58:11] Epoch 7\\Batch 36050\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:16] Epoch 7\\Batch 36100\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:21] Epoch 7\\Batch 36150\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:26] Epoch 7\\Batch 36200\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:31] Epoch 7\\Batch 36250\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:36] Epoch 7\\Batch 36300\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:41] Epoch 7\\Batch 36350\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:46] Epoch 7\\Batch 36400\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:51] Epoch 7\\Batch 36450\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:58:56] Epoch 7\\Batch 36500\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:01] Epoch 7\\Batch 36550\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:06] Epoch 7\\Batch 36600\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:10] Epoch 7\\Batch 36650\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:15] Epoch 7\\Batch 36700\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:20] Epoch 7\\Batch 36750\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:25] Epoch 7\\Batch 36800\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:30] Epoch 7\\Batch 36850\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:35] Epoch 7\\Batch 36900\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:40] Epoch 7\\Batch 36950\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:45] Epoch 7\\Batch 37000\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:50] Epoch 7\\Batch 37050\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 02:59:55] Epoch 7\\Batch 37100\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:00] Epoch 7\\Batch 37150\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:05] Epoch 7\\Batch 37200\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:10] Epoch 7\\Batch 37250\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:15] Epoch 7\\Batch 37300\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:20] Epoch 7\\Batch 37350\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:24] Epoch 7\\Batch 37400\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:29] Epoch 7\\Batch 37450\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:34] Epoch 7\\Batch 37500\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:39] Epoch 7\\Batch 37550\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:44] Epoch 7\\Batch 37600\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:49] Epoch 7\\Batch 37650\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:54] Epoch 7\\Batch 37700\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:00:59] Epoch 7\\Batch 37750\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:04] Epoch 7\\Batch 37800\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:09] Epoch 7\\Batch 37850\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:14] Epoch 7\\Batch 37900\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:18] Epoch 7\\Batch 37950\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:23] Epoch 7\\Batch 38000\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:28] Epoch 7\\Batch 38050\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:33] Epoch 7\\Batch 38100\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:38] Epoch 7\\Batch 38150\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:43] Epoch 7\\Batch 38200\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:48] Epoch 7\\Batch 38250\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:53] Epoch 7\\Batch 38300\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:01:58] Epoch 7\\Batch 38350\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:03] Epoch 7\\Batch 38400\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:07] Epoch 7\\Batch 38450\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:12] Epoch 7\\Batch 38500\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:17] Epoch 7\\Batch 38550\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:22] Epoch 7\\Batch 38600\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:27] Epoch 7\\Batch 38650\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:32] Epoch 7\\Batch 38700\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:37] Epoch 7\\Batch 38750\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:42] Epoch 7\\Batch 38800\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:47] Epoch 7\\Batch 38850\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:52] Epoch 7\\Batch 38900\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:02:56] Epoch 7\\Batch 38950\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:01] Epoch 7\\Batch 39000\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5434.000001, 'TP': 1565.0000009999999, 'FP': 1640.0000009999999}\n",
      "[2019/03/18 03:03:24] Epoch 7/ Validation Loss:9.809/ F1_score:0.307/ Precision:0.488/ Recall:0.224\n",
      "[2019/03/18 03:03:28] Epoch 7\\Batch 39050\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:31] Epoch 7\\Batch 39100\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:34] Epoch 7\\Batch 39150\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:37] Epoch 7\\Batch 39200\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:41] Epoch 7\\Batch 39250\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:44] Epoch 7\\Batch 39300\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:47] Epoch 7\\Batch 39350\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:51] Epoch 7\\Batch 39400\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:03:56] Epoch 7\\Batch 39450\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:01] Epoch 7\\Batch 39500\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:06] Epoch 7\\Batch 39550\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:11] Epoch 7\\Batch 39600\\ Train Loss:10.257\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:04:15] Epoch 7\\Batch 39650\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:20] Epoch 7\\Batch 39700\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:25] Epoch 7\\Batch 39750\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:30] Epoch 7\\Batch 39800\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:35] Epoch 7\\Batch 39850\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:40] Epoch 7\\Batch 39900\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:45] Epoch 7\\Batch 39950\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:50] Epoch 7\\Batch 40000\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:55] Epoch 7\\Batch 40050\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:04:59] Epoch 7\\Batch 40100\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:04] Epoch 7\\Batch 40150\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:09] Epoch 7\\Batch 40200\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:14] Epoch 7\\Batch 40250\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:19] Epoch 7\\Batch 40300\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:24] Epoch 7\\Batch 40350\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:29] Epoch 7\\Batch 40400\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:34] Epoch 7\\Batch 40450\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:39] Epoch 7\\Batch 40500\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:44] Epoch 7\\Batch 40550\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:48] Epoch 7\\Batch 40600\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:53] Epoch 7\\Batch 40650\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:05:58] Epoch 7\\Batch 40700\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:03] Epoch 7\\Batch 40750\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:08] Epoch 7\\Batch 40800\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:13] Epoch 7\\Batch 40850\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:18] Epoch 7\\Batch 40900\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:23] Epoch 7\\Batch 40950\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:28] Epoch 7\\Batch 41000\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:33] Epoch 7\\Batch 41050\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:38] Epoch 7\\Batch 41100\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:43] Epoch 7\\Batch 41150\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:48] Epoch 7\\Batch 41200\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:53] Epoch 7\\Batch 41250\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:06:58] Epoch 7\\Batch 41300\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:03] Epoch 7\\Batch 41350\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:08] Epoch 7\\Batch 41400\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:13] Epoch 7\\Batch 41450\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:18] Epoch 7\\Batch 41500\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:23] Epoch 7\\Batch 41550\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:28] Epoch 7\\Batch 41600\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:33] Epoch 7\\Batch 41650\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:38] Epoch 7\\Batch 41700\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:44] Epoch 7\\Batch 41750\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:48] Epoch 7\\Batch 41800\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:54] Epoch 7\\Batch 41850\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:07:59] Epoch 7\\Batch 41900\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:08:04] Epoch 7\\Batch 41950\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:08:09] Epoch 7\\Batch 42000\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5459.000001, 'TP': 1540.0000009999999, 'FP': 1658.0000009999999}\n",
      "[2019/03/18 03:08:31] Epoch 7/ Validation Loss:9.786/ F1_score:0.302/ Precision:0.482/ Recall:0.220\n",
      "[2019/03/18 03:08:37] Epoch 7\\Batch 42050\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:08:42] Epoch 7\\Batch 42100\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:08:47] Epoch 7\\Batch 42150\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:08:52] Epoch 7\\Batch 42200\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:08:57] Epoch 7\\Batch 42250\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:02] Epoch 7\\Batch 42300\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:07] Epoch 7\\Batch 42350\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:12] Epoch 7\\Batch 42400\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:17] Epoch 7\\Batch 42450\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:22] Epoch 7\\Batch 42500\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:27] Epoch 7\\Batch 42550\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:32] Epoch 7\\Batch 42600\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:37] Epoch 7\\Batch 42650\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:42] Epoch 7\\Batch 42700\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:47] Epoch 7\\Batch 42750\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:52] Epoch 7\\Batch 42800\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:09:57] Epoch 7\\Batch 42850\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:02] Epoch 7\\Batch 42900\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:07] Epoch 7\\Batch 42950\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:12] Epoch 7\\Batch 43000\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:17] Epoch 7\\Batch 43050\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:22] Epoch 7\\Batch 43100\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:27] Epoch 7\\Batch 43150\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:32] Epoch 7\\Batch 43200\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:36] Epoch 7\\Batch 43250\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:39] Epoch 7\\Batch 43300\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:42] Epoch 7\\Batch 43350\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:46] Epoch 7\\Batch 43400\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:49] Epoch 7\\Batch 43450\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:52] Epoch 7\\Batch 43500\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:55] Epoch 7\\Batch 43550\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:10:59] Epoch 7\\Batch 43600\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:04] Epoch 7\\Batch 43650\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:09] Epoch 7\\Batch 43700\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:14] Epoch 7\\Batch 43750\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:19] Epoch 7\\Batch 43800\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:24] Epoch 7\\Batch 43850\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:29] Epoch 7\\Batch 43900\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:34] Epoch 7\\Batch 43950\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:39] Epoch 7\\Batch 44000\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:44] Epoch 7\\Batch 44050\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:49] Epoch 7\\Batch 44100\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:54] Epoch 7\\Batch 44150\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:11:59] Epoch 7\\Batch 44200\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:04] Epoch 7\\Batch 44250\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:09] Epoch 7\\Batch 44300\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:14] Epoch 7\\Batch 44350\\ Train Loss:10.254\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:12:19] Epoch 7\\Batch 44400\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:24] Epoch 7\\Batch 44450\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:29] Epoch 7\\Batch 44500\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:34] Epoch 7\\Batch 44550\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:39] Epoch 7\\Batch 44600\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:44] Epoch 7\\Batch 44650\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:48] Epoch 7\\Batch 44700\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:53] Epoch 7\\Batch 44750\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:12:58] Epoch 7\\Batch 44800\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:13:03] Epoch 7\\Batch 44850\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:13:08] Epoch 7\\Batch 44900\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:13:13] Epoch 7\\Batch 44950\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:13:18] Epoch 7\\Batch 45000\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5462.000001, 'TP': 1537.0000009999999, 'FP': 1662.0000009999999}\n",
      "[2019/03/18 03:13:41] Epoch 7/ Validation Loss:9.797/ F1_score:0.301/ Precision:0.480/ Recall:0.220\n",
      "[2019/03/18 03:13:45] Epoch 7\\Batch 45050\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:13:50] Epoch 7\\Batch 45100\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:13:55] Epoch 7\\Batch 45150\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:00] Epoch 7\\Batch 45200\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:05] Epoch 7\\Batch 45250\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:10] Epoch 7\\Batch 45300\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:15] Epoch 7\\Batch 45350\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:20] Epoch 7\\Batch 45400\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:25] Epoch 7\\Batch 45450\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:30] Epoch 7\\Batch 45500\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:35] Epoch 7\\Batch 45550\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:40] Epoch 7\\Batch 45600\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:45] Epoch 7\\Batch 45650\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:50] Epoch 7\\Batch 45700\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:55] Epoch 7\\Batch 45750\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:14:59] Epoch 7\\Batch 45800\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:04] Epoch 7\\Batch 45850\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:09] Epoch 7\\Batch 45900\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:14] Epoch 7\\Batch 45950\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:19] Epoch 7\\Batch 46000\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:24] Epoch 7\\Batch 46050\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:29] Epoch 7\\Batch 46100\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:34] Epoch 7\\Batch 46150\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:39] Epoch 7\\Batch 46200\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:48] Epoch 8\\Batch 50\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:53] Epoch 8\\Batch 100\\ Train Loss:10.290\\ Learning rate:0.00030\n",
      "[2019/03/18 03:15:58] Epoch 8\\Batch 150\\ Train Loss:10.348\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:03] Epoch 8\\Batch 200\\ Train Loss:10.311\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:08] Epoch 8\\Batch 250\\ Train Loss:10.298\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:13] Epoch 8\\Batch 300\\ Train Loss:10.307\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:18] Epoch 8\\Batch 350\\ Train Loss:10.315\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:23] Epoch 8\\Batch 400\\ Train Loss:10.317\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:28] Epoch 8\\Batch 450\\ Train Loss:10.301\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:33] Epoch 8\\Batch 500\\ Train Loss:10.292\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:38] Epoch 8\\Batch 550\\ Train Loss:10.276\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:43] Epoch 8\\Batch 600\\ Train Loss:10.281\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:48] Epoch 8\\Batch 650\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:53] Epoch 8\\Batch 700\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 03:16:58] Epoch 8\\Batch 750\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:03] Epoch 8\\Batch 800\\ Train Loss:10.272\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:07] Epoch 8\\Batch 850\\ Train Loss:10.275\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:12] Epoch 8\\Batch 900\\ Train Loss:10.282\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:17] Epoch 8\\Batch 950\\ Train Loss:10.271\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:22] Epoch 8\\Batch 1000\\ Train Loss:10.263\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:27] Epoch 8\\Batch 1050\\ Train Loss:10.261\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:32] Epoch 8\\Batch 1100\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:37] Epoch 8\\Batch 1150\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:41] Epoch 8\\Batch 1200\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:44] Epoch 8\\Batch 1250\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:47] Epoch 8\\Batch 1300\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:50] Epoch 8\\Batch 1350\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:53] Epoch 8\\Batch 1400\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:17:57] Epoch 8\\Batch 1450\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:00] Epoch 8\\Batch 1500\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:03] Epoch 8\\Batch 1550\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:08] Epoch 8\\Batch 1600\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:13] Epoch 8\\Batch 1650\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:18] Epoch 8\\Batch 1700\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:23] Epoch 8\\Batch 1750\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:28] Epoch 8\\Batch 1800\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:33] Epoch 8\\Batch 1850\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:38] Epoch 8\\Batch 1900\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:43] Epoch 8\\Batch 1950\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:48] Epoch 8\\Batch 2000\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:53] Epoch 8\\Batch 2050\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:18:58] Epoch 8\\Batch 2100\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:02] Epoch 8\\Batch 2150\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:07] Epoch 8\\Batch 2200\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:12] Epoch 8\\Batch 2250\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:17] Epoch 8\\Batch 2300\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:22] Epoch 8\\Batch 2350\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:27] Epoch 8\\Batch 2400\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:32] Epoch 8\\Batch 2450\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:37] Epoch 8\\Batch 2500\\ Train Loss:10.257\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:42] Epoch 8\\Batch 2550\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:47] Epoch 8\\Batch 2600\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:52] Epoch 8\\Batch 2650\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:19:57] Epoch 8\\Batch 2700\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:20:02] Epoch 8\\Batch 2750\\ Train Loss:10.260\\ Learning rate:0.00030\n",
      "[2019/03/18 03:20:07] Epoch 8\\Batch 2800\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 03:20:12] Epoch 8\\Batch 2850\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 03:20:17] Epoch 8\\Batch 2900\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:20:22] Epoch 8\\Batch 2950\\ Train Loss:10.256\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:20:27] Epoch 8\\Batch 3000\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5473.000001, 'TP': 1526.0000009999999, 'FP': 1661.0000009999999}\n",
      "[2019/03/18 03:20:50] Epoch 8/ Validation Loss:9.808/ F1_score:0.300/ Precision:0.479/ Recall:0.218\n",
      "[2019/03/18 03:20:55] Epoch 8\\Batch 3050\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:00] Epoch 8\\Batch 3100\\ Train Loss:10.256\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:05] Epoch 8\\Batch 3150\\ Train Loss:10.255\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:09] Epoch 8\\Batch 3200\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:15] Epoch 8\\Batch 3250\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:19] Epoch 8\\Batch 3300\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:24] Epoch 8\\Batch 3350\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:29] Epoch 8\\Batch 3400\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:34] Epoch 8\\Batch 3450\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:39] Epoch 8\\Batch 3500\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:44] Epoch 8\\Batch 3550\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:49] Epoch 8\\Batch 3600\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:54] Epoch 8\\Batch 3650\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:21:59] Epoch 8\\Batch 3700\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:04] Epoch 8\\Batch 3750\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:09] Epoch 8\\Batch 3800\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:14] Epoch 8\\Batch 3850\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:19] Epoch 8\\Batch 3900\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:24] Epoch 8\\Batch 3950\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:28] Epoch 8\\Batch 4000\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:33] Epoch 8\\Batch 4050\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:38] Epoch 8\\Batch 4100\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:43] Epoch 8\\Batch 4150\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:48] Epoch 8\\Batch 4200\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:53] Epoch 8\\Batch 4250\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:22:58] Epoch 8\\Batch 4300\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:03] Epoch 8\\Batch 4350\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:07] Epoch 8\\Batch 4400\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:12] Epoch 8\\Batch 4450\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:17] Epoch 8\\Batch 4500\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:22] Epoch 8\\Batch 4550\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:27] Epoch 8\\Batch 4600\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:32] Epoch 8\\Batch 4650\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:37] Epoch 8\\Batch 4700\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:42] Epoch 8\\Batch 4750\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:47] Epoch 8\\Batch 4800\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:52] Epoch 8\\Batch 4850\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:23:57] Epoch 8\\Batch 4900\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:02] Epoch 8\\Batch 4950\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:07] Epoch 8\\Batch 5000\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:12] Epoch 8\\Batch 5050\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:16] Epoch 8\\Batch 5100\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:21] Epoch 8\\Batch 5150\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:26] Epoch 8\\Batch 5200\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:31] Epoch 8\\Batch 5250\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:36] Epoch 8\\Batch 5300\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:41] Epoch 8\\Batch 5350\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:46] Epoch 8\\Batch 5400\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:49] Epoch 8\\Batch 5450\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:53] Epoch 8\\Batch 5500\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:56] Epoch 8\\Batch 5550\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:24:59] Epoch 8\\Batch 5600\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:02] Epoch 8\\Batch 5650\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:05] Epoch 8\\Batch 5700\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:09] Epoch 8\\Batch 5750\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:12] Epoch 8\\Batch 5800\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:17] Epoch 8\\Batch 5850\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:22] Epoch 8\\Batch 5900\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:27] Epoch 8\\Batch 5950\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:25:32] Epoch 8\\Batch 6000\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5453.000001, 'TP': 1546.0000009999999, 'FP': 1660.0000009999999}\n",
      "[2019/03/18 03:25:55] Epoch 8/ Validation Loss:9.789/ F1_score:0.303/ Precision:0.482/ Recall:0.221\n",
      "[2019/03/18 03:26:00] Epoch 8\\Batch 6050\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:05] Epoch 8\\Batch 6100\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:10] Epoch 8\\Batch 6150\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:15] Epoch 8\\Batch 6200\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:21] Epoch 8\\Batch 6250\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:26] Epoch 8\\Batch 6300\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:31] Epoch 8\\Batch 6350\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:36] Epoch 8\\Batch 6400\\ Train Loss:10.253\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:41] Epoch 8\\Batch 6450\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:46] Epoch 8\\Batch 6500\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:51] Epoch 8\\Batch 6550\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:26:56] Epoch 8\\Batch 6600\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:01] Epoch 8\\Batch 6650\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:06] Epoch 8\\Batch 6700\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:11] Epoch 8\\Batch 6750\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:16] Epoch 8\\Batch 6800\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:21] Epoch 8\\Batch 6850\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:26] Epoch 8\\Batch 6900\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:31] Epoch 8\\Batch 6950\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:36] Epoch 8\\Batch 7000\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:41] Epoch 8\\Batch 7050\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:47] Epoch 8\\Batch 7100\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:52] Epoch 8\\Batch 7150\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:27:57] Epoch 8\\Batch 7200\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:02] Epoch 8\\Batch 7250\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:07] Epoch 8\\Batch 7300\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:12] Epoch 8\\Batch 7350\\ Train Loss:10.251\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:17] Epoch 8\\Batch 7400\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:22] Epoch 8\\Batch 7450\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:27] Epoch 8\\Batch 7500\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:32] Epoch 8\\Batch 7550\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:37] Epoch 8\\Batch 7600\\ Train Loss:10.249\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:28:42] Epoch 8\\Batch 7650\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:47] Epoch 8\\Batch 7700\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:53] Epoch 8\\Batch 7750\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:28:58] Epoch 8\\Batch 7800\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:03] Epoch 8\\Batch 7850\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:08] Epoch 8\\Batch 7900\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:13] Epoch 8\\Batch 7950\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:18] Epoch 8\\Batch 8000\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:24] Epoch 8\\Batch 8050\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:29] Epoch 8\\Batch 8100\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:34] Epoch 8\\Batch 8150\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:39] Epoch 8\\Batch 8200\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:44] Epoch 8\\Batch 8250\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:49] Epoch 8\\Batch 8300\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:54] Epoch 8\\Batch 8350\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:29:59] Epoch 8\\Batch 8400\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:04] Epoch 8\\Batch 8450\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:09] Epoch 8\\Batch 8500\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:14] Epoch 8\\Batch 8550\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:19] Epoch 8\\Batch 8600\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:24] Epoch 8\\Batch 8650\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:29] Epoch 8\\Batch 8700\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:34] Epoch 8\\Batch 8750\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:39] Epoch 8\\Batch 8800\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:44] Epoch 8\\Batch 8850\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:49] Epoch 8\\Batch 8900\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:54] Epoch 8\\Batch 8950\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:30:59] Epoch 8\\Batch 9000\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5471.000001, 'TP': 1528.0000009999999, 'FP': 1670.0000009999999}\n",
      "[2019/03/18 03:31:22] Epoch 8/ Validation Loss:9.787/ F1_score:0.300/ Precision:0.478/ Recall:0.218\n",
      "[2019/03/18 03:31:27] Epoch 8\\Batch 9050\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:32] Epoch 8\\Batch 9100\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:37] Epoch 8\\Batch 9150\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:42] Epoch 8\\Batch 9200\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:46] Epoch 8\\Batch 9250\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:50] Epoch 8\\Batch 9300\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:53] Epoch 8\\Batch 9350\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:31:56] Epoch 8\\Batch 9400\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:00] Epoch 8\\Batch 9450\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:03] Epoch 8\\Batch 9500\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:07] Epoch 8\\Batch 9550\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:11] Epoch 8\\Batch 9600\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:16] Epoch 8\\Batch 9650\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:21] Epoch 8\\Batch 9700\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:26] Epoch 8\\Batch 9750\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:31] Epoch 8\\Batch 9800\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:36] Epoch 8\\Batch 9850\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:41] Epoch 8\\Batch 9900\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:46] Epoch 8\\Batch 9950\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:51] Epoch 8\\Batch 10000\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:32:56] Epoch 8\\Batch 10050\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:01] Epoch 8\\Batch 10100\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:06] Epoch 8\\Batch 10150\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:11] Epoch 8\\Batch 10200\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:16] Epoch 8\\Batch 10250\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:21] Epoch 8\\Batch 10300\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:26] Epoch 8\\Batch 10350\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:31] Epoch 8\\Batch 10400\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:36] Epoch 8\\Batch 10450\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:41] Epoch 8\\Batch 10500\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:46] Epoch 8\\Batch 10550\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:51] Epoch 8\\Batch 10600\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:33:56] Epoch 8\\Batch 10650\\ Train Loss:10.249\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:01] Epoch 8\\Batch 10700\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:06] Epoch 8\\Batch 10750\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:11] Epoch 8\\Batch 10800\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:16] Epoch 8\\Batch 10850\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:21] Epoch 8\\Batch 10900\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:26] Epoch 8\\Batch 10950\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:31] Epoch 8\\Batch 11000\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:37] Epoch 8\\Batch 11050\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:42] Epoch 8\\Batch 11100\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:47] Epoch 8\\Batch 11150\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:52] Epoch 8\\Batch 11200\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:34:57] Epoch 8\\Batch 11250\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:02] Epoch 8\\Batch 11300\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:07] Epoch 8\\Batch 11350\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:12] Epoch 8\\Batch 11400\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:17] Epoch 8\\Batch 11450\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:22] Epoch 8\\Batch 11500\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:27] Epoch 8\\Batch 11550\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:32] Epoch 8\\Batch 11600\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:37] Epoch 8\\Batch 11650\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:42] Epoch 8\\Batch 11700\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:47] Epoch 8\\Batch 11750\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:52] Epoch 8\\Batch 11800\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:35:57] Epoch 8\\Batch 11850\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:02] Epoch 8\\Batch 11900\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:07] Epoch 8\\Batch 11950\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:12] Epoch 8\\Batch 12000\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5464.000001, 'TP': 1535.0000009999999, 'FP': 1672.0000009999999}\n",
      "[2019/03/18 03:36:34] Epoch 8/ Validation Loss:9.776/ F1_score:0.301/ Precision:0.479/ Recall:0.219\n",
      "[2019/03/18 03:36:40] Epoch 8\\Batch 12050\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:45] Epoch 8\\Batch 12100\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:50] Epoch 8\\Batch 12150\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:54] Epoch 8\\Batch 12200\\ Train Loss:10.247\\ Learning rate:0.00030\n",
      "[2019/03/18 03:36:59] Epoch 8\\Batch 12250\\ Train Loss:10.246\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:37:05] Epoch 8\\Batch 12300\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:10] Epoch 8\\Batch 12350\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:15] Epoch 8\\Batch 12400\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:20] Epoch 8\\Batch 12450\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:24] Epoch 8\\Batch 12500\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:30] Epoch 8\\Batch 12550\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:34] Epoch 8\\Batch 12600\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:40] Epoch 8\\Batch 12650\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:45] Epoch 8\\Batch 12700\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:50] Epoch 8\\Batch 12750\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:54] Epoch 8\\Batch 12800\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:37:59] Epoch 8\\Batch 12850\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:04] Epoch 8\\Batch 12900\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:09] Epoch 8\\Batch 12950\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:14] Epoch 8\\Batch 13000\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:19] Epoch 8\\Batch 13050\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:24] Epoch 8\\Batch 13100\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:29] Epoch 8\\Batch 13150\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:34] Epoch 8\\Batch 13200\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:39] Epoch 8\\Batch 13250\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:44] Epoch 8\\Batch 13300\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:49] Epoch 8\\Batch 13350\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:54] Epoch 8\\Batch 13400\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:38:57] Epoch 8\\Batch 13450\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:01] Epoch 8\\Batch 13500\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:04] Epoch 8\\Batch 13550\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:08] Epoch 8\\Batch 13600\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:11] Epoch 8\\Batch 13650\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:14] Epoch 8\\Batch 13700\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:18] Epoch 8\\Batch 13750\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:22] Epoch 8\\Batch 13800\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:27] Epoch 8\\Batch 13850\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:32] Epoch 8\\Batch 13900\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:37] Epoch 8\\Batch 13950\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:42] Epoch 8\\Batch 14000\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:47] Epoch 8\\Batch 14050\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:52] Epoch 8\\Batch 14100\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:39:57] Epoch 8\\Batch 14150\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:02] Epoch 8\\Batch 14200\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:07] Epoch 8\\Batch 14250\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:12] Epoch 8\\Batch 14300\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:17] Epoch 8\\Batch 14350\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:22] Epoch 8\\Batch 14400\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:27] Epoch 8\\Batch 14450\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:32] Epoch 8\\Batch 14500\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:37] Epoch 8\\Batch 14550\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:42] Epoch 8\\Batch 14600\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:47] Epoch 8\\Batch 14650\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:52] Epoch 8\\Batch 14700\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:40:57] Epoch 8\\Batch 14750\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:41:02] Epoch 8\\Batch 14800\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:41:07] Epoch 8\\Batch 14850\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:41:12] Epoch 8\\Batch 14900\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:41:17] Epoch 8\\Batch 14950\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:41:22] Epoch 8\\Batch 15000\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5467.000001, 'TP': 1532.0000009999999, 'FP': 1652.0000009999999}\n",
      "[2019/03/18 03:41:45] Epoch 8/ Validation Loss:9.791/ F1_score:0.301/ Precision:0.481/ Recall:0.219\n",
      "[2019/03/18 03:41:50] Epoch 8\\Batch 15050\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:41:55] Epoch 8\\Batch 15100\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:00] Epoch 8\\Batch 15150\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:05] Epoch 8\\Batch 15200\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:10] Epoch 8\\Batch 15250\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:15] Epoch 8\\Batch 15300\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:20] Epoch 8\\Batch 15350\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:25] Epoch 8\\Batch 15400\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:30] Epoch 8\\Batch 15450\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:34] Epoch 8\\Batch 15500\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:39] Epoch 8\\Batch 15550\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:44] Epoch 8\\Batch 15600\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:49] Epoch 8\\Batch 15650\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:54] Epoch 8\\Batch 15700\\ Train Loss:10.246\\ Learning rate:0.00030\n",
      "[2019/03/18 03:42:59] Epoch 8\\Batch 15750\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:04] Epoch 8\\Batch 15800\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:09] Epoch 8\\Batch 15850\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:14] Epoch 8\\Batch 15900\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:19] Epoch 8\\Batch 15950\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:24] Epoch 8\\Batch 16000\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:29] Epoch 8\\Batch 16050\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:34] Epoch 8\\Batch 16100\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:39] Epoch 8\\Batch 16150\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:44] Epoch 8\\Batch 16200\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:48] Epoch 8\\Batch 16250\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:53] Epoch 8\\Batch 16300\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:43:58] Epoch 8\\Batch 16350\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:03] Epoch 8\\Batch 16400\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:08] Epoch 8\\Batch 16450\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:13] Epoch 8\\Batch 16500\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:18] Epoch 8\\Batch 16550\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:23] Epoch 8\\Batch 16600\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:28] Epoch 8\\Batch 16650\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:33] Epoch 8\\Batch 16700\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:38] Epoch 8\\Batch 16750\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:43] Epoch 8\\Batch 16800\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:48] Epoch 8\\Batch 16850\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:52] Epoch 8\\Batch 16900\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:44:57] Epoch 8\\Batch 16950\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:02] Epoch 8\\Batch 17000\\ Train Loss:10.244\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:45:07] Epoch 8\\Batch 17050\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:12] Epoch 8\\Batch 17100\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:17] Epoch 8\\Batch 17150\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:22] Epoch 8\\Batch 17200\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:27] Epoch 8\\Batch 17250\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:32] Epoch 8\\Batch 17300\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:37] Epoch 8\\Batch 17350\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:42] Epoch 8\\Batch 17400\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:47] Epoch 8\\Batch 17450\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:51] Epoch 8\\Batch 17500\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:45:56] Epoch 8\\Batch 17550\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:01] Epoch 8\\Batch 17600\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:05] Epoch 8\\Batch 17650\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:08] Epoch 8\\Batch 17700\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:12] Epoch 8\\Batch 17750\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:15] Epoch 8\\Batch 17800\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:18] Epoch 8\\Batch 17850\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:21] Epoch 8\\Batch 17900\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:25] Epoch 8\\Batch 17950\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:46:28] Epoch 8\\Batch 18000\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5472.000001, 'TP': 1527.0000009999999, 'FP': 1648.0000009999999}\n",
      "[2019/03/18 03:46:51] Epoch 8/ Validation Loss:9.756/ F1_score:0.300/ Precision:0.481/ Recall:0.218\n",
      "[2019/03/18 03:46:56] Epoch 8\\Batch 18050\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:01] Epoch 8\\Batch 18100\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:06] Epoch 8\\Batch 18150\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:10] Epoch 8\\Batch 18200\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:15] Epoch 8\\Batch 18250\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:20] Epoch 8\\Batch 18300\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:25] Epoch 8\\Batch 18350\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:31] Epoch 8\\Batch 18400\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:36] Epoch 8\\Batch 18450\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:40] Epoch 8\\Batch 18500\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:45] Epoch 8\\Batch 18550\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:50] Epoch 8\\Batch 18600\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:47:55] Epoch 8\\Batch 18650\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:00] Epoch 8\\Batch 18700\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:05] Epoch 8\\Batch 18750\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:10] Epoch 8\\Batch 18800\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:15] Epoch 8\\Batch 18850\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:20] Epoch 8\\Batch 18900\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:25] Epoch 8\\Batch 18950\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:30] Epoch 8\\Batch 19000\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:35] Epoch 8\\Batch 19050\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:40] Epoch 8\\Batch 19100\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:44] Epoch 8\\Batch 19150\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:49] Epoch 8\\Batch 19200\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:54] Epoch 8\\Batch 19250\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:48:59] Epoch 8\\Batch 19300\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:04] Epoch 8\\Batch 19350\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:09] Epoch 8\\Batch 19400\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:14] Epoch 8\\Batch 19450\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:19] Epoch 8\\Batch 19500\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:24] Epoch 8\\Batch 19550\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:29] Epoch 8\\Batch 19600\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:34] Epoch 8\\Batch 19650\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:39] Epoch 8\\Batch 19700\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:44] Epoch 8\\Batch 19750\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:49] Epoch 8\\Batch 19800\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:54] Epoch 8\\Batch 19850\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:49:58] Epoch 8\\Batch 19900\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:03] Epoch 8\\Batch 19950\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:08] Epoch 8\\Batch 20000\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:13] Epoch 8\\Batch 20050\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:18] Epoch 8\\Batch 20100\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:23] Epoch 8\\Batch 20150\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:28] Epoch 8\\Batch 20200\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:33] Epoch 8\\Batch 20250\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:38] Epoch 8\\Batch 20300\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:43] Epoch 8\\Batch 20350\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:48] Epoch 8\\Batch 20400\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:53] Epoch 8\\Batch 20450\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:50:58] Epoch 8\\Batch 20500\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:03] Epoch 8\\Batch 20550\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:08] Epoch 8\\Batch 20600\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:12] Epoch 8\\Batch 20650\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:17] Epoch 8\\Batch 20700\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:22] Epoch 8\\Batch 20750\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:27] Epoch 8\\Batch 20800\\ Train Loss:10.244\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:32] Epoch 8\\Batch 20850\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:37] Epoch 8\\Batch 20900\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:42] Epoch 8\\Batch 20950\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 03:51:47] Epoch 8\\Batch 21000\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1649.0000009999999}\n",
      "[2019/03/18 03:52:10] Epoch 8/ Validation Loss:9.756/ F1_score:0.303/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 03:52:15] Epoch 8\\Batch 21050\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:19] Epoch 8\\Batch 21100\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:24] Epoch 8\\Batch 21150\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:29] Epoch 8\\Batch 21200\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:34] Epoch 8\\Batch 21250\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:39] Epoch 8\\Batch 21300\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:44] Epoch 8\\Batch 21350\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:48] Epoch 8\\Batch 21400\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:53] Epoch 8\\Batch 21450\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:52:58] Epoch 8\\Batch 21500\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:01] Epoch 8\\Batch 21550\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:05] Epoch 8\\Batch 21600\\ Train Loss:10.240\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 03:53:08] Epoch 8\\Batch 21650\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:11] Epoch 8\\Batch 21700\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:14] Epoch 8\\Batch 21750\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:18] Epoch 8\\Batch 21800\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:21] Epoch 8\\Batch 21850\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:25] Epoch 8\\Batch 21900\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:30] Epoch 8\\Batch 21950\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:34] Epoch 8\\Batch 22000\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:39] Epoch 8\\Batch 22050\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:44] Epoch 8\\Batch 22100\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:49] Epoch 8\\Batch 22150\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:54] Epoch 8\\Batch 22200\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:53:59] Epoch 8\\Batch 22250\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:04] Epoch 8\\Batch 22300\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:09] Epoch 8\\Batch 22350\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:14] Epoch 8\\Batch 22400\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:19] Epoch 8\\Batch 22450\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:24] Epoch 8\\Batch 22500\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:29] Epoch 8\\Batch 22550\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:34] Epoch 8\\Batch 22600\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:39] Epoch 8\\Batch 22650\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:43] Epoch 8\\Batch 22700\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:48] Epoch 8\\Batch 22750\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:53] Epoch 8\\Batch 22800\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:54:58] Epoch 8\\Batch 22850\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:03] Epoch 8\\Batch 22900\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:09] Epoch 8\\Batch 22950\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:14] Epoch 8\\Batch 23000\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:19] Epoch 8\\Batch 23050\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:24] Epoch 8\\Batch 23100\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:28] Epoch 8\\Batch 23150\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:33] Epoch 8\\Batch 23200\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:38] Epoch 8\\Batch 23250\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:43] Epoch 8\\Batch 23300\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:48] Epoch 8\\Batch 23350\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:53] Epoch 8\\Batch 23400\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:55:58] Epoch 8\\Batch 23450\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:03] Epoch 8\\Batch 23500\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:08] Epoch 8\\Batch 23550\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:13] Epoch 8\\Batch 23600\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:18] Epoch 8\\Batch 23650\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:23] Epoch 8\\Batch 23700\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:28] Epoch 8\\Batch 23750\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:33] Epoch 8\\Batch 23800\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:38] Epoch 8\\Batch 23850\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:43] Epoch 8\\Batch 23900\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:48] Epoch 8\\Batch 23950\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:56:53] Epoch 8\\Batch 24000\\ Train Loss:10.242\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5440.000001, 'TP': 1559.0000009999999, 'FP': 1642.0000009999999}\n",
      "[2019/03/18 03:57:16] Epoch 8/ Validation Loss:9.734/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 03:57:21] Epoch 8\\Batch 24050\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:26] Epoch 8\\Batch 24100\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:31] Epoch 8\\Batch 24150\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:36] Epoch 8\\Batch 24200\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:41] Epoch 8\\Batch 24250\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:46] Epoch 8\\Batch 24300\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:51] Epoch 8\\Batch 24350\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 03:57:56] Epoch 8\\Batch 24400\\ Train Loss:10.239\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:01] Epoch 8\\Batch 24450\\ Train Loss:10.239\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:06] Epoch 8\\Batch 24500\\ Train Loss:10.239\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:11] Epoch 8\\Batch 24550\\ Train Loss:10.239\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:16] Epoch 8\\Batch 24600\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:21] Epoch 8\\Batch 24650\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:26] Epoch 8\\Batch 24700\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:31] Epoch 8\\Batch 24750\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:36] Epoch 8\\Batch 24800\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:41] Epoch 8\\Batch 24850\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:46] Epoch 8\\Batch 24900\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:51] Epoch 8\\Batch 24950\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:58:56] Epoch 8\\Batch 25000\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:01] Epoch 8\\Batch 25050\\ Train Loss:10.238\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:06] Epoch 8\\Batch 25100\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:11] Epoch 8\\Batch 25150\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:16] Epoch 8\\Batch 25200\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:21] Epoch 8\\Batch 25250\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:26] Epoch 8\\Batch 25300\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:31] Epoch 8\\Batch 25350\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:36] Epoch 8\\Batch 25400\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:41] Epoch 8\\Batch 25450\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:46] Epoch 8\\Batch 25500\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:51] Epoch 8\\Batch 25550\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 03:59:56] Epoch 8\\Batch 25600\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:01] Epoch 8\\Batch 25650\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:05] Epoch 8\\Batch 25700\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:09] Epoch 8\\Batch 25750\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:12] Epoch 8\\Batch 25800\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:16] Epoch 8\\Batch 25850\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:19] Epoch 8\\Batch 25900\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:22] Epoch 8\\Batch 25950\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:26] Epoch 8\\Batch 26000\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:29] Epoch 8\\Batch 26050\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:34] Epoch 8\\Batch 26100\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:39] Epoch 8\\Batch 26150\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:44] Epoch 8\\Batch 26200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:49] Epoch 8\\Batch 26250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:54] Epoch 8\\Batch 26300\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:00:59] Epoch 8\\Batch 26350\\ Train Loss:10.235\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:01:04] Epoch 8\\Batch 26400\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:10] Epoch 8\\Batch 26450\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:15] Epoch 8\\Batch 26500\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:20] Epoch 8\\Batch 26550\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:25] Epoch 8\\Batch 26600\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:30] Epoch 8\\Batch 26650\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:35] Epoch 8\\Batch 26700\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:40] Epoch 8\\Batch 26750\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:46] Epoch 8\\Batch 26800\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:51] Epoch 8\\Batch 26850\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:01:56] Epoch 8\\Batch 26900\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:01] Epoch 8\\Batch 26950\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:06] Epoch 8\\Batch 27000\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5466.000001, 'TP': 1533.0000009999999, 'FP': 1656.0000009999999}\n",
      "[2019/03/18 04:02:29] Epoch 8/ Validation Loss:9.761/ F1_score:0.301/ Precision:0.481/ Recall:0.219\n",
      "[2019/03/18 04:02:34] Epoch 8\\Batch 27050\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:39] Epoch 8\\Batch 27100\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:44] Epoch 8\\Batch 27150\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:49] Epoch 8\\Batch 27200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:54] Epoch 8\\Batch 27250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:02:59] Epoch 8\\Batch 27300\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:04] Epoch 8\\Batch 27350\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:09] Epoch 8\\Batch 27400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:14] Epoch 8\\Batch 27450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:19] Epoch 8\\Batch 27500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:24] Epoch 8\\Batch 27550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:29] Epoch 8\\Batch 27600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:34] Epoch 8\\Batch 27650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:39] Epoch 8\\Batch 27700\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:44] Epoch 8\\Batch 27750\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:50] Epoch 8\\Batch 27800\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:03:55] Epoch 8\\Batch 27850\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:00] Epoch 8\\Batch 27900\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:05] Epoch 8\\Batch 27950\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:10] Epoch 8\\Batch 28000\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:15] Epoch 8\\Batch 28050\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:20] Epoch 8\\Batch 28100\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:25] Epoch 8\\Batch 28150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:30] Epoch 8\\Batch 28200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:35] Epoch 8\\Batch 28250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:40] Epoch 8\\Batch 28300\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:45] Epoch 8\\Batch 28350\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:50] Epoch 8\\Batch 28400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:04:55] Epoch 8\\Batch 28450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:00] Epoch 8\\Batch 28500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:05] Epoch 8\\Batch 28550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:10] Epoch 8\\Batch 28600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:15] Epoch 8\\Batch 28650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:20] Epoch 8\\Batch 28700\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:25] Epoch 8\\Batch 28750\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:30] Epoch 8\\Batch 28800\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:36] Epoch 8\\Batch 28850\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:41] Epoch 8\\Batch 28900\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:46] Epoch 8\\Batch 28950\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:51] Epoch 8\\Batch 29000\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:05:56] Epoch 8\\Batch 29050\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:01] Epoch 8\\Batch 29100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:06] Epoch 8\\Batch 29150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:11] Epoch 8\\Batch 29200\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:16] Epoch 8\\Batch 29250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:21] Epoch 8\\Batch 29300\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:26] Epoch 8\\Batch 29350\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:31] Epoch 8\\Batch 29400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:36] Epoch 8\\Batch 29450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:41] Epoch 8\\Batch 29500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:46] Epoch 8\\Batch 29550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:51] Epoch 8\\Batch 29600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:06:56] Epoch 8\\Batch 29650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:01] Epoch 8\\Batch 29700\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:06] Epoch 8\\Batch 29750\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:11] Epoch 8\\Batch 29800\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:16] Epoch 8\\Batch 29850\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:19] Epoch 8\\Batch 29900\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:23] Epoch 8\\Batch 29950\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:26] Epoch 8\\Batch 30000\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5454.000001, 'TP': 1545.0000009999999, 'FP': 1658.0000009999999}\n",
      "[2019/03/18 04:07:46] Epoch 8/ Validation Loss:9.752/ F1_score:0.303/ Precision:0.482/ Recall:0.221\n",
      "[2019/03/18 04:07:51] Epoch 8\\Batch 30050\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:07:56] Epoch 8\\Batch 30100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:01] Epoch 8\\Batch 30150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:06] Epoch 8\\Batch 30200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:11] Epoch 8\\Batch 30250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:16] Epoch 8\\Batch 30300\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:21] Epoch 8\\Batch 30350\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:26] Epoch 8\\Batch 30400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:31] Epoch 8\\Batch 30450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:36] Epoch 8\\Batch 30500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:42] Epoch 8\\Batch 30550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:47] Epoch 8\\Batch 30600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:52] Epoch 8\\Batch 30650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:08:57] Epoch 8\\Batch 30700\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:01] Epoch 8\\Batch 30750\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:06] Epoch 8\\Batch 30800\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:11] Epoch 8\\Batch 30850\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:17] Epoch 8\\Batch 30900\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:22] Epoch 8\\Batch 30950\\ Train Loss:10.235\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:09:27] Epoch 8\\Batch 31000\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:32] Epoch 8\\Batch 31050\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:37] Epoch 8\\Batch 31100\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:42] Epoch 8\\Batch 31150\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:47] Epoch 8\\Batch 31200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:52] Epoch 8\\Batch 31250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:09:57] Epoch 8\\Batch 31300\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:02] Epoch 8\\Batch 31350\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:07] Epoch 8\\Batch 31400\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:12] Epoch 8\\Batch 31450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:17] Epoch 8\\Batch 31500\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:22] Epoch 8\\Batch 31550\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:27] Epoch 8\\Batch 31600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:32] Epoch 8\\Batch 31650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:37] Epoch 8\\Batch 31700\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:42] Epoch 8\\Batch 31750\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:47] Epoch 8\\Batch 31800\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:52] Epoch 8\\Batch 31850\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:10:57] Epoch 8\\Batch 31900\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:02] Epoch 8\\Batch 31950\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:07] Epoch 8\\Batch 32000\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:12] Epoch 8\\Batch 32050\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:17] Epoch 8\\Batch 32100\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:21] Epoch 8\\Batch 32150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:26] Epoch 8\\Batch 32200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:31] Epoch 8\\Batch 32250\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:36] Epoch 8\\Batch 32300\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:41] Epoch 8\\Batch 32350\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:46] Epoch 8\\Batch 32400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:51] Epoch 8\\Batch 32450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:11:56] Epoch 8\\Batch 32500\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:01] Epoch 8\\Batch 32550\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:06] Epoch 8\\Batch 32600\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:11] Epoch 8\\Batch 32650\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:16] Epoch 8\\Batch 32700\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:21] Epoch 8\\Batch 32750\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:26] Epoch 8\\Batch 32800\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:31] Epoch 8\\Batch 32850\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:36] Epoch 8\\Batch 32900\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:41] Epoch 8\\Batch 32950\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:12:46] Epoch 8\\Batch 33000\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5434.000001, 'TP': 1565.0000009999999, 'FP': 1653.0000009999999}\n",
      "[2019/03/18 04:13:09] Epoch 8/ Validation Loss:9.758/ F1_score:0.306/ Precision:0.486/ Recall:0.224\n",
      "[2019/03/18 04:13:14] Epoch 8\\Batch 33050\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:19] Epoch 8\\Batch 33100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:24] Epoch 8\\Batch 33150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:29] Epoch 8\\Batch 33200\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:34] Epoch 8\\Batch 33250\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:39] Epoch 8\\Batch 33300\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:44] Epoch 8\\Batch 33350\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:49] Epoch 8\\Batch 33400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:54] Epoch 8\\Batch 33450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:13:59] Epoch 8\\Batch 33500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:04] Epoch 8\\Batch 33550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:09] Epoch 8\\Batch 33600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:13] Epoch 8\\Batch 33650\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:17] Epoch 8\\Batch 33700\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:20] Epoch 8\\Batch 33750\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:23] Epoch 8\\Batch 33800\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:26] Epoch 8\\Batch 33850\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:30] Epoch 8\\Batch 33900\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:33] Epoch 8\\Batch 33950\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:36] Epoch 8\\Batch 34000\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:41] Epoch 8\\Batch 34050\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:46] Epoch 8\\Batch 34100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:51] Epoch 8\\Batch 34150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:14:55] Epoch 8\\Batch 34200\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:00] Epoch 8\\Batch 34250\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:05] Epoch 8\\Batch 34300\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:10] Epoch 8\\Batch 34350\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:15] Epoch 8\\Batch 34400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:20] Epoch 8\\Batch 34450\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:25] Epoch 8\\Batch 34500\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:30] Epoch 8\\Batch 34550\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:35] Epoch 8\\Batch 34600\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:40] Epoch 8\\Batch 34650\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:45] Epoch 8\\Batch 34700\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:50] Epoch 8\\Batch 34750\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:15:55] Epoch 8\\Batch 34800\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:00] Epoch 8\\Batch 34850\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:05] Epoch 8\\Batch 34900\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:10] Epoch 8\\Batch 34950\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:15] Epoch 8\\Batch 35000\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:19] Epoch 8\\Batch 35050\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:24] Epoch 8\\Batch 35100\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:29] Epoch 8\\Batch 35150\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:34] Epoch 8\\Batch 35200\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:39] Epoch 8\\Batch 35250\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:44] Epoch 8\\Batch 35300\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:49] Epoch 8\\Batch 35350\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:54] Epoch 8\\Batch 35400\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:16:59] Epoch 8\\Batch 35450\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:04] Epoch 8\\Batch 35500\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:09] Epoch 8\\Batch 35550\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:14] Epoch 8\\Batch 35600\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:19] Epoch 8\\Batch 35650\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:24] Epoch 8\\Batch 35700\\ Train Loss:10.235\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:17:29] Epoch 8\\Batch 35750\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:34] Epoch 8\\Batch 35800\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:38] Epoch 8\\Batch 35850\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:43] Epoch 8\\Batch 35900\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:48] Epoch 8\\Batch 35950\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:17:53] Epoch 8\\Batch 36000\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5452.000001, 'TP': 1547.0000009999999, 'FP': 1653.0000009999999}\n",
      "[2019/03/18 04:18:16] Epoch 8/ Validation Loss:9.774/ F1_score:0.303/ Precision:0.483/ Recall:0.221\n",
      "[2019/03/18 04:18:21] Epoch 8\\Batch 36050\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:26] Epoch 8\\Batch 36100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:31] Epoch 8\\Batch 36150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:36] Epoch 8\\Batch 36200\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:41] Epoch 8\\Batch 36250\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:45] Epoch 8\\Batch 36300\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:51] Epoch 8\\Batch 36350\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:18:55] Epoch 8\\Batch 36400\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:00] Epoch 8\\Batch 36450\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:05] Epoch 8\\Batch 36500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:10] Epoch 8\\Batch 36550\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:15] Epoch 8\\Batch 36600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:20] Epoch 8\\Batch 36650\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:25] Epoch 8\\Batch 36700\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:30] Epoch 8\\Batch 36750\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:35] Epoch 8\\Batch 36800\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:40] Epoch 8\\Batch 36850\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:45] Epoch 8\\Batch 36900\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:50] Epoch 8\\Batch 36950\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:19:55] Epoch 8\\Batch 37000\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:00] Epoch 8\\Batch 37050\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:05] Epoch 8\\Batch 37100\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:10] Epoch 8\\Batch 37150\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:15] Epoch 8\\Batch 37200\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:19] Epoch 8\\Batch 37250\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:24] Epoch 8\\Batch 37300\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:29] Epoch 8\\Batch 37350\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:34] Epoch 8\\Batch 37400\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:39] Epoch 8\\Batch 37450\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:44] Epoch 8\\Batch 37500\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:49] Epoch 8\\Batch 37550\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:54] Epoch 8\\Batch 37600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:20:59] Epoch 8\\Batch 37650\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:04] Epoch 8\\Batch 37700\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:09] Epoch 8\\Batch 37750\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:14] Epoch 8\\Batch 37800\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:19] Epoch 8\\Batch 37850\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:22] Epoch 8\\Batch 37900\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:26] Epoch 8\\Batch 37950\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:29] Epoch 8\\Batch 38000\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:32] Epoch 8\\Batch 38050\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:35] Epoch 8\\Batch 38100\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:39] Epoch 8\\Batch 38150\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:42] Epoch 8\\Batch 38200\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:46] Epoch 8\\Batch 38250\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:51] Epoch 8\\Batch 38300\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:21:56] Epoch 8\\Batch 38350\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:01] Epoch 8\\Batch 38400\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:05] Epoch 8\\Batch 38450\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:10] Epoch 8\\Batch 38500\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:15] Epoch 8\\Batch 38550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:20] Epoch 8\\Batch 38600\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:25] Epoch 8\\Batch 38650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:30] Epoch 8\\Batch 38700\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:35] Epoch 8\\Batch 38750\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:40] Epoch 8\\Batch 38800\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:44] Epoch 8\\Batch 38850\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:49] Epoch 8\\Batch 38900\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:54] Epoch 8\\Batch 38950\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:22:59] Epoch 8\\Batch 39000\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5417.000001, 'TP': 1582.0000009999999, 'FP': 1630.0000009999999}\n",
      "[2019/03/18 04:23:22] Epoch 8/ Validation Loss:9.788/ F1_score:0.310/ Precision:0.493/ Recall:0.226\n",
      "[2019/03/18 04:23:27] Epoch 8\\Batch 39050\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:23:32] Epoch 8\\Batch 39100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:23:37] Epoch 8\\Batch 39150\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:23:42] Epoch 8\\Batch 39200\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:23:47] Epoch 8\\Batch 39250\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:23:52] Epoch 8\\Batch 39300\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:23:57] Epoch 8\\Batch 39350\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:01] Epoch 8\\Batch 39400\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:06] Epoch 8\\Batch 39450\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:11] Epoch 8\\Batch 39500\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:16] Epoch 8\\Batch 39550\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:21] Epoch 8\\Batch 39600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:26] Epoch 8\\Batch 39650\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:31] Epoch 8\\Batch 39700\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:36] Epoch 8\\Batch 39750\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:41] Epoch 8\\Batch 39800\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:46] Epoch 8\\Batch 39850\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:51] Epoch 8\\Batch 39900\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:24:55] Epoch 8\\Batch 39950\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:00] Epoch 8\\Batch 40000\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:05] Epoch 8\\Batch 40050\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:10] Epoch 8\\Batch 40100\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:15] Epoch 8\\Batch 40150\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:20] Epoch 8\\Batch 40200\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:25] Epoch 8\\Batch 40250\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:30] Epoch 8\\Batch 40300\\ Train Loss:10.233\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:25:35] Epoch 8\\Batch 40350\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:40] Epoch 8\\Batch 40400\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:45] Epoch 8\\Batch 40450\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:50] Epoch 8\\Batch 40500\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:25:55] Epoch 8\\Batch 40550\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:00] Epoch 8\\Batch 40600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:05] Epoch 8\\Batch 40650\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:10] Epoch 8\\Batch 40700\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:15] Epoch 8\\Batch 40750\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:20] Epoch 8\\Batch 40800\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:25] Epoch 8\\Batch 40850\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:30] Epoch 8\\Batch 40900\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:35] Epoch 8\\Batch 40950\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:40] Epoch 8\\Batch 41000\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:45] Epoch 8\\Batch 41050\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:50] Epoch 8\\Batch 41100\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:26:55] Epoch 8\\Batch 41150\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:00] Epoch 8\\Batch 41200\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:05] Epoch 8\\Batch 41250\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:10] Epoch 8\\Batch 41300\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:14] Epoch 8\\Batch 41350\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:19] Epoch 8\\Batch 41400\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:25] Epoch 8\\Batch 41450\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:30] Epoch 8\\Batch 41500\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:34] Epoch 8\\Batch 41550\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:39] Epoch 8\\Batch 41600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:44] Epoch 8\\Batch 41650\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:49] Epoch 8\\Batch 41700\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:27:55] Epoch 8\\Batch 41750\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:00] Epoch 8\\Batch 41800\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:05] Epoch 8\\Batch 41850\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:10] Epoch 8\\Batch 41900\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:15] Epoch 8\\Batch 41950\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:19] Epoch 8\\Batch 42000\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5431.000001, 'TP': 1568.0000009999999, 'FP': 1629.0000009999999}\n",
      "[2019/03/18 04:28:38] Epoch 8/ Validation Loss:9.772/ F1_score:0.308/ Precision:0.490/ Recall:0.224\n",
      "[2019/03/18 04:28:41] Epoch 8\\Batch 42050\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:45] Epoch 8\\Batch 42100\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:50] Epoch 8\\Batch 42150\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:28:55] Epoch 8\\Batch 42200\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:00] Epoch 8\\Batch 42250\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:05] Epoch 8\\Batch 42300\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:10] Epoch 8\\Batch 42350\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:15] Epoch 8\\Batch 42400\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:20] Epoch 8\\Batch 42450\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:25] Epoch 8\\Batch 42500\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:30] Epoch 8\\Batch 42550\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:35] Epoch 8\\Batch 42600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:40] Epoch 8\\Batch 42650\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:45] Epoch 8\\Batch 42700\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:50] Epoch 8\\Batch 42750\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:29:55] Epoch 8\\Batch 42800\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:00] Epoch 8\\Batch 42850\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:05] Epoch 8\\Batch 42900\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:10] Epoch 8\\Batch 42950\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:15] Epoch 8\\Batch 43000\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:20] Epoch 8\\Batch 43050\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:25] Epoch 8\\Batch 43100\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:30] Epoch 8\\Batch 43150\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:35] Epoch 8\\Batch 43200\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:40] Epoch 8\\Batch 43250\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:45] Epoch 8\\Batch 43300\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:50] Epoch 8\\Batch 43350\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:30:55] Epoch 8\\Batch 43400\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:00] Epoch 8\\Batch 43450\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:05] Epoch 8\\Batch 43500\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:09] Epoch 8\\Batch 43550\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:14] Epoch 8\\Batch 43600\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:19] Epoch 8\\Batch 43650\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:24] Epoch 8\\Batch 43700\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:29] Epoch 8\\Batch 43750\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:34] Epoch 8\\Batch 43800\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:38] Epoch 8\\Batch 43850\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:42] Epoch 8\\Batch 43900\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:45] Epoch 8\\Batch 43950\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:48] Epoch 8\\Batch 44000\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:52] Epoch 8\\Batch 44050\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:55] Epoch 8\\Batch 44100\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:31:58] Epoch 8\\Batch 44150\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:02] Epoch 8\\Batch 44200\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:05] Epoch 8\\Batch 44250\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:09] Epoch 8\\Batch 44300\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:12] Epoch 8\\Batch 44350\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:15] Epoch 8\\Batch 44400\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:19] Epoch 8\\Batch 44450\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:22] Epoch 8\\Batch 44500\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:26] Epoch 8\\Batch 44550\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:31] Epoch 8\\Batch 44600\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:36] Epoch 8\\Batch 44650\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:41] Epoch 8\\Batch 44700\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:46] Epoch 8\\Batch 44750\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:51] Epoch 8\\Batch 44800\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:32:56] Epoch 8\\Batch 44850\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:01] Epoch 8\\Batch 44900\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:06] Epoch 8\\Batch 44950\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:11] Epoch 8\\Batch 45000\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5467.000001, 'TP': 1532.0000009999999, 'FP': 1648.0000009999999}\n",
      "[2019/03/18 04:33:34] Epoch 8/ Validation Loss:9.769/ F1_score:0.301/ Precision:0.482/ Recall:0.219\n",
      "[2019/03/18 04:33:39] Epoch 8\\Batch 45050\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:44] Epoch 8\\Batch 45100\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:49] Epoch 8\\Batch 45150\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:54] Epoch 8\\Batch 45200\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:33:59] Epoch 8\\Batch 45250\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:04] Epoch 8\\Batch 45300\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:09] Epoch 8\\Batch 45350\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:14] Epoch 8\\Batch 45400\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:19] Epoch 8\\Batch 45450\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:24] Epoch 8\\Batch 45500\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:29] Epoch 8\\Batch 45550\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:34] Epoch 8\\Batch 45600\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:39] Epoch 8\\Batch 45650\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:44] Epoch 8\\Batch 45700\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:49] Epoch 8\\Batch 45750\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:54] Epoch 8\\Batch 45800\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:34:59] Epoch 8\\Batch 45850\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:04] Epoch 8\\Batch 45900\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:09] Epoch 8\\Batch 45950\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:14] Epoch 8\\Batch 46000\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:19] Epoch 8\\Batch 46050\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:24] Epoch 8\\Batch 46100\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:29] Epoch 8\\Batch 46150\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:34] Epoch 8\\Batch 46200\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:44] Epoch 9\\Batch 50\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:49] Epoch 9\\Batch 100\\ Train Loss:10.284\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:54] Epoch 9\\Batch 150\\ Train Loss:10.320\\ Learning rate:0.00030\n",
      "[2019/03/18 04:35:59] Epoch 9\\Batch 200\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:04] Epoch 9\\Batch 250\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:09] Epoch 9\\Batch 300\\ Train Loss:10.269\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:13] Epoch 9\\Batch 350\\ Train Loss:10.279\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:18] Epoch 9\\Batch 400\\ Train Loss:10.282\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:23] Epoch 9\\Batch 450\\ Train Loss:10.262\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:28] Epoch 9\\Batch 500\\ Train Loss:10.248\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:33] Epoch 9\\Batch 550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:38] Epoch 9\\Batch 600\\ Train Loss:10.239\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:43] Epoch 9\\Batch 650\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:48] Epoch 9\\Batch 700\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:53] Epoch 9\\Batch 750\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 04:36:58] Epoch 9\\Batch 800\\ Train Loss:10.237\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:03] Epoch 9\\Batch 850\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:08] Epoch 9\\Batch 900\\ Train Loss:10.245\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:13] Epoch 9\\Batch 950\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:18] Epoch 9\\Batch 1000\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:23] Epoch 9\\Batch 1050\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:28] Epoch 9\\Batch 1100\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:33] Epoch 9\\Batch 1150\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:37] Epoch 9\\Batch 1200\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:42] Epoch 9\\Batch 1250\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:47] Epoch 9\\Batch 1300\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:52] Epoch 9\\Batch 1350\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:37:57] Epoch 9\\Batch 1400\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:02] Epoch 9\\Batch 1450\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:07] Epoch 9\\Batch 1500\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:12] Epoch 9\\Batch 1550\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:17] Epoch 9\\Batch 1600\\ Train Loss:10.217\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:22] Epoch 9\\Batch 1650\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:27] Epoch 9\\Batch 1700\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:32] Epoch 9\\Batch 1750\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:36] Epoch 9\\Batch 1800\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:41] Epoch 9\\Batch 1850\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:46] Epoch 9\\Batch 1900\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:51] Epoch 9\\Batch 1950\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:38:56] Epoch 9\\Batch 2000\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:01] Epoch 9\\Batch 2050\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:06] Epoch 9\\Batch 2100\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:09] Epoch 9\\Batch 2150\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:13] Epoch 9\\Batch 2200\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:16] Epoch 9\\Batch 2250\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:19] Epoch 9\\Batch 2300\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:22] Epoch 9\\Batch 2350\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:26] Epoch 9\\Batch 2400\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:29] Epoch 9\\Batch 2450\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:33] Epoch 9\\Batch 2500\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:38] Epoch 9\\Batch 2550\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:43] Epoch 9\\Batch 2600\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:48] Epoch 9\\Batch 2650\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:53] Epoch 9\\Batch 2700\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:39:58] Epoch 9\\Batch 2750\\ Train Loss:10.239\\ Learning rate:0.00030\n",
      "[2019/03/18 04:40:03] Epoch 9\\Batch 2800\\ Train Loss:10.236\\ Learning rate:0.00030\n",
      "[2019/03/18 04:40:07] Epoch 9\\Batch 2850\\ Train Loss:10.235\\ Learning rate:0.00030\n",
      "[2019/03/18 04:40:12] Epoch 9\\Batch 2900\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:40:17] Epoch 9\\Batch 2950\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:40:22] Epoch 9\\Batch 3000\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5490.000001, 'TP': 1509.0000009999999, 'FP': 1664.0000009999999}\n",
      "[2019/03/18 04:40:45] Epoch 9/ Validation Loss:9.772/ F1_score:0.297/ Precision:0.476/ Recall:0.216\n",
      "[2019/03/18 04:40:50] Epoch 9\\Batch 3050\\ Train Loss:10.232\\ Learning rate:0.00030\n",
      "[2019/03/18 04:40:55] Epoch 9\\Batch 3100\\ Train Loss:10.234\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:00] Epoch 9\\Batch 3150\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:05] Epoch 9\\Batch 3200\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:10] Epoch 9\\Batch 3250\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:15] Epoch 9\\Batch 3300\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:20] Epoch 9\\Batch 3350\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:25] Epoch 9\\Batch 3400\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:30] Epoch 9\\Batch 3450\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:34] Epoch 9\\Batch 3500\\ Train Loss:10.223\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:41:39] Epoch 9\\Batch 3550\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:44] Epoch 9\\Batch 3600\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:49] Epoch 9\\Batch 3650\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:54] Epoch 9\\Batch 3700\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 04:41:59] Epoch 9\\Batch 3750\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:04] Epoch 9\\Batch 3800\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:09] Epoch 9\\Batch 3850\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:14] Epoch 9\\Batch 3900\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:19] Epoch 9\\Batch 3950\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:24] Epoch 9\\Batch 4000\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:29] Epoch 9\\Batch 4050\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:34] Epoch 9\\Batch 4100\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:39] Epoch 9\\Batch 4150\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:44] Epoch 9\\Batch 4200\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:49] Epoch 9\\Batch 4250\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:54] Epoch 9\\Batch 4300\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 04:42:59] Epoch 9\\Batch 4350\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:04] Epoch 9\\Batch 4400\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:09] Epoch 9\\Batch 4450\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:13] Epoch 9\\Batch 4500\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:18] Epoch 9\\Batch 4550\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:23] Epoch 9\\Batch 4600\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:28] Epoch 9\\Batch 4650\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:33] Epoch 9\\Batch 4700\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:38] Epoch 9\\Batch 4750\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:43] Epoch 9\\Batch 4800\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:48] Epoch 9\\Batch 4850\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:53] Epoch 9\\Batch 4900\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:43:58] Epoch 9\\Batch 4950\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:02] Epoch 9\\Batch 5000\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:07] Epoch 9\\Batch 5050\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:12] Epoch 9\\Batch 5100\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:17] Epoch 9\\Batch 5150\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:22] Epoch 9\\Batch 5200\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:27] Epoch 9\\Batch 5250\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:32] Epoch 9\\Batch 5300\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:37] Epoch 9\\Batch 5350\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:42] Epoch 9\\Batch 5400\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:47] Epoch 9\\Batch 5450\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:52] Epoch 9\\Batch 5500\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:44:56] Epoch 9\\Batch 5550\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:01] Epoch 9\\Batch 5600\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:06] Epoch 9\\Batch 5650\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:11] Epoch 9\\Batch 5700\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:16] Epoch 9\\Batch 5750\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:21] Epoch 9\\Batch 5800\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:26] Epoch 9\\Batch 5850\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:31] Epoch 9\\Batch 5900\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:36] Epoch 9\\Batch 5950\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:45:41] Epoch 9\\Batch 6000\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1653.0000009999999}\n",
      "[2019/03/18 04:46:03] Epoch 9/ Validation Loss:9.777/ F1_score:0.303/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 04:46:06] Epoch 9\\Batch 6050\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:10] Epoch 9\\Batch 6100\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:13] Epoch 9\\Batch 6150\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:16] Epoch 9\\Batch 6200\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:19] Epoch 9\\Batch 6250\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:23] Epoch 9\\Batch 6300\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:26] Epoch 9\\Batch 6350\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:31] Epoch 9\\Batch 6400\\ Train Loss:10.231\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:36] Epoch 9\\Batch 6450\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:41] Epoch 9\\Batch 6500\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:46] Epoch 9\\Batch 6550\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:51] Epoch 9\\Batch 6600\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:46:55] Epoch 9\\Batch 6650\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:00] Epoch 9\\Batch 6700\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:05] Epoch 9\\Batch 6750\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:10] Epoch 9\\Batch 6800\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:15] Epoch 9\\Batch 6850\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:20] Epoch 9\\Batch 6900\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:25] Epoch 9\\Batch 6950\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:30] Epoch 9\\Batch 7000\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:35] Epoch 9\\Batch 7050\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:39] Epoch 9\\Batch 7100\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:45] Epoch 9\\Batch 7150\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:49] Epoch 9\\Batch 7200\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:54] Epoch 9\\Batch 7250\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:47:59] Epoch 9\\Batch 7300\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:04] Epoch 9\\Batch 7350\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:09] Epoch 9\\Batch 7400\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:14] Epoch 9\\Batch 7450\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:19] Epoch 9\\Batch 7500\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:24] Epoch 9\\Batch 7550\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:29] Epoch 9\\Batch 7600\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:34] Epoch 9\\Batch 7650\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:39] Epoch 9\\Batch 7700\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:44] Epoch 9\\Batch 7750\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:49] Epoch 9\\Batch 7800\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:54] Epoch 9\\Batch 7850\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:48:59] Epoch 9\\Batch 7900\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:04] Epoch 9\\Batch 7950\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:09] Epoch 9\\Batch 8000\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:14] Epoch 9\\Batch 8050\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:19] Epoch 9\\Batch 8100\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:23] Epoch 9\\Batch 8150\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:28] Epoch 9\\Batch 8200\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:33] Epoch 9\\Batch 8250\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:38] Epoch 9\\Batch 8300\\ Train Loss:10.224\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:49:43] Epoch 9\\Batch 8350\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:48] Epoch 9\\Batch 8400\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:53] Epoch 9\\Batch 8450\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:49:58] Epoch 9\\Batch 8500\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:03] Epoch 9\\Batch 8550\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:08] Epoch 9\\Batch 8600\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:12] Epoch 9\\Batch 8650\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:17] Epoch 9\\Batch 8700\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:22] Epoch 9\\Batch 8750\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:27] Epoch 9\\Batch 8800\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:32] Epoch 9\\Batch 8850\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:37] Epoch 9\\Batch 8900\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:42] Epoch 9\\Batch 8950\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:50:47] Epoch 9\\Batch 9000\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5459.000001, 'TP': 1540.0000009999999, 'FP': 1656.0000009999999}\n",
      "[2019/03/18 04:51:10] Epoch 9/ Validation Loss:9.755/ F1_score:0.302/ Precision:0.482/ Recall:0.220\n",
      "[2019/03/18 04:51:15] Epoch 9\\Batch 9050\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:20] Epoch 9\\Batch 9100\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:25] Epoch 9\\Batch 9150\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:30] Epoch 9\\Batch 9200\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:35] Epoch 9\\Batch 9250\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:40] Epoch 9\\Batch 9300\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:45] Epoch 9\\Batch 9350\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:49] Epoch 9\\Batch 9400\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:54] Epoch 9\\Batch 9450\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:51:59] Epoch 9\\Batch 9500\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:04] Epoch 9\\Batch 9550\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:09] Epoch 9\\Batch 9600\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:14] Epoch 9\\Batch 9650\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:19] Epoch 9\\Batch 9700\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:24] Epoch 9\\Batch 9750\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:29] Epoch 9\\Batch 9800\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:34] Epoch 9\\Batch 9850\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:39] Epoch 9\\Batch 9900\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:44] Epoch 9\\Batch 9950\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:49] Epoch 9\\Batch 10000\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:54] Epoch 9\\Batch 10050\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:52:59] Epoch 9\\Batch 10100\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:04] Epoch 9\\Batch 10150\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:08] Epoch 9\\Batch 10200\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:12] Epoch 9\\Batch 10250\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:15] Epoch 9\\Batch 10300\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:18] Epoch 9\\Batch 10350\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:21] Epoch 9\\Batch 10400\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:25] Epoch 9\\Batch 10450\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:28] Epoch 9\\Batch 10500\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:31] Epoch 9\\Batch 10550\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:35] Epoch 9\\Batch 10600\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:40] Epoch 9\\Batch 10650\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:45] Epoch 9\\Batch 10700\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:50] Epoch 9\\Batch 10750\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:53:55] Epoch 9\\Batch 10800\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:00] Epoch 9\\Batch 10850\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:05] Epoch 9\\Batch 10900\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:10] Epoch 9\\Batch 10950\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:15] Epoch 9\\Batch 11000\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:19] Epoch 9\\Batch 11050\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:24] Epoch 9\\Batch 11100\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:29] Epoch 9\\Batch 11150\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:34] Epoch 9\\Batch 11200\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:39] Epoch 9\\Batch 11250\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:44] Epoch 9\\Batch 11300\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:49] Epoch 9\\Batch 11350\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:54] Epoch 9\\Batch 11400\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:54:59] Epoch 9\\Batch 11450\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:04] Epoch 9\\Batch 11500\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:09] Epoch 9\\Batch 11550\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:14] Epoch 9\\Batch 11600\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:19] Epoch 9\\Batch 11650\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:24] Epoch 9\\Batch 11700\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:29] Epoch 9\\Batch 11750\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:33] Epoch 9\\Batch 11800\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:38] Epoch 9\\Batch 11850\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:43] Epoch 9\\Batch 11900\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:48] Epoch 9\\Batch 11950\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:55:53] Epoch 9\\Batch 12000\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5465.000001, 'TP': 1534.0000009999999, 'FP': 1664.0000009999999}\n",
      "[2019/03/18 04:56:16] Epoch 9/ Validation Loss:9.744/ F1_score:0.301/ Precision:0.480/ Recall:0.219\n",
      "[2019/03/18 04:56:21] Epoch 9\\Batch 12050\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:26] Epoch 9\\Batch 12100\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:31] Epoch 9\\Batch 12150\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:36] Epoch 9\\Batch 12200\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:41] Epoch 9\\Batch 12250\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:46] Epoch 9\\Batch 12300\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:51] Epoch 9\\Batch 12350\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:56:56] Epoch 9\\Batch 12400\\ Train Loss:10.226\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:01] Epoch 9\\Batch 12450\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:06] Epoch 9\\Batch 12500\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:11] Epoch 9\\Batch 12550\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:15] Epoch 9\\Batch 12600\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:20] Epoch 9\\Batch 12650\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:25] Epoch 9\\Batch 12700\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:30] Epoch 9\\Batch 12750\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:35] Epoch 9\\Batch 12800\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:40] Epoch 9\\Batch 12850\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:45] Epoch 9\\Batch 12900\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:57:50] Epoch 9\\Batch 12950\\ Train Loss:10.222\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 04:57:55] Epoch 9\\Batch 13000\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:00] Epoch 9\\Batch 13050\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:04] Epoch 9\\Batch 13100\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:09] Epoch 9\\Batch 13150\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:14] Epoch 9\\Batch 13200\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:19] Epoch 9\\Batch 13250\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:24] Epoch 9\\Batch 13300\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:29] Epoch 9\\Batch 13350\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:34] Epoch 9\\Batch 13400\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:39] Epoch 9\\Batch 13450\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:44] Epoch 9\\Batch 13500\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:49] Epoch 9\\Batch 13550\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:54] Epoch 9\\Batch 13600\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:58:59] Epoch 9\\Batch 13650\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:04] Epoch 9\\Batch 13700\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:09] Epoch 9\\Batch 13750\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:13] Epoch 9\\Batch 13800\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:18] Epoch 9\\Batch 13850\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:23] Epoch 9\\Batch 13900\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:28] Epoch 9\\Batch 13950\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:33] Epoch 9\\Batch 14000\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:38] Epoch 9\\Batch 14050\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:43] Epoch 9\\Batch 14100\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:48] Epoch 9\\Batch 14150\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:53] Epoch 9\\Batch 14200\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 04:59:58] Epoch 9\\Batch 14250\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:03] Epoch 9\\Batch 14300\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:08] Epoch 9\\Batch 14350\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:13] Epoch 9\\Batch 14400\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:16] Epoch 9\\Batch 14450\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:20] Epoch 9\\Batch 14500\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:23] Epoch 9\\Batch 14550\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:26] Epoch 9\\Batch 14600\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:29] Epoch 9\\Batch 14650\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:33] Epoch 9\\Batch 14700\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:36] Epoch 9\\Batch 14750\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:40] Epoch 9\\Batch 14800\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:44] Epoch 9\\Batch 14850\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:50] Epoch 9\\Batch 14900\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:55] Epoch 9\\Batch 14950\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:00:59] Epoch 9\\Batch 15000\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5471.000001, 'TP': 1528.0000009999999, 'FP': 1664.0000009999999}\n",
      "[2019/03/18 05:01:22] Epoch 9/ Validation Loss:9.750/ F1_score:0.300/ Precision:0.479/ Recall:0.218\n",
      "[2019/03/18 05:01:27] Epoch 9\\Batch 15050\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:01:32] Epoch 9\\Batch 15100\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:01:37] Epoch 9\\Batch 15150\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:01:42] Epoch 9\\Batch 15200\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:01:47] Epoch 9\\Batch 15250\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:01:52] Epoch 9\\Batch 15300\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:01:57] Epoch 9\\Batch 15350\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:02] Epoch 9\\Batch 15400\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:07] Epoch 9\\Batch 15450\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:11] Epoch 9\\Batch 15500\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:16] Epoch 9\\Batch 15550\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:21] Epoch 9\\Batch 15600\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:26] Epoch 9\\Batch 15650\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:31] Epoch 9\\Batch 15700\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:36] Epoch 9\\Batch 15750\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:41] Epoch 9\\Batch 15800\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:46] Epoch 9\\Batch 15850\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:51] Epoch 9\\Batch 15900\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:02:56] Epoch 9\\Batch 15950\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:01] Epoch 9\\Batch 16000\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:06] Epoch 9\\Batch 16050\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:11] Epoch 9\\Batch 16100\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:16] Epoch 9\\Batch 16150\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:20] Epoch 9\\Batch 16200\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:25] Epoch 9\\Batch 16250\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:30] Epoch 9\\Batch 16300\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:36] Epoch 9\\Batch 16350\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:41] Epoch 9\\Batch 16400\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:45] Epoch 9\\Batch 16450\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:50] Epoch 9\\Batch 16500\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:03:55] Epoch 9\\Batch 16550\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:00] Epoch 9\\Batch 16600\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:05] Epoch 9\\Batch 16650\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:10] Epoch 9\\Batch 16700\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:15] Epoch 9\\Batch 16750\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:20] Epoch 9\\Batch 16800\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:25] Epoch 9\\Batch 16850\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:30] Epoch 9\\Batch 16900\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:35] Epoch 9\\Batch 16950\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:40] Epoch 9\\Batch 17000\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:45] Epoch 9\\Batch 17050\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:50] Epoch 9\\Batch 17100\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:04:55] Epoch 9\\Batch 17150\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:00] Epoch 9\\Batch 17200\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:05] Epoch 9\\Batch 17250\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:10] Epoch 9\\Batch 17300\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:15] Epoch 9\\Batch 17350\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:20] Epoch 9\\Batch 17400\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:24] Epoch 9\\Batch 17450\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:29] Epoch 9\\Batch 17500\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:34] Epoch 9\\Batch 17550\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:39] Epoch 9\\Batch 17600\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:44] Epoch 9\\Batch 17650\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:49] Epoch 9\\Batch 17700\\ Train Loss:10.220\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 05:05:54] Epoch 9\\Batch 17750\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:05:59] Epoch 9\\Batch 17800\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:06:04] Epoch 9\\Batch 17850\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:06:09] Epoch 9\\Batch 17900\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:06:14] Epoch 9\\Batch 17950\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:06:19] Epoch 9\\Batch 18000\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1655.0000009999999}\n",
      "[2019/03/18 05:06:42] Epoch 9/ Validation Loss:9.733/ F1_score:0.303/ Precision:0.482/ Recall:0.220\n",
      "[2019/03/18 05:06:47] Epoch 9\\Batch 18050\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:06:52] Epoch 9\\Batch 18100\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:06:57] Epoch 9\\Batch 18150\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:02] Epoch 9\\Batch 18200\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:07] Epoch 9\\Batch 18250\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:11] Epoch 9\\Batch 18300\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:14] Epoch 9\\Batch 18350\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:17] Epoch 9\\Batch 18400\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:20] Epoch 9\\Batch 18450\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:24] Epoch 9\\Batch 18500\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:27] Epoch 9\\Batch 18550\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:30] Epoch 9\\Batch 18600\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:33] Epoch 9\\Batch 18650\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:38] Epoch 9\\Batch 18700\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:43] Epoch 9\\Batch 18750\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:48] Epoch 9\\Batch 18800\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:53] Epoch 9\\Batch 18850\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:07:58] Epoch 9\\Batch 18900\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:03] Epoch 9\\Batch 18950\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:08] Epoch 9\\Batch 19000\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:13] Epoch 9\\Batch 19050\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:18] Epoch 9\\Batch 19100\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:23] Epoch 9\\Batch 19150\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:28] Epoch 9\\Batch 19200\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:32] Epoch 9\\Batch 19250\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:37] Epoch 9\\Batch 19300\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:42] Epoch 9\\Batch 19350\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:48] Epoch 9\\Batch 19400\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:53] Epoch 9\\Batch 19450\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:08:57] Epoch 9\\Batch 19500\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:02] Epoch 9\\Batch 19550\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:07] Epoch 9\\Batch 19600\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:12] Epoch 9\\Batch 19650\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:17] Epoch 9\\Batch 19700\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:22] Epoch 9\\Batch 19750\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:27] Epoch 9\\Batch 19800\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:31] Epoch 9\\Batch 19850\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:36] Epoch 9\\Batch 19900\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:41] Epoch 9\\Batch 19950\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:46] Epoch 9\\Batch 20000\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:51] Epoch 9\\Batch 20050\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:09:56] Epoch 9\\Batch 20100\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:01] Epoch 9\\Batch 20150\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:06] Epoch 9\\Batch 20200\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:11] Epoch 9\\Batch 20250\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:16] Epoch 9\\Batch 20300\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:21] Epoch 9\\Batch 20350\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:26] Epoch 9\\Batch 20400\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:31] Epoch 9\\Batch 20450\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:36] Epoch 9\\Batch 20500\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:41] Epoch 9\\Batch 20550\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:46] Epoch 9\\Batch 20600\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:51] Epoch 9\\Batch 20650\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:10:56] Epoch 9\\Batch 20700\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:01] Epoch 9\\Batch 20750\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:06] Epoch 9\\Batch 20800\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:11] Epoch 9\\Batch 20850\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:16] Epoch 9\\Batch 20900\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:21] Epoch 9\\Batch 20950\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:26] Epoch 9\\Batch 21000\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5436.000001, 'TP': 1563.0000009999999, 'FP': 1641.0000009999999}\n",
      "[2019/03/18 05:11:48] Epoch 9/ Validation Loss:9.744/ F1_score:0.306/ Precision:0.488/ Recall:0.223\n",
      "[2019/03/18 05:11:53] Epoch 9\\Batch 21050\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:11:58] Epoch 9\\Batch 21100\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:03] Epoch 9\\Batch 21150\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:08] Epoch 9\\Batch 21200\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:14] Epoch 9\\Batch 21250\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:19] Epoch 9\\Batch 21300\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:24] Epoch 9\\Batch 21350\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:28] Epoch 9\\Batch 21400\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:33] Epoch 9\\Batch 21450\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:38] Epoch 9\\Batch 21500\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:43] Epoch 9\\Batch 21550\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:48] Epoch 9\\Batch 21600\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:53] Epoch 9\\Batch 21650\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:12:58] Epoch 9\\Batch 21700\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:03] Epoch 9\\Batch 21750\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:08] Epoch 9\\Batch 21800\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:13] Epoch 9\\Batch 21850\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:18] Epoch 9\\Batch 21900\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:23] Epoch 9\\Batch 21950\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:28] Epoch 9\\Batch 22000\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:33] Epoch 9\\Batch 22050\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:38] Epoch 9\\Batch 22100\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:42] Epoch 9\\Batch 22150\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:47] Epoch 9\\Batch 22200\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:52] Epoch 9\\Batch 22250\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:13:57] Epoch 9\\Batch 22300\\ Train Loss:10.219\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 05:14:02] Epoch 9\\Batch 22350\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:07] Epoch 9\\Batch 22400\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:12] Epoch 9\\Batch 22450\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:17] Epoch 9\\Batch 22500\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:20] Epoch 9\\Batch 22550\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:23] Epoch 9\\Batch 22600\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:26] Epoch 9\\Batch 22650\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:30] Epoch 9\\Batch 22700\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:33] Epoch 9\\Batch 22750\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:36] Epoch 9\\Batch 22800\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:39] Epoch 9\\Batch 22850\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:44] Epoch 9\\Batch 22900\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:49] Epoch 9\\Batch 22950\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:54] Epoch 9\\Batch 23000\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:14:58] Epoch 9\\Batch 23050\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:03] Epoch 9\\Batch 23100\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:08] Epoch 9\\Batch 23150\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:13] Epoch 9\\Batch 23200\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:18] Epoch 9\\Batch 23250\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:23] Epoch 9\\Batch 23300\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:28] Epoch 9\\Batch 23350\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:33] Epoch 9\\Batch 23400\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:38] Epoch 9\\Batch 23450\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:43] Epoch 9\\Batch 23500\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:48] Epoch 9\\Batch 23550\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:53] Epoch 9\\Batch 23600\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:15:58] Epoch 9\\Batch 23650\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:03] Epoch 9\\Batch 23700\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:08] Epoch 9\\Batch 23750\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:13] Epoch 9\\Batch 23800\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:17] Epoch 9\\Batch 23850\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:22] Epoch 9\\Batch 23900\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:27] Epoch 9\\Batch 23950\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "[2019/03/18 05:16:32] Epoch 9\\Batch 24000\\ Train Loss:10.220\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5435.000001, 'TP': 1564.0000009999999, 'FP': 1640.0000009999999}\n",
      "[2019/03/18 05:16:55] Epoch 9/ Validation Loss:9.718/ F1_score:0.307/ Precision:0.488/ Recall:0.223\n",
      "[2019/03/18 05:17:00] Epoch 9\\Batch 24050\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:05] Epoch 9\\Batch 24100\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:10] Epoch 9\\Batch 24150\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:15] Epoch 9\\Batch 24200\\ Train Loss:10.219\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:20] Epoch 9\\Batch 24250\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:25] Epoch 9\\Batch 24300\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:30] Epoch 9\\Batch 24350\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:35] Epoch 9\\Batch 24400\\ Train Loss:10.217\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:40] Epoch 9\\Batch 24450\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:45] Epoch 9\\Batch 24500\\ Train Loss:10.217\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:50] Epoch 9\\Batch 24550\\ Train Loss:10.217\\ Learning rate:0.00030\n",
      "[2019/03/18 05:17:55] Epoch 9\\Batch 24600\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:00] Epoch 9\\Batch 24650\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:05] Epoch 9\\Batch 24700\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:10] Epoch 9\\Batch 24750\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:15] Epoch 9\\Batch 24800\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:20] Epoch 9\\Batch 24850\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:24] Epoch 9\\Batch 24900\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:29] Epoch 9\\Batch 24950\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:34] Epoch 9\\Batch 25000\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:39] Epoch 9\\Batch 25050\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:44] Epoch 9\\Batch 25100\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:49] Epoch 9\\Batch 25150\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:54] Epoch 9\\Batch 25200\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:18:59] Epoch 9\\Batch 25250\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:04] Epoch 9\\Batch 25300\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:09] Epoch 9\\Batch 25350\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:14] Epoch 9\\Batch 25400\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:19] Epoch 9\\Batch 25450\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:24] Epoch 9\\Batch 25500\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:29] Epoch 9\\Batch 25550\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:34] Epoch 9\\Batch 25600\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:39] Epoch 9\\Batch 25650\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:44] Epoch 9\\Batch 25700\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:49] Epoch 9\\Batch 25750\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:54] Epoch 9\\Batch 25800\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:19:59] Epoch 9\\Batch 25850\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:04] Epoch 9\\Batch 25900\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:09] Epoch 9\\Batch 25950\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:14] Epoch 9\\Batch 26000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:19] Epoch 9\\Batch 26050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:24] Epoch 9\\Batch 26100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:29] Epoch 9\\Batch 26150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:34] Epoch 9\\Batch 26200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:39] Epoch 9\\Batch 26250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:44] Epoch 9\\Batch 26300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:48] Epoch 9\\Batch 26350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:53] Epoch 9\\Batch 26400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:20:58] Epoch 9\\Batch 26450\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:04] Epoch 9\\Batch 26500\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:09] Epoch 9\\Batch 26550\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:13] Epoch 9\\Batch 26600\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:18] Epoch 9\\Batch 26650\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:23] Epoch 9\\Batch 26700\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:27] Epoch 9\\Batch 26750\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:30] Epoch 9\\Batch 26800\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:33] Epoch 9\\Batch 26850\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:37] Epoch 9\\Batch 26900\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:40] Epoch 9\\Batch 26950\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:21:43] Epoch 9\\Batch 27000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 5469.000001, 'TP': 1530.0000009999999, 'FP': 1657.0000009999999}\n",
      "[2019/03/18 05:22:05] Epoch 9/ Validation Loss:9.745/ F1_score:0.300/ Precision:0.480/ Recall:0.219\n",
      "[2019/03/18 05:22:10] Epoch 9\\Batch 27050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:15] Epoch 9\\Batch 27100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:20] Epoch 9\\Batch 27150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:25] Epoch 9\\Batch 27200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:30] Epoch 9\\Batch 27250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:35] Epoch 9\\Batch 27300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:40] Epoch 9\\Batch 27350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:45] Epoch 9\\Batch 27400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:50] Epoch 9\\Batch 27450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:22:55] Epoch 9\\Batch 27500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:00] Epoch 9\\Batch 27550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:05] Epoch 9\\Batch 27600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:10] Epoch 9\\Batch 27650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:14] Epoch 9\\Batch 27700\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:19] Epoch 9\\Batch 27750\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:24] Epoch 9\\Batch 27800\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:29] Epoch 9\\Batch 27850\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:34] Epoch 9\\Batch 27900\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:39] Epoch 9\\Batch 27950\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:44] Epoch 9\\Batch 28000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:49] Epoch 9\\Batch 28050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:54] Epoch 9\\Batch 28100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:23:59] Epoch 9\\Batch 28150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:04] Epoch 9\\Batch 28200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:09] Epoch 9\\Batch 28250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:14] Epoch 9\\Batch 28300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:19] Epoch 9\\Batch 28350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:24] Epoch 9\\Batch 28400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:29] Epoch 9\\Batch 28450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:34] Epoch 9\\Batch 28500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:39] Epoch 9\\Batch 28550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:44] Epoch 9\\Batch 28600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:48] Epoch 9\\Batch 28650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:53] Epoch 9\\Batch 28700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:24:58] Epoch 9\\Batch 28750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:03] Epoch 9\\Batch 28800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:08] Epoch 9\\Batch 28850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:13] Epoch 9\\Batch 28900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:18] Epoch 9\\Batch 28950\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:23] Epoch 9\\Batch 29000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:28] Epoch 9\\Batch 29050\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:33] Epoch 9\\Batch 29100\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:37] Epoch 9\\Batch 29150\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:42] Epoch 9\\Batch 29200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:47] Epoch 9\\Batch 29250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:52] Epoch 9\\Batch 29300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:25:57] Epoch 9\\Batch 29350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:02] Epoch 9\\Batch 29400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:07] Epoch 9\\Batch 29450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:12] Epoch 9\\Batch 29500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:17] Epoch 9\\Batch 29550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:22] Epoch 9\\Batch 29600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:27] Epoch 9\\Batch 29650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:32] Epoch 9\\Batch 29700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:36] Epoch 9\\Batch 29750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:41] Epoch 9\\Batch 29800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:46] Epoch 9\\Batch 29850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:51] Epoch 9\\Batch 29900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:26:56] Epoch 9\\Batch 29950\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:01] Epoch 9\\Batch 30000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5449.000001, 'TP': 1550.0000009999999, 'FP': 1637.0000009999999}\n",
      "[2019/03/18 05:27:24] Epoch 9/ Validation Loss:9.734/ F1_score:0.304/ Precision:0.486/ Recall:0.221\n",
      "[2019/03/18 05:27:29] Epoch 9\\Batch 30050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:33] Epoch 9\\Batch 30100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:38] Epoch 9\\Batch 30150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:43] Epoch 9\\Batch 30200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:48] Epoch 9\\Batch 30250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:53] Epoch 9\\Batch 30300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:27:58] Epoch 9\\Batch 30350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:03] Epoch 9\\Batch 30400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:08] Epoch 9\\Batch 30450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:13] Epoch 9\\Batch 30500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:18] Epoch 9\\Batch 30550\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:21] Epoch 9\\Batch 30600\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:25] Epoch 9\\Batch 30650\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:28] Epoch 9\\Batch 30700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:31] Epoch 9\\Batch 30750\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:34] Epoch 9\\Batch 30800\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:38] Epoch 9\\Batch 30850\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:41] Epoch 9\\Batch 30900\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:44] Epoch 9\\Batch 30950\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:49] Epoch 9\\Batch 31000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:54] Epoch 9\\Batch 31050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:28:59] Epoch 9\\Batch 31100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:04] Epoch 9\\Batch 31150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:09] Epoch 9\\Batch 31200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:14] Epoch 9\\Batch 31250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:19] Epoch 9\\Batch 31300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:24] Epoch 9\\Batch 31350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:29] Epoch 9\\Batch 31400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:34] Epoch 9\\Batch 31450\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:38] Epoch 9\\Batch 31500\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:43] Epoch 9\\Batch 31550\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:48] Epoch 9\\Batch 31600\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:29:53] Epoch 9\\Batch 31650\\ Train Loss:10.213\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 05:29:58] Epoch 9\\Batch 31700\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:03] Epoch 9\\Batch 31750\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:08] Epoch 9\\Batch 31800\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:13] Epoch 9\\Batch 31850\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:18] Epoch 9\\Batch 31900\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:23] Epoch 9\\Batch 31950\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:28] Epoch 9\\Batch 32000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:33] Epoch 9\\Batch 32050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:38] Epoch 9\\Batch 32100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:43] Epoch 9\\Batch 32150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:47] Epoch 9\\Batch 32200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:52] Epoch 9\\Batch 32250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:30:57] Epoch 9\\Batch 32300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:02] Epoch 9\\Batch 32350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:07] Epoch 9\\Batch 32400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:12] Epoch 9\\Batch 32450\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:17] Epoch 9\\Batch 32500\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:22] Epoch 9\\Batch 32550\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:27] Epoch 9\\Batch 32600\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:32] Epoch 9\\Batch 32650\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:37] Epoch 9\\Batch 32700\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:42] Epoch 9\\Batch 32750\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:47] Epoch 9\\Batch 32800\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:52] Epoch 9\\Batch 32850\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:31:57] Epoch 9\\Batch 32900\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:02] Epoch 9\\Batch 32950\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:07] Epoch 9\\Batch 33000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5460.000001, 'TP': 1539.0000009999999, 'FP': 1648.0000009999999}\n",
      "[2019/03/18 05:32:29] Epoch 9/ Validation Loss:9.739/ F1_score:0.302/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 05:32:34] Epoch 9\\Batch 33050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:39] Epoch 9\\Batch 33100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:44] Epoch 9\\Batch 33150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:49] Epoch 9\\Batch 33200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:54] Epoch 9\\Batch 33250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:32:59] Epoch 9\\Batch 33300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:04] Epoch 9\\Batch 33350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:09] Epoch 9\\Batch 33400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:13] Epoch 9\\Batch 33450\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:18] Epoch 9\\Batch 33500\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:23] Epoch 9\\Batch 33550\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:28] Epoch 9\\Batch 33600\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:33] Epoch 9\\Batch 33650\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:38] Epoch 9\\Batch 33700\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:43] Epoch 9\\Batch 33750\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:48] Epoch 9\\Batch 33800\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:53] Epoch 9\\Batch 33850\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:33:58] Epoch 9\\Batch 33900\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:03] Epoch 9\\Batch 33950\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:08] Epoch 9\\Batch 34000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:12] Epoch 9\\Batch 34050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:17] Epoch 9\\Batch 34100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:22] Epoch 9\\Batch 34150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:27] Epoch 9\\Batch 34200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:32] Epoch 9\\Batch 34250\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:37] Epoch 9\\Batch 34300\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:42] Epoch 9\\Batch 34350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:47] Epoch 9\\Batch 34400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:52] Epoch 9\\Batch 34450\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:34:57] Epoch 9\\Batch 34500\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:02] Epoch 9\\Batch 34550\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:07] Epoch 9\\Batch 34600\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:11] Epoch 9\\Batch 34650\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:16] Epoch 9\\Batch 34700\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:21] Epoch 9\\Batch 34750\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:26] Epoch 9\\Batch 34800\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:29] Epoch 9\\Batch 34850\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:32] Epoch 9\\Batch 34900\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:35] Epoch 9\\Batch 34950\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:39] Epoch 9\\Batch 35000\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:42] Epoch 9\\Batch 35050\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:45] Epoch 9\\Batch 35100\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:48] Epoch 9\\Batch 35150\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:53] Epoch 9\\Batch 35200\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:35:58] Epoch 9\\Batch 35250\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:03] Epoch 9\\Batch 35300\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:08] Epoch 9\\Batch 35350\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:12] Epoch 9\\Batch 35400\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:17] Epoch 9\\Batch 35450\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:22] Epoch 9\\Batch 35500\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:27] Epoch 9\\Batch 35550\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:32] Epoch 9\\Batch 35600\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:37] Epoch 9\\Batch 35650\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:42] Epoch 9\\Batch 35700\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:47] Epoch 9\\Batch 35750\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:52] Epoch 9\\Batch 35800\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:36:57] Epoch 9\\Batch 35850\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:37:02] Epoch 9\\Batch 35900\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:37:07] Epoch 9\\Batch 35950\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:37:12] Epoch 9\\Batch 36000\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5448.000001, 'TP': 1551.0000009999999, 'FP': 1637.0000009999999}\n",
      "[2019/03/18 05:37:35] Epoch 9/ Validation Loss:9.754/ F1_score:0.305/ Precision:0.487/ Recall:0.222\n",
      "[2019/03/18 05:37:40] Epoch 9\\Batch 36050\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:37:45] Epoch 9\\Batch 36100\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:37:50] Epoch 9\\Batch 36150\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:37:55] Epoch 9\\Batch 36200\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:00] Epoch 9\\Batch 36250\\ Train Loss:10.213\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 05:38:05] Epoch 9\\Batch 36300\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:10] Epoch 9\\Batch 36350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:15] Epoch 9\\Batch 36400\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:20] Epoch 9\\Batch 36450\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:25] Epoch 9\\Batch 36500\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:30] Epoch 9\\Batch 36550\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:35] Epoch 9\\Batch 36600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:40] Epoch 9\\Batch 36650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:45] Epoch 9\\Batch 36700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:50] Epoch 9\\Batch 36750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:38:55] Epoch 9\\Batch 36800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:01] Epoch 9\\Batch 36850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:05] Epoch 9\\Batch 36900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:10] Epoch 9\\Batch 36950\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:16] Epoch 9\\Batch 37000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:21] Epoch 9\\Batch 37050\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:26] Epoch 9\\Batch 37100\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:31] Epoch 9\\Batch 37150\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:36] Epoch 9\\Batch 37200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:41] Epoch 9\\Batch 37250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:46] Epoch 9\\Batch 37300\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:51] Epoch 9\\Batch 37350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:39:56] Epoch 9\\Batch 37400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:01] Epoch 9\\Batch 37450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:07] Epoch 9\\Batch 37500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:12] Epoch 9\\Batch 37550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:17] Epoch 9\\Batch 37600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:22] Epoch 9\\Batch 37650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:27] Epoch 9\\Batch 37700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:32] Epoch 9\\Batch 37750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:37] Epoch 9\\Batch 37800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:42] Epoch 9\\Batch 37850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:47] Epoch 9\\Batch 37900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:52] Epoch 9\\Batch 37950\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:40:57] Epoch 9\\Batch 38000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:02] Epoch 9\\Batch 38050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:07] Epoch 9\\Batch 38100\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:12] Epoch 9\\Batch 38150\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:17] Epoch 9\\Batch 38200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:22] Epoch 9\\Batch 38250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:27] Epoch 9\\Batch 38300\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:32] Epoch 9\\Batch 38350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:37] Epoch 9\\Batch 38400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:42] Epoch 9\\Batch 38450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:47] Epoch 9\\Batch 38500\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:52] Epoch 9\\Batch 38550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:41:57] Epoch 9\\Batch 38600\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:02] Epoch 9\\Batch 38650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:07] Epoch 9\\Batch 38700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:12] Epoch 9\\Batch 38750\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:18] Epoch 9\\Batch 38800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:23] Epoch 9\\Batch 38850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:28] Epoch 9\\Batch 38900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:33] Epoch 9\\Batch 38950\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:42:37] Epoch 9\\Batch 39000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5424.000001, 'TP': 1575.0000009999999, 'FP': 1625.0000009999999}\n",
      "[2019/03/18 05:42:54] Epoch 9/ Validation Loss:9.763/ F1_score:0.309/ Precision:0.492/ Recall:0.225\n",
      "[2019/03/18 05:42:59] Epoch 9\\Batch 39050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:04] Epoch 9\\Batch 39100\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:09] Epoch 9\\Batch 39150\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:14] Epoch 9\\Batch 39200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:19] Epoch 9\\Batch 39250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:24] Epoch 9\\Batch 39300\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:29] Epoch 9\\Batch 39350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:34] Epoch 9\\Batch 39400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:39] Epoch 9\\Batch 39450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:44] Epoch 9\\Batch 39500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:49] Epoch 9\\Batch 39550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:54] Epoch 9\\Batch 39600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:43:59] Epoch 9\\Batch 39650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:04] Epoch 9\\Batch 39700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:09] Epoch 9\\Batch 39750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:14] Epoch 9\\Batch 39800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:19] Epoch 9\\Batch 39850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:24] Epoch 9\\Batch 39900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:29] Epoch 9\\Batch 39950\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:34] Epoch 9\\Batch 40000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:39] Epoch 9\\Batch 40050\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:44] Epoch 9\\Batch 40100\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:49] Epoch 9\\Batch 40150\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:54] Epoch 9\\Batch 40200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:44:59] Epoch 9\\Batch 40250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:04] Epoch 9\\Batch 40300\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:09] Epoch 9\\Batch 40350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:14] Epoch 9\\Batch 40400\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:19] Epoch 9\\Batch 40450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:24] Epoch 9\\Batch 40500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:29] Epoch 9\\Batch 40550\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:34] Epoch 9\\Batch 40600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:39] Epoch 9\\Batch 40650\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:44] Epoch 9\\Batch 40700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:49] Epoch 9\\Batch 40750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:54] Epoch 9\\Batch 40800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:45:59] Epoch 9\\Batch 40850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:04] Epoch 9\\Batch 40900\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:10] Epoch 9\\Batch 40950\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:15] Epoch 9\\Batch 41000\\ Train Loss:10.212\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 05:46:20] Epoch 9\\Batch 41050\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:25] Epoch 9\\Batch 41100\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:30] Epoch 9\\Batch 41150\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:35] Epoch 9\\Batch 41200\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:40] Epoch 9\\Batch 41250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:45] Epoch 9\\Batch 41300\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:50] Epoch 9\\Batch 41350\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:46:55] Epoch 9\\Batch 41400\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:00] Epoch 9\\Batch 41450\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:06] Epoch 9\\Batch 41500\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:11] Epoch 9\\Batch 41550\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:16] Epoch 9\\Batch 41600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:21] Epoch 9\\Batch 41650\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:26] Epoch 9\\Batch 41700\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:31] Epoch 9\\Batch 41750\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:36] Epoch 9\\Batch 41800\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:41] Epoch 9\\Batch 41850\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:46] Epoch 9\\Batch 41900\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:51] Epoch 9\\Batch 41950\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:47:56] Epoch 9\\Batch 42000\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5421.000001, 'TP': 1578.0000009999999, 'FP': 1622.0000009999999}\n",
      "[2019/03/18 05:48:19] Epoch 9/ Validation Loss:9.737/ F1_score:0.309/ Precision:0.493/ Recall:0.225\n",
      "[2019/03/18 05:48:24] Epoch 9\\Batch 42050\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:29] Epoch 9\\Batch 42100\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:33] Epoch 9\\Batch 42150\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:38] Epoch 9\\Batch 42200\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:43] Epoch 9\\Batch 42250\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:48] Epoch 9\\Batch 42300\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:53] Epoch 9\\Batch 42350\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:48:58] Epoch 9\\Batch 42400\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:03] Epoch 9\\Batch 42450\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:08] Epoch 9\\Batch 42500\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:13] Epoch 9\\Batch 42550\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:18] Epoch 9\\Batch 42600\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:23] Epoch 9\\Batch 42650\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:28] Epoch 9\\Batch 42700\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:33] Epoch 9\\Batch 42750\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:38] Epoch 9\\Batch 42800\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:41] Epoch 9\\Batch 42850\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:44] Epoch 9\\Batch 42900\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:48] Epoch 9\\Batch 42950\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:51] Epoch 9\\Batch 43000\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:54] Epoch 9\\Batch 43050\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:49:58] Epoch 9\\Batch 43100\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:01] Epoch 9\\Batch 43150\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:05] Epoch 9\\Batch 43200\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:10] Epoch 9\\Batch 43250\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:15] Epoch 9\\Batch 43300\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:20] Epoch 9\\Batch 43350\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:25] Epoch 9\\Batch 43400\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:30] Epoch 9\\Batch 43450\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:35] Epoch 9\\Batch 43500\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:40] Epoch 9\\Batch 43550\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:45] Epoch 9\\Batch 43600\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:50] Epoch 9\\Batch 43650\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:50:55] Epoch 9\\Batch 43700\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:00] Epoch 9\\Batch 43750\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:05] Epoch 9\\Batch 43800\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:10] Epoch 9\\Batch 43850\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:15] Epoch 9\\Batch 43900\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:20] Epoch 9\\Batch 43950\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:25] Epoch 9\\Batch 44000\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:30] Epoch 9\\Batch 44050\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:35] Epoch 9\\Batch 44100\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:40] Epoch 9\\Batch 44150\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:45] Epoch 9\\Batch 44200\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:50] Epoch 9\\Batch 44250\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:51:55] Epoch 9\\Batch 44300\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:00] Epoch 9\\Batch 44350\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:05] Epoch 9\\Batch 44400\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:10] Epoch 9\\Batch 44450\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:15] Epoch 9\\Batch 44500\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:20] Epoch 9\\Batch 44550\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:25] Epoch 9\\Batch 44600\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:30] Epoch 9\\Batch 44650\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:35] Epoch 9\\Batch 44700\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:40] Epoch 9\\Batch 44750\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:46] Epoch 9\\Batch 44800\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:51] Epoch 9\\Batch 44850\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:52:56] Epoch 9\\Batch 44900\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:01] Epoch 9\\Batch 44950\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:06] Epoch 9\\Batch 45000\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5444.000001, 'TP': 1555.0000009999999, 'FP': 1627.0000009999999}\n",
      "[2019/03/18 05:53:29] Epoch 9/ Validation Loss:9.752/ F1_score:0.305/ Precision:0.489/ Recall:0.222\n",
      "[2019/03/18 05:53:34] Epoch 9\\Batch 45050\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:39] Epoch 9\\Batch 45100\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:44] Epoch 9\\Batch 45150\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:49] Epoch 9\\Batch 45200\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:54] Epoch 9\\Batch 45250\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:53:59] Epoch 9\\Batch 45300\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:04] Epoch 9\\Batch 45350\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:09] Epoch 9\\Batch 45400\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:14] Epoch 9\\Batch 45450\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:19] Epoch 9\\Batch 45500\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:24] Epoch 9\\Batch 45550\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:29] Epoch 9\\Batch 45600\\ Train Loss:10.209\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 05:54:34] Epoch 9\\Batch 45650\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:39] Epoch 9\\Batch 45700\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:44] Epoch 9\\Batch 45750\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:49] Epoch 9\\Batch 45800\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:54] Epoch 9\\Batch 45850\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:54:58] Epoch 9\\Batch 45900\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:03] Epoch 9\\Batch 45950\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:08] Epoch 9\\Batch 46000\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:13] Epoch 9\\Batch 46050\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:18] Epoch 9\\Batch 46100\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:23] Epoch 9\\Batch 46150\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:28] Epoch 9\\Batch 46200\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:38] Epoch 10\\Batch 50\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:43] Epoch 10\\Batch 100\\ Train Loss:10.280\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:48] Epoch 10\\Batch 150\\ Train Loss:10.313\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:53] Epoch 10\\Batch 200\\ Train Loss:10.264\\ Learning rate:0.00030\n",
      "[2019/03/18 05:55:58] Epoch 10\\Batch 250\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:03] Epoch 10\\Batch 300\\ Train Loss:10.268\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:08] Epoch 10\\Batch 350\\ Train Loss:10.266\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:13] Epoch 10\\Batch 400\\ Train Loss:10.270\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:18] Epoch 10\\Batch 450\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:23] Epoch 10\\Batch 500\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:28] Epoch 10\\Batch 550\\ Train Loss:10.222\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:32] Epoch 10\\Batch 600\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:37] Epoch 10\\Batch 650\\ Train Loss:10.221\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:42] Epoch 10\\Batch 700\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:47] Epoch 10\\Batch 750\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:50] Epoch 10\\Batch 800\\ Train Loss:10.224\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:53] Epoch 10\\Batch 850\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 05:56:57] Epoch 10\\Batch 900\\ Train Loss:10.233\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:00] Epoch 10\\Batch 950\\ Train Loss:10.225\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:03] Epoch 10\\Batch 1000\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:06] Epoch 10\\Batch 1050\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:10] Epoch 10\\Batch 1100\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:14] Epoch 10\\Batch 1150\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:19] Epoch 10\\Batch 1200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:24] Epoch 10\\Batch 1250\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:29] Epoch 10\\Batch 1300\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:34] Epoch 10\\Batch 1350\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:39] Epoch 10\\Batch 1400\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:44] Epoch 10\\Batch 1450\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:49] Epoch 10\\Batch 1500\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:53] Epoch 10\\Batch 1550\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 05:57:58] Epoch 10\\Batch 1600\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:03] Epoch 10\\Batch 1650\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:08] Epoch 10\\Batch 1700\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:13] Epoch 10\\Batch 1750\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:18] Epoch 10\\Batch 1800\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:23] Epoch 10\\Batch 1850\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:28] Epoch 10\\Batch 1900\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:33] Epoch 10\\Batch 1950\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:38] Epoch 10\\Batch 2000\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:43] Epoch 10\\Batch 2050\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:48] Epoch 10\\Batch 2100\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:53] Epoch 10\\Batch 2150\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 05:58:58] Epoch 10\\Batch 2200\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:03] Epoch 10\\Batch 2250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:08] Epoch 10\\Batch 2300\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:13] Epoch 10\\Batch 2350\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:17] Epoch 10\\Batch 2400\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:22] Epoch 10\\Batch 2450\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:27] Epoch 10\\Batch 2500\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:32] Epoch 10\\Batch 2550\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:37] Epoch 10\\Batch 2600\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:42] Epoch 10\\Batch 2650\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:47] Epoch 10\\Batch 2700\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:52] Epoch 10\\Batch 2750\\ Train Loss:10.217\\ Learning rate:0.00030\n",
      "[2019/03/18 05:59:57] Epoch 10\\Batch 2800\\ Train Loss:10.216\\ Learning rate:0.00030\n",
      "[2019/03/18 06:00:02] Epoch 10\\Batch 2850\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 06:00:07] Epoch 10\\Batch 2900\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 06:00:12] Epoch 10\\Batch 2950\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 06:00:17] Epoch 10\\Batch 3000\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5471.000001, 'TP': 1528.0000009999999, 'FP': 1655.0000009999999}\n",
      "[2019/03/18 06:00:40] Epoch 10/ Validation Loss:9.765/ F1_score:0.300/ Precision:0.480/ Recall:0.218\n",
      "[2019/03/18 06:00:45] Epoch 10\\Batch 3050\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 06:00:50] Epoch 10\\Batch 3100\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 06:00:55] Epoch 10\\Batch 3150\\ Train Loss:10.214\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:00] Epoch 10\\Batch 3200\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:05] Epoch 10\\Batch 3250\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:10] Epoch 10\\Batch 3300\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:15] Epoch 10\\Batch 3350\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:19] Epoch 10\\Batch 3400\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:24] Epoch 10\\Batch 3450\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:29] Epoch 10\\Batch 3500\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:34] Epoch 10\\Batch 3550\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:39] Epoch 10\\Batch 3600\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:44] Epoch 10\\Batch 3650\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:49] Epoch 10\\Batch 3700\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:54] Epoch 10\\Batch 3750\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:01:59] Epoch 10\\Batch 3800\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:03] Epoch 10\\Batch 3850\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:08] Epoch 10\\Batch 3900\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:13] Epoch 10\\Batch 3950\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:18] Epoch 10\\Batch 4000\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:23] Epoch 10\\Batch 4050\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:28] Epoch 10\\Batch 4100\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:33] Epoch 10\\Batch 4150\\ Train Loss:10.202\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:02:38] Epoch 10\\Batch 4200\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:43] Epoch 10\\Batch 4250\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:48] Epoch 10\\Batch 4300\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:53] Epoch 10\\Batch 4350\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:02:58] Epoch 10\\Batch 4400\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:03] Epoch 10\\Batch 4450\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:07] Epoch 10\\Batch 4500\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:12] Epoch 10\\Batch 4550\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:17] Epoch 10\\Batch 4600\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:22] Epoch 10\\Batch 4650\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:27] Epoch 10\\Batch 4700\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:32] Epoch 10\\Batch 4750\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:37] Epoch 10\\Batch 4800\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:42] Epoch 10\\Batch 4850\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:47] Epoch 10\\Batch 4900\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:52] Epoch 10\\Batch 4950\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:56] Epoch 10\\Batch 5000\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:03:59] Epoch 10\\Batch 5050\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:02] Epoch 10\\Batch 5100\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:05] Epoch 10\\Batch 5150\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:08] Epoch 10\\Batch 5200\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:12] Epoch 10\\Batch 5250\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:15] Epoch 10\\Batch 5300\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:18] Epoch 10\\Batch 5350\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:23] Epoch 10\\Batch 5400\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:28] Epoch 10\\Batch 5450\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:33] Epoch 10\\Batch 5500\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:38] Epoch 10\\Batch 5550\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:42] Epoch 10\\Batch 5600\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:47] Epoch 10\\Batch 5650\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:52] Epoch 10\\Batch 5700\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:04:57] Epoch 10\\Batch 5750\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:05:02] Epoch 10\\Batch 5800\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:05:07] Epoch 10\\Batch 5850\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:05:12] Epoch 10\\Batch 5900\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:05:17] Epoch 10\\Batch 5950\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:05:22] Epoch 10\\Batch 6000\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1649.0000009999999}\n",
      "[2019/03/18 06:05:45] Epoch 10/ Validation Loss:9.754/ F1_score:0.303/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 06:05:50] Epoch 10\\Batch 6050\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:05:55] Epoch 10\\Batch 6100\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:00] Epoch 10\\Batch 6150\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:05] Epoch 10\\Batch 6200\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:10] Epoch 10\\Batch 6250\\ Train Loss:10.212\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:15] Epoch 10\\Batch 6300\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:20] Epoch 10\\Batch 6350\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:25] Epoch 10\\Batch 6400\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:30] Epoch 10\\Batch 6450\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:35] Epoch 10\\Batch 6500\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:40] Epoch 10\\Batch 6550\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:45] Epoch 10\\Batch 6600\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:50] Epoch 10\\Batch 6650\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:54] Epoch 10\\Batch 6700\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:06:59] Epoch 10\\Batch 6750\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:04] Epoch 10\\Batch 6800\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:09] Epoch 10\\Batch 6850\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:14] Epoch 10\\Batch 6900\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:19] Epoch 10\\Batch 6950\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:24] Epoch 10\\Batch 7000\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:29] Epoch 10\\Batch 7050\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:34] Epoch 10\\Batch 7100\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:39] Epoch 10\\Batch 7150\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:44] Epoch 10\\Batch 7200\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:49] Epoch 10\\Batch 7250\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:54] Epoch 10\\Batch 7300\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:07:59] Epoch 10\\Batch 7350\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:04] Epoch 10\\Batch 7400\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:09] Epoch 10\\Batch 7450\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:14] Epoch 10\\Batch 7500\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:18] Epoch 10\\Batch 7550\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:23] Epoch 10\\Batch 7600\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:28] Epoch 10\\Batch 7650\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:33] Epoch 10\\Batch 7700\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:38] Epoch 10\\Batch 7750\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:43] Epoch 10\\Batch 7800\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:48] Epoch 10\\Batch 7850\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:53] Epoch 10\\Batch 7900\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:08:58] Epoch 10\\Batch 7950\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:03] Epoch 10\\Batch 8000\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:08] Epoch 10\\Batch 8050\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:13] Epoch 10\\Batch 8100\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:18] Epoch 10\\Batch 8150\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:23] Epoch 10\\Batch 8200\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:28] Epoch 10\\Batch 8250\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:32] Epoch 10\\Batch 8300\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:37] Epoch 10\\Batch 8350\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:42] Epoch 10\\Batch 8400\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:47] Epoch 10\\Batch 8450\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:52] Epoch 10\\Batch 8500\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:09:57] Epoch 10\\Batch 8550\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:02] Epoch 10\\Batch 8600\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:07] Epoch 10\\Batch 8650\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:12] Epoch 10\\Batch 8700\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:17] Epoch 10\\Batch 8750\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:22] Epoch 10\\Batch 8800\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:27] Epoch 10\\Batch 8850\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:32] Epoch 10\\Batch 8900\\ Train Loss:10.206\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:10:36] Epoch 10\\Batch 8950\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:10:41] Epoch 10\\Batch 9000\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5427.000001, 'TP': 1572.0000009999999, 'FP': 1635.0000009999999}\n",
      "[2019/03/18 06:11:02] Epoch 10/ Validation Loss:9.734/ F1_score:0.308/ Precision:0.490/ Recall:0.225\n",
      "[2019/03/18 06:11:05] Epoch 10\\Batch 9050\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:08] Epoch 10\\Batch 9100\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:12] Epoch 10\\Batch 9150\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:15] Epoch 10\\Batch 9200\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:19] Epoch 10\\Batch 9250\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:24] Epoch 10\\Batch 9300\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:29] Epoch 10\\Batch 9350\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:34] Epoch 10\\Batch 9400\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:39] Epoch 10\\Batch 9450\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:44] Epoch 10\\Batch 9500\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:48] Epoch 10\\Batch 9550\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:53] Epoch 10\\Batch 9600\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:11:58] Epoch 10\\Batch 9650\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:03] Epoch 10\\Batch 9700\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:08] Epoch 10\\Batch 9750\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:13] Epoch 10\\Batch 9800\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:18] Epoch 10\\Batch 9850\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:23] Epoch 10\\Batch 9900\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:28] Epoch 10\\Batch 9950\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:33] Epoch 10\\Batch 10000\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:38] Epoch 10\\Batch 10050\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:42] Epoch 10\\Batch 10100\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:47] Epoch 10\\Batch 10150\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:52] Epoch 10\\Batch 10200\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:12:57] Epoch 10\\Batch 10250\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:02] Epoch 10\\Batch 10300\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:07] Epoch 10\\Batch 10350\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:12] Epoch 10\\Batch 10400\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:17] Epoch 10\\Batch 10450\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:22] Epoch 10\\Batch 10500\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:27] Epoch 10\\Batch 10550\\ Train Loss:10.208\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:32] Epoch 10\\Batch 10600\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:37] Epoch 10\\Batch 10650\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:42] Epoch 10\\Batch 10700\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:47] Epoch 10\\Batch 10750\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:52] Epoch 10\\Batch 10800\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:13:57] Epoch 10\\Batch 10850\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:02] Epoch 10\\Batch 10900\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:07] Epoch 10\\Batch 10950\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:11] Epoch 10\\Batch 11000\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:16] Epoch 10\\Batch 11050\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:21] Epoch 10\\Batch 11100\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:26] Epoch 10\\Batch 11150\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:31] Epoch 10\\Batch 11200\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:36] Epoch 10\\Batch 11250\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:41] Epoch 10\\Batch 11300\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:46] Epoch 10\\Batch 11350\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:51] Epoch 10\\Batch 11400\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:14:56] Epoch 10\\Batch 11450\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:01] Epoch 10\\Batch 11500\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:06] Epoch 10\\Batch 11550\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:11] Epoch 10\\Batch 11600\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:16] Epoch 10\\Batch 11650\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:21] Epoch 10\\Batch 11700\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:25] Epoch 10\\Batch 11750\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:30] Epoch 10\\Batch 11800\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:35] Epoch 10\\Batch 11850\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:40] Epoch 10\\Batch 11900\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:45] Epoch 10\\Batch 11950\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:15:50] Epoch 10\\Batch 12000\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5433.000001, 'TP': 1566.0000009999999, 'FP': 1634.0000009999999}\n",
      "[2019/03/18 06:16:13] Epoch 10/ Validation Loss:9.731/ F1_score:0.307/ Precision:0.489/ Recall:0.224\n",
      "[2019/03/18 06:16:18] Epoch 10\\Batch 12050\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:23] Epoch 10\\Batch 12100\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:28] Epoch 10\\Batch 12150\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:33] Epoch 10\\Batch 12200\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:38] Epoch 10\\Batch 12250\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:43] Epoch 10\\Batch 12300\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:48] Epoch 10\\Batch 12350\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:53] Epoch 10\\Batch 12400\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:16:58] Epoch 10\\Batch 12450\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:03] Epoch 10\\Batch 12500\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:08] Epoch 10\\Batch 12550\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:13] Epoch 10\\Batch 12600\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:18] Epoch 10\\Batch 12650\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:23] Epoch 10\\Batch 12700\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:27] Epoch 10\\Batch 12750\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:32] Epoch 10\\Batch 12800\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:37] Epoch 10\\Batch 12850\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:42] Epoch 10\\Batch 12900\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:47] Epoch 10\\Batch 12950\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:52] Epoch 10\\Batch 13000\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:17:57] Epoch 10\\Batch 13050\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:01] Epoch 10\\Batch 13100\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:04] Epoch 10\\Batch 13150\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:08] Epoch 10\\Batch 13200\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:11] Epoch 10\\Batch 13250\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:14] Epoch 10\\Batch 13300\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:17] Epoch 10\\Batch 13350\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:21] Epoch 10\\Batch 13400\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:24] Epoch 10\\Batch 13450\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:29] Epoch 10\\Batch 13500\\ Train Loss:10.203\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:18:33] Epoch 10\\Batch 13550\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:38] Epoch 10\\Batch 13600\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:43] Epoch 10\\Batch 13650\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:48] Epoch 10\\Batch 13700\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:53] Epoch 10\\Batch 13750\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:18:58] Epoch 10\\Batch 13800\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:03] Epoch 10\\Batch 13850\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:08] Epoch 10\\Batch 13900\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:13] Epoch 10\\Batch 13950\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:18] Epoch 10\\Batch 14000\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:23] Epoch 10\\Batch 14050\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:28] Epoch 10\\Batch 14100\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:33] Epoch 10\\Batch 14150\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:38] Epoch 10\\Batch 14200\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:43] Epoch 10\\Batch 14250\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:48] Epoch 10\\Batch 14300\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:53] Epoch 10\\Batch 14350\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:19:58] Epoch 10\\Batch 14400\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:03] Epoch 10\\Batch 14450\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:08] Epoch 10\\Batch 14500\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:13] Epoch 10\\Batch 14550\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:18] Epoch 10\\Batch 14600\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:23] Epoch 10\\Batch 14650\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:28] Epoch 10\\Batch 14700\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:33] Epoch 10\\Batch 14750\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:38] Epoch 10\\Batch 14800\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:43] Epoch 10\\Batch 14850\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:48] Epoch 10\\Batch 14900\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:53] Epoch 10\\Batch 14950\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:20:58] Epoch 10\\Batch 15000\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5458.000001, 'TP': 1541.0000009999999, 'FP': 1634.0000009999999}\n",
      "[2019/03/18 06:21:18] Epoch 10/ Validation Loss:9.736/ F1_score:0.303/ Precision:0.485/ Recall:0.220\n",
      "[2019/03/18 06:21:21] Epoch 10\\Batch 15050\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:25] Epoch 10\\Batch 15100\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:28] Epoch 10\\Batch 15150\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:32] Epoch 10\\Batch 15200\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:35] Epoch 10\\Batch 15250\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:39] Epoch 10\\Batch 15300\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:42] Epoch 10\\Batch 15350\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:45] Epoch 10\\Batch 15400\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:49] Epoch 10\\Batch 15450\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:52] Epoch 10\\Batch 15500\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:21:56] Epoch 10\\Batch 15550\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:00] Epoch 10\\Batch 15600\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:06] Epoch 10\\Batch 15650\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:11] Epoch 10\\Batch 15700\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:16] Epoch 10\\Batch 15750\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:21] Epoch 10\\Batch 15800\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:26] Epoch 10\\Batch 15850\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:31] Epoch 10\\Batch 15900\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:35] Epoch 10\\Batch 15950\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:40] Epoch 10\\Batch 16000\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:45] Epoch 10\\Batch 16050\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:50] Epoch 10\\Batch 16100\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:22:56] Epoch 10\\Batch 16150\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:01] Epoch 10\\Batch 16200\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:06] Epoch 10\\Batch 16250\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:11] Epoch 10\\Batch 16300\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:16] Epoch 10\\Batch 16350\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:21] Epoch 10\\Batch 16400\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:26] Epoch 10\\Batch 16450\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:31] Epoch 10\\Batch 16500\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:36] Epoch 10\\Batch 16550\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:41] Epoch 10\\Batch 16600\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:46] Epoch 10\\Batch 16650\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:51] Epoch 10\\Batch 16700\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:23:55] Epoch 10\\Batch 16750\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:00] Epoch 10\\Batch 16800\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:05] Epoch 10\\Batch 16850\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:10] Epoch 10\\Batch 16900\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:15] Epoch 10\\Batch 16950\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:20] Epoch 10\\Batch 17000\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:25] Epoch 10\\Batch 17050\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:30] Epoch 10\\Batch 17100\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:35] Epoch 10\\Batch 17150\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:40] Epoch 10\\Batch 17200\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:45] Epoch 10\\Batch 17250\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:50] Epoch 10\\Batch 17300\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:24:55] Epoch 10\\Batch 17350\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:00] Epoch 10\\Batch 17400\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:05] Epoch 10\\Batch 17450\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:10] Epoch 10\\Batch 17500\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:15] Epoch 10\\Batch 17550\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:20] Epoch 10\\Batch 17600\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:25] Epoch 10\\Batch 17650\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:30] Epoch 10\\Batch 17700\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:35] Epoch 10\\Batch 17750\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:39] Epoch 10\\Batch 17800\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:44] Epoch 10\\Batch 17850\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:49] Epoch 10\\Batch 17900\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:54] Epoch 10\\Batch 17950\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:25:59] Epoch 10\\Batch 18000\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5429.000001, 'TP': 1570.0000009999999, 'FP': 1648.0000009999999}\n",
      "[2019/03/18 06:26:22] Epoch 10/ Validation Loss:9.739/ F1_score:0.307/ Precision:0.488/ Recall:0.224\n",
      "[2019/03/18 06:26:27] Epoch 10\\Batch 18050\\ Train Loss:10.199\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:26:32] Epoch 10\\Batch 18100\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:26:37] Epoch 10\\Batch 18150\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:26:42] Epoch 10\\Batch 18200\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 06:26:47] Epoch 10\\Batch 18250\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:26:52] Epoch 10\\Batch 18300\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:26:57] Epoch 10\\Batch 18350\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:02] Epoch 10\\Batch 18400\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:07] Epoch 10\\Batch 18450\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:12] Epoch 10\\Batch 18500\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:17] Epoch 10\\Batch 18550\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:21] Epoch 10\\Batch 18600\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:26] Epoch 10\\Batch 18650\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:31] Epoch 10\\Batch 18700\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:36] Epoch 10\\Batch 18750\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:41] Epoch 10\\Batch 18800\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:46] Epoch 10\\Batch 18850\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:51] Epoch 10\\Batch 18900\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:27:56] Epoch 10\\Batch 18950\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:01] Epoch 10\\Batch 19000\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:06] Epoch 10\\Batch 19050\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:11] Epoch 10\\Batch 19100\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:16] Epoch 10\\Batch 19150\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:21] Epoch 10\\Batch 19200\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:26] Epoch 10\\Batch 19250\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:31] Epoch 10\\Batch 19300\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:36] Epoch 10\\Batch 19350\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:40] Epoch 10\\Batch 19400\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:43] Epoch 10\\Batch 19450\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:47] Epoch 10\\Batch 19500\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:50] Epoch 10\\Batch 19550\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:53] Epoch 10\\Batch 19600\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:28:56] Epoch 10\\Batch 19650\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:00] Epoch 10\\Batch 19700\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:03] Epoch 10\\Batch 19750\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:07] Epoch 10\\Batch 19800\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:12] Epoch 10\\Batch 19850\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:17] Epoch 10\\Batch 19900\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:22] Epoch 10\\Batch 19950\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:27] Epoch 10\\Batch 20000\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:32] Epoch 10\\Batch 20050\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:37] Epoch 10\\Batch 20100\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:42] Epoch 10\\Batch 20150\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:47] Epoch 10\\Batch 20200\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:52] Epoch 10\\Batch 20250\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:29:56] Epoch 10\\Batch 20300\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:01] Epoch 10\\Batch 20350\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:06] Epoch 10\\Batch 20400\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:11] Epoch 10\\Batch 20450\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:16] Epoch 10\\Batch 20500\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:21] Epoch 10\\Batch 20550\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:26] Epoch 10\\Batch 20600\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:31] Epoch 10\\Batch 20650\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:36] Epoch 10\\Batch 20700\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:41] Epoch 10\\Batch 20750\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:46] Epoch 10\\Batch 20800\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:51] Epoch 10\\Batch 20850\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 06:30:56] Epoch 10\\Batch 20900\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:00] Epoch 10\\Batch 20950\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:05] Epoch 10\\Batch 21000\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5434.000001, 'TP': 1565.0000009999999, 'FP': 1638.0000009999999}\n",
      "[2019/03/18 06:31:28] Epoch 10/ Validation Loss:9.715/ F1_score:0.307/ Precision:0.489/ Recall:0.224\n",
      "[2019/03/18 06:31:33] Epoch 10\\Batch 21050\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:38] Epoch 10\\Batch 21100\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:43] Epoch 10\\Batch 21150\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:48] Epoch 10\\Batch 21200\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:53] Epoch 10\\Batch 21250\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:31:58] Epoch 10\\Batch 21300\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:03] Epoch 10\\Batch 21350\\ Train Loss:10.201\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:08] Epoch 10\\Batch 21400\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:13] Epoch 10\\Batch 21450\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:18] Epoch 10\\Batch 21500\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:23] Epoch 10\\Batch 21550\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:28] Epoch 10\\Batch 21600\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:33] Epoch 10\\Batch 21650\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:38] Epoch 10\\Batch 21700\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:43] Epoch 10\\Batch 21750\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:48] Epoch 10\\Batch 21800\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:52] Epoch 10\\Batch 21850\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:32:57] Epoch 10\\Batch 21900\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:02] Epoch 10\\Batch 21950\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:07] Epoch 10\\Batch 22000\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:12] Epoch 10\\Batch 22050\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:17] Epoch 10\\Batch 22100\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:22] Epoch 10\\Batch 22150\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:27] Epoch 10\\Batch 22200\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:32] Epoch 10\\Batch 22250\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:36] Epoch 10\\Batch 22300\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:41] Epoch 10\\Batch 22350\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:46] Epoch 10\\Batch 22400\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:51] Epoch 10\\Batch 22450\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:33:56] Epoch 10\\Batch 22500\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:01] Epoch 10\\Batch 22550\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:06] Epoch 10\\Batch 22600\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:11] Epoch 10\\Batch 22650\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:16] Epoch 10\\Batch 22700\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:21] Epoch 10\\Batch 22750\\ Train Loss:10.199\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:34:25] Epoch 10\\Batch 22800\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:30] Epoch 10\\Batch 22850\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:35] Epoch 10\\Batch 22900\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:40] Epoch 10\\Batch 22950\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:45] Epoch 10\\Batch 23000\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:50] Epoch 10\\Batch 23050\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:34:55] Epoch 10\\Batch 23100\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:00] Epoch 10\\Batch 23150\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:05] Epoch 10\\Batch 23200\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:10] Epoch 10\\Batch 23250\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:15] Epoch 10\\Batch 23300\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:20] Epoch 10\\Batch 23350\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:25] Epoch 10\\Batch 23400\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:30] Epoch 10\\Batch 23450\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:35] Epoch 10\\Batch 23500\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:39] Epoch 10\\Batch 23550\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:44] Epoch 10\\Batch 23600\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:48] Epoch 10\\Batch 23650\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:51] Epoch 10\\Batch 23700\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:54] Epoch 10\\Batch 23750\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:35:58] Epoch 10\\Batch 23800\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:01] Epoch 10\\Batch 23850\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:04] Epoch 10\\Batch 23900\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:07] Epoch 10\\Batch 23950\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:11] Epoch 10\\Batch 24000\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5419.000001, 'TP': 1580.0000009999999, 'FP': 1649.0000009999999}\n",
      "[2019/03/18 06:36:33] Epoch 10/ Validation Loss:9.705/ F1_score:0.309/ Precision:0.489/ Recall:0.226\n",
      "[2019/03/18 06:36:38] Epoch 10\\Batch 24050\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:43] Epoch 10\\Batch 24100\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:48] Epoch 10\\Batch 24150\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:53] Epoch 10\\Batch 24200\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:36:58] Epoch 10\\Batch 24250\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:03] Epoch 10\\Batch 24300\\ Train Loss:10.199\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:08] Epoch 10\\Batch 24350\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:13] Epoch 10\\Batch 24400\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:18] Epoch 10\\Batch 24450\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:23] Epoch 10\\Batch 24500\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:28] Epoch 10\\Batch 24550\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:33] Epoch 10\\Batch 24600\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:38] Epoch 10\\Batch 24650\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:43] Epoch 10\\Batch 24700\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:48] Epoch 10\\Batch 24750\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:53] Epoch 10\\Batch 24800\\ Train Loss:10.197\\ Learning rate:0.00030\n",
      "[2019/03/18 06:37:58] Epoch 10\\Batch 24850\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:02] Epoch 10\\Batch 24900\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:07] Epoch 10\\Batch 24950\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:12] Epoch 10\\Batch 25000\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:17] Epoch 10\\Batch 25050\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:22] Epoch 10\\Batch 25100\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:27] Epoch 10\\Batch 25150\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:32] Epoch 10\\Batch 25200\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:37] Epoch 10\\Batch 25250\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:42] Epoch 10\\Batch 25300\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:47] Epoch 10\\Batch 25350\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:52] Epoch 10\\Batch 25400\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 06:38:57] Epoch 10\\Batch 25450\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:02] Epoch 10\\Batch 25500\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:07] Epoch 10\\Batch 25550\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:11] Epoch 10\\Batch 25600\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:16] Epoch 10\\Batch 25650\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:21] Epoch 10\\Batch 25700\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:26] Epoch 10\\Batch 25750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:31] Epoch 10\\Batch 25800\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:36] Epoch 10\\Batch 25850\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:41] Epoch 10\\Batch 25900\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:46] Epoch 10\\Batch 25950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:51] Epoch 10\\Batch 26000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:39:55] Epoch 10\\Batch 26050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:00] Epoch 10\\Batch 26100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:05] Epoch 10\\Batch 26150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:10] Epoch 10\\Batch 26200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:15] Epoch 10\\Batch 26250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:20] Epoch 10\\Batch 26300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:25] Epoch 10\\Batch 26350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:29] Epoch 10\\Batch 26400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:34] Epoch 10\\Batch 26450\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:39] Epoch 10\\Batch 26500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:44] Epoch 10\\Batch 26550\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:49] Epoch 10\\Batch 26600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:54] Epoch 10\\Batch 26650\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:40:59] Epoch 10\\Batch 26700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:41:04] Epoch 10\\Batch 26750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:41:09] Epoch 10\\Batch 26800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:41:14] Epoch 10\\Batch 26850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:41:18] Epoch 10\\Batch 26900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:41:23] Epoch 10\\Batch 26950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:41:28] Epoch 10\\Batch 27000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5463.000001, 'TP': 1536.0000009999999, 'FP': 1650.0000009999999}\n",
      "[2019/03/18 06:41:51] Epoch 10/ Validation Loss:9.733/ F1_score:0.302/ Precision:0.482/ Recall:0.219\n",
      "[2019/03/18 06:41:56] Epoch 10\\Batch 27050\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:01] Epoch 10\\Batch 27100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:06] Epoch 10\\Batch 27150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:11] Epoch 10\\Batch 27200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:16] Epoch 10\\Batch 27250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:20] Epoch 10\\Batch 27300\\ Train Loss:10.194\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:42:25] Epoch 10\\Batch 27350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:30] Epoch 10\\Batch 27400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:35] Epoch 10\\Batch 27450\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:40] Epoch 10\\Batch 27500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:44] Epoch 10\\Batch 27550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:47] Epoch 10\\Batch 27600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:50] Epoch 10\\Batch 27650\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:53] Epoch 10\\Batch 27700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:42:56] Epoch 10\\Batch 27750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:00] Epoch 10\\Batch 27800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:03] Epoch 10\\Batch 27850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:07] Epoch 10\\Batch 27900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:12] Epoch 10\\Batch 27950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:17] Epoch 10\\Batch 28000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:22] Epoch 10\\Batch 28050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:27] Epoch 10\\Batch 28100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:32] Epoch 10\\Batch 28150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:36] Epoch 10\\Batch 28200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:41] Epoch 10\\Batch 28250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:46] Epoch 10\\Batch 28300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:51] Epoch 10\\Batch 28350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:43:56] Epoch 10\\Batch 28400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:01] Epoch 10\\Batch 28450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:06] Epoch 10\\Batch 28500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:11] Epoch 10\\Batch 28550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:16] Epoch 10\\Batch 28600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:21] Epoch 10\\Batch 28650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:26] Epoch 10\\Batch 28700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:31] Epoch 10\\Batch 28750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:36] Epoch 10\\Batch 28800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:41] Epoch 10\\Batch 28850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:46] Epoch 10\\Batch 28900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:51] Epoch 10\\Batch 28950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:44:56] Epoch 10\\Batch 29000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:01] Epoch 10\\Batch 29050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:06] Epoch 10\\Batch 29100\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:11] Epoch 10\\Batch 29150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:16] Epoch 10\\Batch 29200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:21] Epoch 10\\Batch 29250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:25] Epoch 10\\Batch 29300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:30] Epoch 10\\Batch 29350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:35] Epoch 10\\Batch 29400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:40] Epoch 10\\Batch 29450\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:45] Epoch 10\\Batch 29500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:50] Epoch 10\\Batch 29550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:45:55] Epoch 10\\Batch 29600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:00] Epoch 10\\Batch 29650\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:05] Epoch 10\\Batch 29700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:09] Epoch 10\\Batch 29750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:14] Epoch 10\\Batch 29800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:19] Epoch 10\\Batch 29850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:24] Epoch 10\\Batch 29900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:29] Epoch 10\\Batch 29950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:46:34] Epoch 10\\Batch 30000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5449.000001, 'TP': 1550.0000009999999, 'FP': 1638.0000009999999}\n",
      "[2019/03/18 06:46:57] Epoch 10/ Validation Loss:9.709/ F1_score:0.304/ Precision:0.486/ Recall:0.221\n",
      "[2019/03/18 06:47:02] Epoch 10\\Batch 30050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:07] Epoch 10\\Batch 30100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:11] Epoch 10\\Batch 30150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:16] Epoch 10\\Batch 30200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:21] Epoch 10\\Batch 30250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:26] Epoch 10\\Batch 30300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:31] Epoch 10\\Batch 30350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:36] Epoch 10\\Batch 30400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:41] Epoch 10\\Batch 30450\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:46] Epoch 10\\Batch 30500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:51] Epoch 10\\Batch 30550\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:47:56] Epoch 10\\Batch 30600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:01] Epoch 10\\Batch 30650\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:05] Epoch 10\\Batch 30700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:10] Epoch 10\\Batch 30750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:15] Epoch 10\\Batch 30800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:20] Epoch 10\\Batch 30850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:25] Epoch 10\\Batch 30900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:30] Epoch 10\\Batch 30950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:35] Epoch 10\\Batch 31000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:40] Epoch 10\\Batch 31050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:45] Epoch 10\\Batch 31100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:50] Epoch 10\\Batch 31150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:48:55] Epoch 10\\Batch 31200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:00] Epoch 10\\Batch 31250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:05] Epoch 10\\Batch 31300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:09] Epoch 10\\Batch 31350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:14] Epoch 10\\Batch 31400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:19] Epoch 10\\Batch 31450\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:24] Epoch 10\\Batch 31500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:29] Epoch 10\\Batch 31550\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:34] Epoch 10\\Batch 31600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:39] Epoch 10\\Batch 31650\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:44] Epoch 10\\Batch 31700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:48] Epoch 10\\Batch 31750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:51] Epoch 10\\Batch 31800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:55] Epoch 10\\Batch 31850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:49:58] Epoch 10\\Batch 31900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:01] Epoch 10\\Batch 31950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:04] Epoch 10\\Batch 32000\\ Train Loss:10.194\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:50:07] Epoch 10\\Batch 32050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:11] Epoch 10\\Batch 32100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:16] Epoch 10\\Batch 32150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:21] Epoch 10\\Batch 32200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:25] Epoch 10\\Batch 32250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:30] Epoch 10\\Batch 32300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:35] Epoch 10\\Batch 32350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:40] Epoch 10\\Batch 32400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:45] Epoch 10\\Batch 32450\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:50] Epoch 10\\Batch 32500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:50:55] Epoch 10\\Batch 32550\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:00] Epoch 10\\Batch 32600\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:05] Epoch 10\\Batch 32650\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:10] Epoch 10\\Batch 32700\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:15] Epoch 10\\Batch 32750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:20] Epoch 10\\Batch 32800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:25] Epoch 10\\Batch 32850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:30] Epoch 10\\Batch 32900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:35] Epoch 10\\Batch 32950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:51:39] Epoch 10\\Batch 33000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5455.000001, 'TP': 1544.0000009999999, 'FP': 1669.0000009999999}\n",
      "[2019/03/18 06:52:02] Epoch 10/ Validation Loss:9.720/ F1_score:0.302/ Precision:0.481/ Recall:0.221\n",
      "[2019/03/18 06:52:07] Epoch 10\\Batch 33050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:12] Epoch 10\\Batch 33100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:17] Epoch 10\\Batch 33150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:22] Epoch 10\\Batch 33200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:27] Epoch 10\\Batch 33250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:32] Epoch 10\\Batch 33300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:37] Epoch 10\\Batch 33350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:42] Epoch 10\\Batch 33400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:46] Epoch 10\\Batch 33450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:51] Epoch 10\\Batch 33500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:52:56] Epoch 10\\Batch 33550\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:01] Epoch 10\\Batch 33600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:06] Epoch 10\\Batch 33650\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:11] Epoch 10\\Batch 33700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:16] Epoch 10\\Batch 33750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:21] Epoch 10\\Batch 33800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:26] Epoch 10\\Batch 33850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:31] Epoch 10\\Batch 33900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:36] Epoch 10\\Batch 33950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:41] Epoch 10\\Batch 34000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:46] Epoch 10\\Batch 34050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:51] Epoch 10\\Batch 34100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:53:56] Epoch 10\\Batch 34150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:01] Epoch 10\\Batch 34200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:06] Epoch 10\\Batch 34250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:11] Epoch 10\\Batch 34300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:15] Epoch 10\\Batch 34350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:20] Epoch 10\\Batch 34400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:25] Epoch 10\\Batch 34450\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:30] Epoch 10\\Batch 34500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:35] Epoch 10\\Batch 34550\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:40] Epoch 10\\Batch 34600\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:45] Epoch 10\\Batch 34650\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:50] Epoch 10\\Batch 34700\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:54:55] Epoch 10\\Batch 34750\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:00] Epoch 10\\Batch 34800\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:05] Epoch 10\\Batch 34850\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:10] Epoch 10\\Batch 34900\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:15] Epoch 10\\Batch 34950\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:20] Epoch 10\\Batch 35000\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:25] Epoch 10\\Batch 35050\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:30] Epoch 10\\Batch 35100\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:35] Epoch 10\\Batch 35150\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:40] Epoch 10\\Batch 35200\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:45] Epoch 10\\Batch 35250\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:50] Epoch 10\\Batch 35300\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:55:55] Epoch 10\\Batch 35350\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:00] Epoch 10\\Batch 35400\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:05] Epoch 10\\Batch 35450\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:10] Epoch 10\\Batch 35500\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:14] Epoch 10\\Batch 35550\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:19] Epoch 10\\Batch 35600\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:24] Epoch 10\\Batch 35650\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:29] Epoch 10\\Batch 35700\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:34] Epoch 10\\Batch 35750\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:39] Epoch 10\\Batch 35800\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:44] Epoch 10\\Batch 35850\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:49] Epoch 10\\Batch 35900\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:54] Epoch 10\\Batch 35950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:56:58] Epoch 10\\Batch 36000\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5440.000001, 'TP': 1559.0000009999999, 'FP': 1639.0000009999999}\n",
      "[2019/03/18 06:57:15] Epoch 10/ Validation Loss:9.746/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 06:57:20] Epoch 10\\Batch 36050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:25] Epoch 10\\Batch 36100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:30] Epoch 10\\Batch 36150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:34] Epoch 10\\Batch 36200\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:39] Epoch 10\\Batch 36250\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:44] Epoch 10\\Batch 36300\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:49] Epoch 10\\Batch 36350\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:54] Epoch 10\\Batch 36400\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 06:57:59] Epoch 10\\Batch 36450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:04] Epoch 10\\Batch 36500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:09] Epoch 10\\Batch 36550\\ Train Loss:10.193\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 06:58:13] Epoch 10\\Batch 36600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:18] Epoch 10\\Batch 36650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:23] Epoch 10\\Batch 36700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:28] Epoch 10\\Batch 36750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:34] Epoch 10\\Batch 36800\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:39] Epoch 10\\Batch 36850\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:43] Epoch 10\\Batch 36900\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:49] Epoch 10\\Batch 36950\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:53] Epoch 10\\Batch 37000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:58:58] Epoch 10\\Batch 37050\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:03] Epoch 10\\Batch 37100\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:08] Epoch 10\\Batch 37150\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:13] Epoch 10\\Batch 37200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:18] Epoch 10\\Batch 37250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:23] Epoch 10\\Batch 37300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:28] Epoch 10\\Batch 37350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:33] Epoch 10\\Batch 37400\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:38] Epoch 10\\Batch 37450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:43] Epoch 10\\Batch 37500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:48] Epoch 10\\Batch 37550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:53] Epoch 10\\Batch 37600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 06:59:58] Epoch 10\\Batch 37650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:03] Epoch 10\\Batch 37700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:08] Epoch 10\\Batch 37750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:13] Epoch 10\\Batch 37800\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:18] Epoch 10\\Batch 37850\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:23] Epoch 10\\Batch 37900\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:28] Epoch 10\\Batch 37950\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:33] Epoch 10\\Batch 38000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:38] Epoch 10\\Batch 38050\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:43] Epoch 10\\Batch 38100\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:48] Epoch 10\\Batch 38150\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:53] Epoch 10\\Batch 38200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:00:58] Epoch 10\\Batch 38250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:03] Epoch 10\\Batch 38300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:07] Epoch 10\\Batch 38350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:12] Epoch 10\\Batch 38400\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:17] Epoch 10\\Batch 38450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:22] Epoch 10\\Batch 38500\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:27] Epoch 10\\Batch 38550\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:32] Epoch 10\\Batch 38600\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:37] Epoch 10\\Batch 38650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:42] Epoch 10\\Batch 38700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:47] Epoch 10\\Batch 38750\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:52] Epoch 10\\Batch 38800\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:01:57] Epoch 10\\Batch 38850\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:02] Epoch 10\\Batch 38900\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:07] Epoch 10\\Batch 38950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:12] Epoch 10\\Batch 39000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5407.000001, 'TP': 1592.0000009999999, 'FP': 1617.0000009999999}\n",
      "[2019/03/18 07:02:34] Epoch 10/ Validation Loss:9.746/ F1_score:0.312/ Precision:0.496/ Recall:0.227\n",
      "[2019/03/18 07:02:39] Epoch 10\\Batch 39050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:44] Epoch 10\\Batch 39100\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:49] Epoch 10\\Batch 39150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:54] Epoch 10\\Batch 39200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:02:59] Epoch 10\\Batch 39250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:04] Epoch 10\\Batch 39300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:09] Epoch 10\\Batch 39350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:14] Epoch 10\\Batch 39400\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:19] Epoch 10\\Batch 39450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:24] Epoch 10\\Batch 39500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:29] Epoch 10\\Batch 39550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:34] Epoch 10\\Batch 39600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:39] Epoch 10\\Batch 39650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:44] Epoch 10\\Batch 39700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:49] Epoch 10\\Batch 39750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:54] Epoch 10\\Batch 39800\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:03:59] Epoch 10\\Batch 39850\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:02] Epoch 10\\Batch 39900\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:06] Epoch 10\\Batch 39950\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:09] Epoch 10\\Batch 40000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:12] Epoch 10\\Batch 40050\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:16] Epoch 10\\Batch 40100\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:19] Epoch 10\\Batch 40150\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:22] Epoch 10\\Batch 40200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:26] Epoch 10\\Batch 40250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:31] Epoch 10\\Batch 40300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:36] Epoch 10\\Batch 40350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:41] Epoch 10\\Batch 40400\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:46] Epoch 10\\Batch 40450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:51] Epoch 10\\Batch 40500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:04:56] Epoch 10\\Batch 40550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:01] Epoch 10\\Batch 40600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:06] Epoch 10\\Batch 40650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:11] Epoch 10\\Batch 40700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:16] Epoch 10\\Batch 40750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:21] Epoch 10\\Batch 40800\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:26] Epoch 10\\Batch 40850\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:31] Epoch 10\\Batch 40900\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:36] Epoch 10\\Batch 40950\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:40] Epoch 10\\Batch 41000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:45] Epoch 10\\Batch 41050\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:50] Epoch 10\\Batch 41100\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:05:55] Epoch 10\\Batch 41150\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:00] Epoch 10\\Batch 41200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:05] Epoch 10\\Batch 41250\\ Train Loss:10.193\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:06:10] Epoch 10\\Batch 41300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:15] Epoch 10\\Batch 41350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:20] Epoch 10\\Batch 41400\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:25] Epoch 10\\Batch 41450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:30] Epoch 10\\Batch 41500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:35] Epoch 10\\Batch 41550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:40] Epoch 10\\Batch 41600\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:45] Epoch 10\\Batch 41650\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:50] Epoch 10\\Batch 41700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:06:55] Epoch 10\\Batch 41750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:00] Epoch 10\\Batch 41800\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:05] Epoch 10\\Batch 41850\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:10] Epoch 10\\Batch 41900\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:14] Epoch 10\\Batch 41950\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:19] Epoch 10\\Batch 42000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5422.000001, 'TP': 1577.0000009999999, 'FP': 1616.0000009999999}\n",
      "[2019/03/18 07:07:42] Epoch 10/ Validation Loss:9.725/ F1_score:0.309/ Precision:0.494/ Recall:0.225\n",
      "[2019/03/18 07:07:47] Epoch 10\\Batch 42050\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:52] Epoch 10\\Batch 42100\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:07:57] Epoch 10\\Batch 42150\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:02] Epoch 10\\Batch 42200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:07] Epoch 10\\Batch 42250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:12] Epoch 10\\Batch 42300\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:16] Epoch 10\\Batch 42350\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:21] Epoch 10\\Batch 42400\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:26] Epoch 10\\Batch 42450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:31] Epoch 10\\Batch 42500\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:36] Epoch 10\\Batch 42550\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:42] Epoch 10\\Batch 42600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:47] Epoch 10\\Batch 42650\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:52] Epoch 10\\Batch 42700\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:08:56] Epoch 10\\Batch 42750\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:01] Epoch 10\\Batch 42800\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:06] Epoch 10\\Batch 42850\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:11] Epoch 10\\Batch 42900\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:16] Epoch 10\\Batch 42950\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:21] Epoch 10\\Batch 43000\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:26] Epoch 10\\Batch 43050\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:31] Epoch 10\\Batch 43100\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:36] Epoch 10\\Batch 43150\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:41] Epoch 10\\Batch 43200\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:45] Epoch 10\\Batch 43250\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:50] Epoch 10\\Batch 43300\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:09:55] Epoch 10\\Batch 43350\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:00] Epoch 10\\Batch 43400\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:05] Epoch 10\\Batch 43450\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:10] Epoch 10\\Batch 43500\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:15] Epoch 10\\Batch 43550\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:20] Epoch 10\\Batch 43600\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:25] Epoch 10\\Batch 43650\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:30] Epoch 10\\Batch 43700\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:34] Epoch 10\\Batch 43750\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:39] Epoch 10\\Batch 43800\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:44] Epoch 10\\Batch 43850\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:49] Epoch 10\\Batch 43900\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:54] Epoch 10\\Batch 43950\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:10:59] Epoch 10\\Batch 44000\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:04] Epoch 10\\Batch 44050\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:08] Epoch 10\\Batch 44100\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:11] Epoch 10\\Batch 44150\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:14] Epoch 10\\Batch 44200\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:18] Epoch 10\\Batch 44250\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:21] Epoch 10\\Batch 44300\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:24] Epoch 10\\Batch 44350\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:27] Epoch 10\\Batch 44400\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:30] Epoch 10\\Batch 44450\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:35] Epoch 10\\Batch 44500\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:39] Epoch 10\\Batch 44550\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:44] Epoch 10\\Batch 44600\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:49] Epoch 10\\Batch 44650\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:54] Epoch 10\\Batch 44700\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:11:59] Epoch 10\\Batch 44750\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:12:04] Epoch 10\\Batch 44800\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:12:09] Epoch 10\\Batch 44850\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:12:13] Epoch 10\\Batch 44900\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:12:18] Epoch 10\\Batch 44950\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:12:23] Epoch 10\\Batch 45000\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1645.0000009999999}\n",
      "[2019/03/18 07:12:46] Epoch 10/ Validation Loss:9.751/ F1_score:0.303/ Precision:0.484/ Recall:0.220\n",
      "[2019/03/18 07:12:51] Epoch 10\\Batch 45050\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:12:56] Epoch 10\\Batch 45100\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:01] Epoch 10\\Batch 45150\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:06] Epoch 10\\Batch 45200\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:11] Epoch 10\\Batch 45250\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:16] Epoch 10\\Batch 45300\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:21] Epoch 10\\Batch 45350\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:26] Epoch 10\\Batch 45400\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:31] Epoch 10\\Batch 45450\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:36] Epoch 10\\Batch 45500\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:40] Epoch 10\\Batch 45550\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:45] Epoch 10\\Batch 45600\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:50] Epoch 10\\Batch 45650\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:13:55] Epoch 10\\Batch 45700\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:00] Epoch 10\\Batch 45750\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:05] Epoch 10\\Batch 45800\\ Train Loss:10.191\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:14:10] Epoch 10\\Batch 45850\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:15] Epoch 10\\Batch 45900\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:20] Epoch 10\\Batch 45950\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:24] Epoch 10\\Batch 46000\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:29] Epoch 10\\Batch 46050\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:34] Epoch 10\\Batch 46100\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:39] Epoch 10\\Batch 46150\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:44] Epoch 10\\Batch 46200\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:54] Epoch 11\\Batch 50\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:14:59] Epoch 11\\Batch 100\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:04] Epoch 11\\Batch 150\\ Train Loss:10.297\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:09] Epoch 11\\Batch 200\\ Train Loss:10.252\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:14] Epoch 11\\Batch 250\\ Train Loss:10.241\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:19] Epoch 11\\Batch 300\\ Train Loss:10.254\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:24] Epoch 11\\Batch 350\\ Train Loss:10.258\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:29] Epoch 11\\Batch 400\\ Train Loss:10.259\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:34] Epoch 11\\Batch 450\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:39] Epoch 11\\Batch 500\\ Train Loss:10.229\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:44] Epoch 11\\Batch 550\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:49] Epoch 11\\Batch 600\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:54] Epoch 11\\Batch 650\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 07:15:59] Epoch 11\\Batch 700\\ Train Loss:10.213\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:04] Epoch 11\\Batch 750\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:09] Epoch 11\\Batch 800\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:14] Epoch 11\\Batch 850\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:19] Epoch 11\\Batch 900\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:24] Epoch 11\\Batch 950\\ Train Loss:10.202\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:29] Epoch 11\\Batch 1000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:34] Epoch 11\\Batch 1050\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:39] Epoch 11\\Batch 1100\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:44] Epoch 11\\Batch 1150\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:49] Epoch 11\\Batch 1200\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:54] Epoch 11\\Batch 1250\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:16:59] Epoch 11\\Batch 1300\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:04] Epoch 11\\Batch 1350\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:09] Epoch 11\\Batch 1400\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:14] Epoch 11\\Batch 1450\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:19] Epoch 11\\Batch 1500\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:23] Epoch 11\\Batch 1550\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:28] Epoch 11\\Batch 1600\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:33] Epoch 11\\Batch 1650\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:38] Epoch 11\\Batch 1700\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:43] Epoch 11\\Batch 1750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:48] Epoch 11\\Batch 1800\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:53] Epoch 11\\Batch 1850\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 07:17:58] Epoch 11\\Batch 1900\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:03] Epoch 11\\Batch 1950\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:08] Epoch 11\\Batch 2000\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:13] Epoch 11\\Batch 2050\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:16] Epoch 11\\Batch 2100\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:20] Epoch 11\\Batch 2150\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:23] Epoch 11\\Batch 2200\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:26] Epoch 11\\Batch 2250\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:30] Epoch 11\\Batch 2300\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:33] Epoch 11\\Batch 2350\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:37] Epoch 11\\Batch 2400\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:41] Epoch 11\\Batch 2450\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:46] Epoch 11\\Batch 2500\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:51] Epoch 11\\Batch 2550\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:18:56] Epoch 11\\Batch 2600\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:01] Epoch 11\\Batch 2650\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:06] Epoch 11\\Batch 2700\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:11] Epoch 11\\Batch 2750\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:16] Epoch 11\\Batch 2800\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:21] Epoch 11\\Batch 2850\\ Train Loss:10.195\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:26] Epoch 11\\Batch 2900\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:30] Epoch 11\\Batch 2950\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:19:35] Epoch 11\\Batch 3000\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5460.000001, 'TP': 1539.0000009999999, 'FP': 1653.0000009999999}\n",
      "[2019/03/18 07:19:58] Epoch 11/ Validation Loss:9.738/ F1_score:0.302/ Precision:0.482/ Recall:0.220\n",
      "[2019/03/18 07:20:03] Epoch 11\\Batch 3050\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:08] Epoch 11\\Batch 3100\\ Train Loss:10.196\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:13] Epoch 11\\Batch 3150\\ Train Loss:10.194\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:18] Epoch 11\\Batch 3200\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:23] Epoch 11\\Batch 3250\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:28] Epoch 11\\Batch 3300\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:33] Epoch 11\\Batch 3350\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:38] Epoch 11\\Batch 3400\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:43] Epoch 11\\Batch 3450\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:48] Epoch 11\\Batch 3500\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:53] Epoch 11\\Batch 3550\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:20:57] Epoch 11\\Batch 3600\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:02] Epoch 11\\Batch 3650\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:07] Epoch 11\\Batch 3700\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:12] Epoch 11\\Batch 3750\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:17] Epoch 11\\Batch 3800\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:22] Epoch 11\\Batch 3850\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:27] Epoch 11\\Batch 3900\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:32] Epoch 11\\Batch 3950\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:37] Epoch 11\\Batch 4000\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:42] Epoch 11\\Batch 4050\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:47] Epoch 11\\Batch 4100\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:52] Epoch 11\\Batch 4150\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:21:57] Epoch 11\\Batch 4200\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:02] Epoch 11\\Batch 4250\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:07] Epoch 11\\Batch 4300\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:12] Epoch 11\\Batch 4350\\ Train Loss:10.185\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:22:16] Epoch 11\\Batch 4400\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:22] Epoch 11\\Batch 4450\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:27] Epoch 11\\Batch 4500\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:32] Epoch 11\\Batch 4550\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:37] Epoch 11\\Batch 4600\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:42] Epoch 11\\Batch 4650\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:46] Epoch 11\\Batch 4700\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:51] Epoch 11\\Batch 4750\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:22:56] Epoch 11\\Batch 4800\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:01] Epoch 11\\Batch 4850\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:06] Epoch 11\\Batch 4900\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:11] Epoch 11\\Batch 4950\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:16] Epoch 11\\Batch 5000\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:21] Epoch 11\\Batch 5050\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:26] Epoch 11\\Batch 5100\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:31] Epoch 11\\Batch 5150\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:36] Epoch 11\\Batch 5200\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:40] Epoch 11\\Batch 5250\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:45] Epoch 11\\Batch 5300\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:50] Epoch 11\\Batch 5350\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:23:55] Epoch 11\\Batch 5400\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:00] Epoch 11\\Batch 5450\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:05] Epoch 11\\Batch 5500\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:10] Epoch 11\\Batch 5550\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:15] Epoch 11\\Batch 5600\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:20] Epoch 11\\Batch 5650\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:25] Epoch 11\\Batch 5700\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:29] Epoch 11\\Batch 5750\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:34] Epoch 11\\Batch 5800\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:39] Epoch 11\\Batch 5850\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:44] Epoch 11\\Batch 5900\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:49] Epoch 11\\Batch 5950\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:24:54] Epoch 11\\Batch 6000\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5432.000001, 'TP': 1567.0000009999999, 'FP': 1627.0000009999999}\n",
      "[2019/03/18 07:25:16] Epoch 11/ Validation Loss:9.729/ F1_score:0.307/ Precision:0.491/ Recall:0.224\n",
      "[2019/03/18 07:25:19] Epoch 11\\Batch 6050\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:22] Epoch 11\\Batch 6100\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:25] Epoch 11\\Batch 6150\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:29] Epoch 11\\Batch 6200\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:32] Epoch 11\\Batch 6250\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:35] Epoch 11\\Batch 6300\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:40] Epoch 11\\Batch 6350\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:44] Epoch 11\\Batch 6400\\ Train Loss:10.192\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:49] Epoch 11\\Batch 6450\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:54] Epoch 11\\Batch 6500\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:25:59] Epoch 11\\Batch 6550\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:04] Epoch 11\\Batch 6600\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:09] Epoch 11\\Batch 6650\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:14] Epoch 11\\Batch 6700\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:19] Epoch 11\\Batch 6750\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:24] Epoch 11\\Batch 6800\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:29] Epoch 11\\Batch 6850\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:34] Epoch 11\\Batch 6900\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:39] Epoch 11\\Batch 6950\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:44] Epoch 11\\Batch 7000\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:48] Epoch 11\\Batch 7050\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:53] Epoch 11\\Batch 7100\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:26:58] Epoch 11\\Batch 7150\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:03] Epoch 11\\Batch 7200\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:08] Epoch 11\\Batch 7250\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:13] Epoch 11\\Batch 7300\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:18] Epoch 11\\Batch 7350\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:23] Epoch 11\\Batch 7400\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:28] Epoch 11\\Batch 7450\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:33] Epoch 11\\Batch 7500\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:38] Epoch 11\\Batch 7550\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:42] Epoch 11\\Batch 7600\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:47] Epoch 11\\Batch 7650\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:52] Epoch 11\\Batch 7700\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:27:57] Epoch 11\\Batch 7750\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:02] Epoch 11\\Batch 7800\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:07] Epoch 11\\Batch 7850\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:12] Epoch 11\\Batch 7900\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:17] Epoch 11\\Batch 7950\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:22] Epoch 11\\Batch 8000\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:27] Epoch 11\\Batch 8050\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:32] Epoch 11\\Batch 8100\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:37] Epoch 11\\Batch 8150\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:42] Epoch 11\\Batch 8200\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:47] Epoch 11\\Batch 8250\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:52] Epoch 11\\Batch 8300\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:28:56] Epoch 11\\Batch 8350\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:01] Epoch 11\\Batch 8400\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:07] Epoch 11\\Batch 8450\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:12] Epoch 11\\Batch 8500\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:17] Epoch 11\\Batch 8550\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:22] Epoch 11\\Batch 8600\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:27] Epoch 11\\Batch 8650\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:31] Epoch 11\\Batch 8700\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:36] Epoch 11\\Batch 8750\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:41] Epoch 11\\Batch 8800\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:46] Epoch 11\\Batch 8850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:51] Epoch 11\\Batch 8900\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:29:56] Epoch 11\\Batch 8950\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:01] Epoch 11\\Batch 9000\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5443.000001, 'TP': 1556.0000009999999, 'FP': 1654.0000009999999}\n",
      "[2019/03/18 07:30:24] Epoch 11/ Validation Loss:9.721/ F1_score:0.305/ Precision:0.485/ Recall:0.222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:30:29] Epoch 11\\Batch 9050\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:34] Epoch 11\\Batch 9100\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:39] Epoch 11\\Batch 9150\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:44] Epoch 11\\Batch 9200\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:49] Epoch 11\\Batch 9250\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:54] Epoch 11\\Batch 9300\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:30:58] Epoch 11\\Batch 9350\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:03] Epoch 11\\Batch 9400\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:08] Epoch 11\\Batch 9450\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:13] Epoch 11\\Batch 9500\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:18] Epoch 11\\Batch 9550\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:23] Epoch 11\\Batch 9600\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:28] Epoch 11\\Batch 9650\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:32] Epoch 11\\Batch 9700\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:37] Epoch 11\\Batch 9750\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:42] Epoch 11\\Batch 9800\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:47] Epoch 11\\Batch 9850\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:52] Epoch 11\\Batch 9900\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:31:57] Epoch 11\\Batch 9950\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:02] Epoch 11\\Batch 10000\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:07] Epoch 11\\Batch 10050\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:12] Epoch 11\\Batch 10100\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:17] Epoch 11\\Batch 10150\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:21] Epoch 11\\Batch 10200\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:24] Epoch 11\\Batch 10250\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:27] Epoch 11\\Batch 10300\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:30] Epoch 11\\Batch 10350\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:34] Epoch 11\\Batch 10400\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:37] Epoch 11\\Batch 10450\\ Train Loss:10.191\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:40] Epoch 11\\Batch 10500\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:43] Epoch 11\\Batch 10550\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:48] Epoch 11\\Batch 10600\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:53] Epoch 11\\Batch 10650\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:32:58] Epoch 11\\Batch 10700\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:03] Epoch 11\\Batch 10750\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:08] Epoch 11\\Batch 10800\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:13] Epoch 11\\Batch 10850\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:18] Epoch 11\\Batch 10900\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:23] Epoch 11\\Batch 10950\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:28] Epoch 11\\Batch 11000\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:33] Epoch 11\\Batch 11050\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:37] Epoch 11\\Batch 11100\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:42] Epoch 11\\Batch 11150\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:47] Epoch 11\\Batch 11200\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:52] Epoch 11\\Batch 11250\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:33:57] Epoch 11\\Batch 11300\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:02] Epoch 11\\Batch 11350\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:07] Epoch 11\\Batch 11400\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:12] Epoch 11\\Batch 11450\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:17] Epoch 11\\Batch 11500\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:22] Epoch 11\\Batch 11550\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:27] Epoch 11\\Batch 11600\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:31] Epoch 11\\Batch 11650\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:36] Epoch 11\\Batch 11700\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:41] Epoch 11\\Batch 11750\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:46] Epoch 11\\Batch 11800\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:51] Epoch 11\\Batch 11850\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:34:56] Epoch 11\\Batch 11900\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:01] Epoch 11\\Batch 11950\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:06] Epoch 11\\Batch 12000\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5432.000001, 'TP': 1567.0000009999999, 'FP': 1633.0000009999999}\n",
      "[2019/03/18 07:35:29] Epoch 11/ Validation Loss:9.719/ F1_score:0.307/ Precision:0.490/ Recall:0.224\n",
      "[2019/03/18 07:35:34] Epoch 11\\Batch 12050\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:39] Epoch 11\\Batch 12100\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:44] Epoch 11\\Batch 12150\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:49] Epoch 11\\Batch 12200\\ Train Loss:10.190\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:54] Epoch 11\\Batch 12250\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:35:59] Epoch 11\\Batch 12300\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:04] Epoch 11\\Batch 12350\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:08] Epoch 11\\Batch 12400\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:14] Epoch 11\\Batch 12450\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:18] Epoch 11\\Batch 12500\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:23] Epoch 11\\Batch 12550\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:28] Epoch 11\\Batch 12600\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:33] Epoch 11\\Batch 12650\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:38] Epoch 11\\Batch 12700\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:43] Epoch 11\\Batch 12750\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:48] Epoch 11\\Batch 12800\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:53] Epoch 11\\Batch 12850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:36:58] Epoch 11\\Batch 12900\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:03] Epoch 11\\Batch 12950\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:08] Epoch 11\\Batch 13000\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:13] Epoch 11\\Batch 13050\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:18] Epoch 11\\Batch 13100\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:23] Epoch 11\\Batch 13150\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:28] Epoch 11\\Batch 13200\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:33] Epoch 11\\Batch 13250\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:38] Epoch 11\\Batch 13300\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:43] Epoch 11\\Batch 13350\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:48] Epoch 11\\Batch 13400\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:53] Epoch 11\\Batch 13450\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:37:58] Epoch 11\\Batch 13500\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:03] Epoch 11\\Batch 13550\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:07] Epoch 11\\Batch 13600\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:12] Epoch 11\\Batch 13650\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:17] Epoch 11\\Batch 13700\\ Train Loss:10.187\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:38:22] Epoch 11\\Batch 13750\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:27] Epoch 11\\Batch 13800\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:32] Epoch 11\\Batch 13850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:37] Epoch 11\\Batch 13900\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:42] Epoch 11\\Batch 13950\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:47] Epoch 11\\Batch 14000\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:52] Epoch 11\\Batch 14050\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:38:57] Epoch 11\\Batch 14100\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:02] Epoch 11\\Batch 14150\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:07] Epoch 11\\Batch 14200\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:12] Epoch 11\\Batch 14250\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:17] Epoch 11\\Batch 14300\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:21] Epoch 11\\Batch 14350\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:26] Epoch 11\\Batch 14400\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:30] Epoch 11\\Batch 14450\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:33] Epoch 11\\Batch 14500\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:37] Epoch 11\\Batch 14550\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:40] Epoch 11\\Batch 14600\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:43] Epoch 11\\Batch 14650\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:46] Epoch 11\\Batch 14700\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:50] Epoch 11\\Batch 14750\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:53] Epoch 11\\Batch 14800\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:39:58] Epoch 11\\Batch 14850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:40:03] Epoch 11\\Batch 14900\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:40:08] Epoch 11\\Batch 14950\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:40:13] Epoch 11\\Batch 15000\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5450.000001, 'TP': 1549.0000009999999, 'FP': 1642.0000009999999}\n",
      "[2019/03/18 07:40:35] Epoch 11/ Validation Loss:9.720/ F1_score:0.304/ Precision:0.485/ Recall:0.221\n",
      "[2019/03/18 07:40:41] Epoch 11\\Batch 15050\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:40:46] Epoch 11\\Batch 15100\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:40:51] Epoch 11\\Batch 15150\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:40:55] Epoch 11\\Batch 15200\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:00] Epoch 11\\Batch 15250\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:05] Epoch 11\\Batch 15300\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:10] Epoch 11\\Batch 15350\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:15] Epoch 11\\Batch 15400\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:20] Epoch 11\\Batch 15450\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:25] Epoch 11\\Batch 15500\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:30] Epoch 11\\Batch 15550\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:35] Epoch 11\\Batch 15600\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:40] Epoch 11\\Batch 15650\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:45] Epoch 11\\Batch 15700\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:50] Epoch 11\\Batch 15750\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:41:55] Epoch 11\\Batch 15800\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:00] Epoch 11\\Batch 15850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:05] Epoch 11\\Batch 15900\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:10] Epoch 11\\Batch 15950\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:15] Epoch 11\\Batch 16000\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:20] Epoch 11\\Batch 16050\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:25] Epoch 11\\Batch 16100\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:29] Epoch 11\\Batch 16150\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:35] Epoch 11\\Batch 16200\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:40] Epoch 11\\Batch 16250\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:45] Epoch 11\\Batch 16300\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:49] Epoch 11\\Batch 16350\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:55] Epoch 11\\Batch 16400\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:42:59] Epoch 11\\Batch 16450\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:04] Epoch 11\\Batch 16500\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:09] Epoch 11\\Batch 16550\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:14] Epoch 11\\Batch 16600\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:19] Epoch 11\\Batch 16650\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:24] Epoch 11\\Batch 16700\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:29] Epoch 11\\Batch 16750\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:34] Epoch 11\\Batch 16800\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:39] Epoch 11\\Batch 16850\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:44] Epoch 11\\Batch 16900\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:49] Epoch 11\\Batch 16950\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:54] Epoch 11\\Batch 17000\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:43:58] Epoch 11\\Batch 17050\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:03] Epoch 11\\Batch 17100\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:08] Epoch 11\\Batch 17150\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:13] Epoch 11\\Batch 17200\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:18] Epoch 11\\Batch 17250\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:23] Epoch 11\\Batch 17300\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:28] Epoch 11\\Batch 17350\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:33] Epoch 11\\Batch 17400\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:38] Epoch 11\\Batch 17450\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:43] Epoch 11\\Batch 17500\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:47] Epoch 11\\Batch 17550\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:52] Epoch 11\\Batch 17600\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:44:57] Epoch 11\\Batch 17650\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:02] Epoch 11\\Batch 17700\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:07] Epoch 11\\Batch 17750\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:12] Epoch 11\\Batch 17800\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:17] Epoch 11\\Batch 17850\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:22] Epoch 11\\Batch 17900\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:27] Epoch 11\\Batch 17950\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:45:32] Epoch 11\\Batch 18000\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5454.000001, 'TP': 1545.0000009999999, 'FP': 1652.0000009999999}\n",
      "[2019/03/18 07:45:54] Epoch 11/ Validation Loss:9.725/ F1_score:0.303/ Precision:0.483/ Recall:0.221\n",
      "[2019/03/18 07:45:59] Epoch 11\\Batch 18050\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:04] Epoch 11\\Batch 18100\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:09] Epoch 11\\Batch 18150\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:14] Epoch 11\\Batch 18200\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:19] Epoch 11\\Batch 18250\\ Train Loss:10.183\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:46:24] Epoch 11\\Batch 18300\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:27] Epoch 11\\Batch 18350\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:30] Epoch 11\\Batch 18400\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:33] Epoch 11\\Batch 18450\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:36] Epoch 11\\Batch 18500\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:40] Epoch 11\\Batch 18550\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:43] Epoch 11\\Batch 18600\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:46] Epoch 11\\Batch 18650\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:50] Epoch 11\\Batch 18700\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:46:55] Epoch 11\\Batch 18750\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:00] Epoch 11\\Batch 18800\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:05] Epoch 11\\Batch 18850\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:10] Epoch 11\\Batch 18900\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:15] Epoch 11\\Batch 18950\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:20] Epoch 11\\Batch 19000\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:25] Epoch 11\\Batch 19050\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:30] Epoch 11\\Batch 19100\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:35] Epoch 11\\Batch 19150\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:40] Epoch 11\\Batch 19200\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:45] Epoch 11\\Batch 19250\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:50] Epoch 11\\Batch 19300\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:55] Epoch 11\\Batch 19350\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:47:59] Epoch 11\\Batch 19400\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:04] Epoch 11\\Batch 19450\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:09] Epoch 11\\Batch 19500\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:14] Epoch 11\\Batch 19550\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:19] Epoch 11\\Batch 19600\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:24] Epoch 11\\Batch 19650\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:29] Epoch 11\\Batch 19700\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:34] Epoch 11\\Batch 19750\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:39] Epoch 11\\Batch 19800\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:44] Epoch 11\\Batch 19850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:49] Epoch 11\\Batch 19900\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:54] Epoch 11\\Batch 19950\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:48:59] Epoch 11\\Batch 20000\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:03] Epoch 11\\Batch 20050\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:08] Epoch 11\\Batch 20100\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:13] Epoch 11\\Batch 20150\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:18] Epoch 11\\Batch 20200\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:23] Epoch 11\\Batch 20250\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:28] Epoch 11\\Batch 20300\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:33] Epoch 11\\Batch 20350\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:37] Epoch 11\\Batch 20400\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:42] Epoch 11\\Batch 20450\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:47] Epoch 11\\Batch 20500\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:52] Epoch 11\\Batch 20550\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:49:57] Epoch 11\\Batch 20600\\ Train Loss:10.188\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:02] Epoch 11\\Batch 20650\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:07] Epoch 11\\Batch 20700\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:12] Epoch 11\\Batch 20750\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:17] Epoch 11\\Batch 20800\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:22] Epoch 11\\Batch 20850\\ Train Loss:10.187\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:27] Epoch 11\\Batch 20900\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:32] Epoch 11\\Batch 20950\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:50:37] Epoch 11\\Batch 21000\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5432.000001, 'TP': 1567.0000009999999, 'FP': 1630.0000009999999}\n",
      "[2019/03/18 07:50:59] Epoch 11/ Validation Loss:9.711/ F1_score:0.307/ Precision:0.490/ Recall:0.224\n",
      "[2019/03/18 07:51:05] Epoch 11\\Batch 21050\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:09] Epoch 11\\Batch 21100\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:14] Epoch 11\\Batch 21150\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:19] Epoch 11\\Batch 21200\\ Train Loss:10.186\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:24] Epoch 11\\Batch 21250\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:29] Epoch 11\\Batch 21300\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:34] Epoch 11\\Batch 21350\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:39] Epoch 11\\Batch 21400\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:44] Epoch 11\\Batch 21450\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:49] Epoch 11\\Batch 21500\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:54] Epoch 11\\Batch 21550\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:51:59] Epoch 11\\Batch 21600\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:04] Epoch 11\\Batch 21650\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:09] Epoch 11\\Batch 21700\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:13] Epoch 11\\Batch 21750\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:18] Epoch 11\\Batch 21800\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:23] Epoch 11\\Batch 21850\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:28] Epoch 11\\Batch 21900\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:33] Epoch 11\\Batch 21950\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:38] Epoch 11\\Batch 22000\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:43] Epoch 11\\Batch 22050\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:48] Epoch 11\\Batch 22100\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:53] Epoch 11\\Batch 22150\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:52:57] Epoch 11\\Batch 22200\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:02] Epoch 11\\Batch 22250\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:07] Epoch 11\\Batch 22300\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:12] Epoch 11\\Batch 22350\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:17] Epoch 11\\Batch 22400\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:22] Epoch 11\\Batch 22450\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:26] Epoch 11\\Batch 22500\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:31] Epoch 11\\Batch 22550\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:34] Epoch 11\\Batch 22600\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:37] Epoch 11\\Batch 22650\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:40] Epoch 11\\Batch 22700\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:44] Epoch 11\\Batch 22750\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:47] Epoch 11\\Batch 22800\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:50] Epoch 11\\Batch 22850\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:53] Epoch 11\\Batch 22900\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:53:58] Epoch 11\\Batch 22950\\ Train Loss:10.184\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 07:54:03] Epoch 11\\Batch 23000\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:08] Epoch 11\\Batch 23050\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:13] Epoch 11\\Batch 23100\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:18] Epoch 11\\Batch 23150\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:22] Epoch 11\\Batch 23200\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:27] Epoch 11\\Batch 23250\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:32] Epoch 11\\Batch 23300\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:37] Epoch 11\\Batch 23350\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:42] Epoch 11\\Batch 23400\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:47] Epoch 11\\Batch 23450\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:52] Epoch 11\\Batch 23500\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:54:57] Epoch 11\\Batch 23550\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:02] Epoch 11\\Batch 23600\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:07] Epoch 11\\Batch 23650\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:11] Epoch 11\\Batch 23700\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:16] Epoch 11\\Batch 23750\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:21] Epoch 11\\Batch 23800\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:26] Epoch 11\\Batch 23850\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:31] Epoch 11\\Batch 23900\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:36] Epoch 11\\Batch 23950\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 07:55:41] Epoch 11\\Batch 24000\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5439.000001, 'TP': 1560.0000009999999, 'FP': 1652.0000009999999}\n",
      "[2019/03/18 07:56:04] Epoch 11/ Validation Loss:9.700/ F1_score:0.306/ Precision:0.486/ Recall:0.223\n",
      "[2019/03/18 07:56:09] Epoch 11\\Batch 24050\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:14] Epoch 11\\Batch 24100\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:19] Epoch 11\\Batch 24150\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:24] Epoch 11\\Batch 24200\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:29] Epoch 11\\Batch 24250\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:34] Epoch 11\\Batch 24300\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:39] Epoch 11\\Batch 24350\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:44] Epoch 11\\Batch 24400\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:49] Epoch 11\\Batch 24450\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:54] Epoch 11\\Batch 24500\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 07:56:59] Epoch 11\\Batch 24550\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:04] Epoch 11\\Batch 24600\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:08] Epoch 11\\Batch 24650\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:13] Epoch 11\\Batch 24700\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:18] Epoch 11\\Batch 24750\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:23] Epoch 11\\Batch 24800\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:28] Epoch 11\\Batch 24850\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:33] Epoch 11\\Batch 24900\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:38] Epoch 11\\Batch 24950\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:43] Epoch 11\\Batch 25000\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:48] Epoch 11\\Batch 25050\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:53] Epoch 11\\Batch 25100\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 07:57:57] Epoch 11\\Batch 25150\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:02] Epoch 11\\Batch 25200\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:07] Epoch 11\\Batch 25250\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:12] Epoch 11\\Batch 25300\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:17] Epoch 11\\Batch 25350\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:22] Epoch 11\\Batch 25400\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:27] Epoch 11\\Batch 25450\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:32] Epoch 11\\Batch 25500\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:37] Epoch 11\\Batch 25550\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:42] Epoch 11\\Batch 25600\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:47] Epoch 11\\Batch 25650\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:52] Epoch 11\\Batch 25700\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:58:57] Epoch 11\\Batch 25750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:02] Epoch 11\\Batch 25800\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:07] Epoch 11\\Batch 25850\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:12] Epoch 11\\Batch 25900\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:17] Epoch 11\\Batch 25950\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:21] Epoch 11\\Batch 26000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:26] Epoch 11\\Batch 26050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:31] Epoch 11\\Batch 26100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:36] Epoch 11\\Batch 26150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:41] Epoch 11\\Batch 26200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:46] Epoch 11\\Batch 26250\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:51] Epoch 11\\Batch 26300\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 07:59:56] Epoch 11\\Batch 26350\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:01] Epoch 11\\Batch 26400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:05] Epoch 11\\Batch 26450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:10] Epoch 11\\Batch 26500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:15] Epoch 11\\Batch 26550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:20] Epoch 11\\Batch 26600\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:25] Epoch 11\\Batch 26650\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:30] Epoch 11\\Batch 26700\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:35] Epoch 11\\Batch 26750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:39] Epoch 11\\Batch 26800\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:42] Epoch 11\\Batch 26850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:45] Epoch 11\\Batch 26900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:48] Epoch 11\\Batch 26950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:00:51] Epoch 11\\Batch 27000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5455.000001, 'TP': 1544.0000009999999, 'FP': 1649.0000009999999}\n",
      "[2019/03/18 08:01:12] Epoch 11/ Validation Loss:9.715/ F1_score:0.303/ Precision:0.484/ Recall:0.221\n",
      "[2019/03/18 08:01:17] Epoch 11\\Batch 27050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:22] Epoch 11\\Batch 27100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:27] Epoch 11\\Batch 27150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:32] Epoch 11\\Batch 27200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:37] Epoch 11\\Batch 27250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:42] Epoch 11\\Batch 27300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:46] Epoch 11\\Batch 27350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:51] Epoch 11\\Batch 27400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:01:56] Epoch 11\\Batch 27450\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:01] Epoch 11\\Batch 27500\\ Train Loss:10.178\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:02:06] Epoch 11\\Batch 27550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:11] Epoch 11\\Batch 27600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:16] Epoch 11\\Batch 27650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:21] Epoch 11\\Batch 27700\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:26] Epoch 11\\Batch 27750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:31] Epoch 11\\Batch 27800\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:35] Epoch 11\\Batch 27850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:40] Epoch 11\\Batch 27900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:45] Epoch 11\\Batch 27950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:50] Epoch 11\\Batch 28000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:02:55] Epoch 11\\Batch 28050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:00] Epoch 11\\Batch 28100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:05] Epoch 11\\Batch 28150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:10] Epoch 11\\Batch 28200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:15] Epoch 11\\Batch 28250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:20] Epoch 11\\Batch 28300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:25] Epoch 11\\Batch 28350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:30] Epoch 11\\Batch 28400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:35] Epoch 11\\Batch 28450\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:40] Epoch 11\\Batch 28500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:45] Epoch 11\\Batch 28550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:50] Epoch 11\\Batch 28600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:54] Epoch 11\\Batch 28650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:03:59] Epoch 11\\Batch 28700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:04] Epoch 11\\Batch 28750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:09] Epoch 11\\Batch 28800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:14] Epoch 11\\Batch 28850\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:19] Epoch 11\\Batch 28900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:24] Epoch 11\\Batch 28950\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:29] Epoch 11\\Batch 29000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:34] Epoch 11\\Batch 29050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:39] Epoch 11\\Batch 29100\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:44] Epoch 11\\Batch 29150\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:49] Epoch 11\\Batch 29200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:54] Epoch 11\\Batch 29250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:04:59] Epoch 11\\Batch 29300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:03] Epoch 11\\Batch 29350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:09] Epoch 11\\Batch 29400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:13] Epoch 11\\Batch 29450\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:18] Epoch 11\\Batch 29500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:23] Epoch 11\\Batch 29550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:28] Epoch 11\\Batch 29600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:33] Epoch 11\\Batch 29650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:38] Epoch 11\\Batch 29700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:43] Epoch 11\\Batch 29750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:48] Epoch 11\\Batch 29800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:53] Epoch 11\\Batch 29850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:05:58] Epoch 11\\Batch 29900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:06:03] Epoch 11\\Batch 29950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:06:08] Epoch 11\\Batch 30000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5421.000001, 'TP': 1578.0000009999999, 'FP': 1611.0000009999999}\n",
      "[2019/03/18 08:06:30] Epoch 11/ Validation Loss:9.717/ F1_score:0.310/ Precision:0.495/ Recall:0.225\n",
      "[2019/03/18 08:06:36] Epoch 11\\Batch 30050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:06:41] Epoch 11\\Batch 30100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:06:46] Epoch 11\\Batch 30150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:06:51] Epoch 11\\Batch 30200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:06:55] Epoch 11\\Batch 30250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:00] Epoch 11\\Batch 30300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:05] Epoch 11\\Batch 30350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:10] Epoch 11\\Batch 30400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:15] Epoch 11\\Batch 30450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:20] Epoch 11\\Batch 30500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:25] Epoch 11\\Batch 30550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:30] Epoch 11\\Batch 30600\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:34] Epoch 11\\Batch 30650\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:38] Epoch 11\\Batch 30700\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:41] Epoch 11\\Batch 30750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:44] Epoch 11\\Batch 30800\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:47] Epoch 11\\Batch 30850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:51] Epoch 11\\Batch 30900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:54] Epoch 11\\Batch 30950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:07:57] Epoch 11\\Batch 31000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:02] Epoch 11\\Batch 31050\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:07] Epoch 11\\Batch 31100\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:12] Epoch 11\\Batch 31150\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:17] Epoch 11\\Batch 31200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:22] Epoch 11\\Batch 31250\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:27] Epoch 11\\Batch 31300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:32] Epoch 11\\Batch 31350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:36] Epoch 11\\Batch 31400\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:41] Epoch 11\\Batch 31450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:46] Epoch 11\\Batch 31500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:51] Epoch 11\\Batch 31550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:08:56] Epoch 11\\Batch 31600\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:01] Epoch 11\\Batch 31650\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:06] Epoch 11\\Batch 31700\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:11] Epoch 11\\Batch 31750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:16] Epoch 11\\Batch 31800\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:21] Epoch 11\\Batch 31850\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:26] Epoch 11\\Batch 31900\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:30] Epoch 11\\Batch 31950\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:35] Epoch 11\\Batch 32000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:40] Epoch 11\\Batch 32050\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:45] Epoch 11\\Batch 32100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:50] Epoch 11\\Batch 32150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:09:55] Epoch 11\\Batch 32200\\ Train Loss:10.179\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:10:00] Epoch 11\\Batch 32250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:05] Epoch 11\\Batch 32300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:10] Epoch 11\\Batch 32350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:15] Epoch 11\\Batch 32400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:20] Epoch 11\\Batch 32450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:24] Epoch 11\\Batch 32500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:29] Epoch 11\\Batch 32550\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:34] Epoch 11\\Batch 32600\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:39] Epoch 11\\Batch 32650\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:44] Epoch 11\\Batch 32700\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:49] Epoch 11\\Batch 32750\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:53] Epoch 11\\Batch 32800\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:56] Epoch 11\\Batch 32850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:10:59] Epoch 11\\Batch 32900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:02] Epoch 11\\Batch 32950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:06] Epoch 11\\Batch 33000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5429.000001, 'TP': 1570.0000009999999, 'FP': 1649.0000009999999}\n",
      "[2019/03/18 08:11:22] Epoch 11/ Validation Loss:9.703/ F1_score:0.307/ Precision:0.488/ Recall:0.224\n",
      "[2019/03/18 08:11:26] Epoch 11\\Batch 33050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:29] Epoch 11\\Batch 33100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:32] Epoch 11\\Batch 33150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:35] Epoch 11\\Batch 33200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:38] Epoch 11\\Batch 33250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:41] Epoch 11\\Batch 33300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:44] Epoch 11\\Batch 33350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:47] Epoch 11\\Batch 33400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:50] Epoch 11\\Batch 33450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:53] Epoch 11\\Batch 33500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:56] Epoch 11\\Batch 33550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:11:59] Epoch 11\\Batch 33600\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:02] Epoch 11\\Batch 33650\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:05] Epoch 11\\Batch 33700\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:08] Epoch 11\\Batch 33750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:11] Epoch 11\\Batch 33800\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:14] Epoch 11\\Batch 33850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:18] Epoch 11\\Batch 33900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:21] Epoch 11\\Batch 33950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:24] Epoch 11\\Batch 34000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:27] Epoch 11\\Batch 34050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:30] Epoch 11\\Batch 34100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:33] Epoch 11\\Batch 34150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:36] Epoch 11\\Batch 34200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:39] Epoch 11\\Batch 34250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:42] Epoch 11\\Batch 34300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:45] Epoch 11\\Batch 34350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:48] Epoch 11\\Batch 34400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:51] Epoch 11\\Batch 34450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:54] Epoch 11\\Batch 34500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:12:57] Epoch 11\\Batch 34550\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:00] Epoch 11\\Batch 34600\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:03] Epoch 11\\Batch 34650\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:06] Epoch 11\\Batch 34700\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:09] Epoch 11\\Batch 34750\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:12] Epoch 11\\Batch 34800\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:15] Epoch 11\\Batch 34850\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:18] Epoch 11\\Batch 34900\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:21] Epoch 11\\Batch 34950\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:24] Epoch 11\\Batch 35000\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:27] Epoch 11\\Batch 35050\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:30] Epoch 11\\Batch 35100\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:33] Epoch 11\\Batch 35150\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:36] Epoch 11\\Batch 35200\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:39] Epoch 11\\Batch 35250\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:42] Epoch 11\\Batch 35300\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:45] Epoch 11\\Batch 35350\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:49] Epoch 11\\Batch 35400\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:52] Epoch 11\\Batch 35450\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:55] Epoch 11\\Batch 35500\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:13:58] Epoch 11\\Batch 35550\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:01] Epoch 11\\Batch 35600\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:04] Epoch 11\\Batch 35650\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:07] Epoch 11\\Batch 35700\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:10] Epoch 11\\Batch 35750\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:13] Epoch 11\\Batch 35800\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:16] Epoch 11\\Batch 35850\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:19] Epoch 11\\Batch 35900\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:22] Epoch 11\\Batch 35950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:25] Epoch 11\\Batch 36000\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5445.000001, 'TP': 1554.0000009999999, 'FP': 1643.0000009999999}\n",
      "[2019/03/18 08:14:40] Epoch 11/ Validation Loss:9.727/ F1_score:0.305/ Precision:0.486/ Recall:0.222\n",
      "[2019/03/18 08:14:43] Epoch 11\\Batch 36050\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:46] Epoch 11\\Batch 36100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:49] Epoch 11\\Batch 36150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:52] Epoch 11\\Batch 36200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:55] Epoch 11\\Batch 36250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:14:58] Epoch 11\\Batch 36300\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:01] Epoch 11\\Batch 36350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:04] Epoch 11\\Batch 36400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:07] Epoch 11\\Batch 36450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:10] Epoch 11\\Batch 36500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:12] Epoch 11\\Batch 36550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:15] Epoch 11\\Batch 36600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:18] Epoch 11\\Batch 36650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:21] Epoch 11\\Batch 36700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:24] Epoch 11\\Batch 36750\\ Train Loss:10.178\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:15:27] Epoch 11\\Batch 36800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:30] Epoch 11\\Batch 36850\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:33] Epoch 11\\Batch 36900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:36] Epoch 11\\Batch 36950\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:38] Epoch 11\\Batch 37000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:41] Epoch 11\\Batch 37050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:44] Epoch 11\\Batch 37100\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:47] Epoch 11\\Batch 37150\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:50] Epoch 11\\Batch 37200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:53] Epoch 11\\Batch 37250\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:56] Epoch 11\\Batch 37300\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:15:59] Epoch 11\\Batch 37350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:02] Epoch 11\\Batch 37400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:05] Epoch 11\\Batch 37450\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:07] Epoch 11\\Batch 37500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:10] Epoch 11\\Batch 37550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:13] Epoch 11\\Batch 37600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:16] Epoch 11\\Batch 37650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:19] Epoch 11\\Batch 37700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:22] Epoch 11\\Batch 37750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:25] Epoch 11\\Batch 37800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:28] Epoch 11\\Batch 37850\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:31] Epoch 11\\Batch 37900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:33] Epoch 11\\Batch 37950\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:36] Epoch 11\\Batch 38000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:39] Epoch 11\\Batch 38050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:42] Epoch 11\\Batch 38100\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:45] Epoch 11\\Batch 38150\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:48] Epoch 11\\Batch 38200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:51] Epoch 11\\Batch 38250\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:54] Epoch 11\\Batch 38300\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:57] Epoch 11\\Batch 38350\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:16:59] Epoch 11\\Batch 38400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:02] Epoch 11\\Batch 38450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:05] Epoch 11\\Batch 38500\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:08] Epoch 11\\Batch 38550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:11] Epoch 11\\Batch 38600\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:14] Epoch 11\\Batch 38650\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:17] Epoch 11\\Batch 38700\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:20] Epoch 11\\Batch 38750\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:22] Epoch 11\\Batch 38800\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:25] Epoch 11\\Batch 38850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:28] Epoch 11\\Batch 38900\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:31] Epoch 11\\Batch 38950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:34] Epoch 11\\Batch 39000\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5440.000001, 'TP': 1559.0000009999999, 'FP': 1658.0000009999999}\n",
      "[2019/03/18 08:17:49] Epoch 11/ Validation Loss:9.742/ F1_score:0.305/ Precision:0.485/ Recall:0.223\n",
      "[2019/03/18 08:17:52] Epoch 11\\Batch 39050\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:55] Epoch 11\\Batch 39100\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:17:58] Epoch 11\\Batch 39150\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:01] Epoch 11\\Batch 39200\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:04] Epoch 11\\Batch 39250\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:07] Epoch 11\\Batch 39300\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:09] Epoch 11\\Batch 39350\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:12] Epoch 11\\Batch 39400\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:15] Epoch 11\\Batch 39450\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:18] Epoch 11\\Batch 39500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:21] Epoch 11\\Batch 39550\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:24] Epoch 11\\Batch 39600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:27] Epoch 11\\Batch 39650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:30] Epoch 11\\Batch 39700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:33] Epoch 11\\Batch 39750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:35] Epoch 11\\Batch 39800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:38] Epoch 11\\Batch 39850\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:41] Epoch 11\\Batch 39900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:44] Epoch 11\\Batch 39950\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:47] Epoch 11\\Batch 40000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:50] Epoch 11\\Batch 40050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:53] Epoch 11\\Batch 40100\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:56] Epoch 11\\Batch 40150\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:18:58] Epoch 11\\Batch 40200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:01] Epoch 11\\Batch 40250\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:04] Epoch 11\\Batch 40300\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:07] Epoch 11\\Batch 40350\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:10] Epoch 11\\Batch 40400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:13] Epoch 11\\Batch 40450\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:16] Epoch 11\\Batch 40500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:19] Epoch 11\\Batch 40550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:22] Epoch 11\\Batch 40600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:24] Epoch 11\\Batch 40650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:27] Epoch 11\\Batch 40700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:30] Epoch 11\\Batch 40750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:33] Epoch 11\\Batch 40800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:36] Epoch 11\\Batch 40850\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:39] Epoch 11\\Batch 40900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:42] Epoch 11\\Batch 40950\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:45] Epoch 11\\Batch 41000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:47] Epoch 11\\Batch 41050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:50] Epoch 11\\Batch 41100\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:53] Epoch 11\\Batch 41150\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:56] Epoch 11\\Batch 41200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:19:59] Epoch 11\\Batch 41250\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:02] Epoch 11\\Batch 41300\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:05] Epoch 11\\Batch 41350\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:08] Epoch 11\\Batch 41400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:11] Epoch 11\\Batch 41450\\ Train Loss:10.178\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:20:13] Epoch 11\\Batch 41500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:16] Epoch 11\\Batch 41550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:19] Epoch 11\\Batch 41600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:22] Epoch 11\\Batch 41650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:25] Epoch 11\\Batch 41700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:28] Epoch 11\\Batch 41750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:31] Epoch 11\\Batch 41800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:34] Epoch 11\\Batch 41850\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:36] Epoch 11\\Batch 41900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:39] Epoch 11\\Batch 41950\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:20:42] Epoch 11\\Batch 42000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5410.000001, 'TP': 1589.0000009999999, 'FP': 1616.0000009999999}\n",
      "[2019/03/18 08:20:58] Epoch 11/ Validation Loss:9.715/ F1_score:0.311/ Precision:0.496/ Recall:0.227\n",
      "[2019/03/18 08:21:01] Epoch 11\\Batch 42050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:03] Epoch 11\\Batch 42100\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:06] Epoch 11\\Batch 42150\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:09] Epoch 11\\Batch 42200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:12] Epoch 11\\Batch 42250\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:15] Epoch 11\\Batch 42300\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:18] Epoch 11\\Batch 42350\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:21] Epoch 11\\Batch 42400\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:24] Epoch 11\\Batch 42450\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:26] Epoch 11\\Batch 42500\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:29] Epoch 11\\Batch 42550\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:32] Epoch 11\\Batch 42600\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:35] Epoch 11\\Batch 42650\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:38] Epoch 11\\Batch 42700\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:41] Epoch 11\\Batch 42750\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:44] Epoch 11\\Batch 42800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:47] Epoch 11\\Batch 42850\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:50] Epoch 11\\Batch 42900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:52] Epoch 11\\Batch 42950\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:55] Epoch 11\\Batch 43000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:21:58] Epoch 11\\Batch 43050\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:01] Epoch 11\\Batch 43100\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:04] Epoch 11\\Batch 43150\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:07] Epoch 11\\Batch 43200\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:10] Epoch 11\\Batch 43250\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:13] Epoch 11\\Batch 43300\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:15] Epoch 11\\Batch 43350\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:18] Epoch 11\\Batch 43400\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:21] Epoch 11\\Batch 43450\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:24] Epoch 11\\Batch 43500\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:27] Epoch 11\\Batch 43550\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:30] Epoch 11\\Batch 43600\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:33] Epoch 11\\Batch 43650\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:36] Epoch 11\\Batch 43700\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:39] Epoch 11\\Batch 43750\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:41] Epoch 11\\Batch 43800\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:44] Epoch 11\\Batch 43850\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:47] Epoch 11\\Batch 43900\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:50] Epoch 11\\Batch 43950\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:53] Epoch 11\\Batch 44000\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:56] Epoch 11\\Batch 44050\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:22:59] Epoch 11\\Batch 44100\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:02] Epoch 11\\Batch 44150\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:04] Epoch 11\\Batch 44200\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:07] Epoch 11\\Batch 44250\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:10] Epoch 11\\Batch 44300\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:13] Epoch 11\\Batch 44350\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:16] Epoch 11\\Batch 44400\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:19] Epoch 11\\Batch 44450\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:22] Epoch 11\\Batch 44500\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:25] Epoch 11\\Batch 44550\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:28] Epoch 11\\Batch 44600\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:30] Epoch 11\\Batch 44650\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:33] Epoch 11\\Batch 44700\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:36] Epoch 11\\Batch 44750\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:39] Epoch 11\\Batch 44800\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:42] Epoch 11\\Batch 44850\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:45] Epoch 11\\Batch 44900\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:48] Epoch 11\\Batch 44950\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:23:51] Epoch 11\\Batch 45000\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5449.000001, 'TP': 1550.0000009999999, 'FP': 1635.0000009999999}\n",
      "[2019/03/18 08:24:06] Epoch 11/ Validation Loss:9.724/ F1_score:0.304/ Precision:0.487/ Recall:0.221\n",
      "[2019/03/18 08:24:09] Epoch 11\\Batch 45050\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:12] Epoch 11\\Batch 45100\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:15] Epoch 11\\Batch 45150\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:18] Epoch 11\\Batch 45200\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:21] Epoch 11\\Batch 45250\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:24] Epoch 11\\Batch 45300\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:26] Epoch 11\\Batch 45350\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:29] Epoch 11\\Batch 45400\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:32] Epoch 11\\Batch 45450\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:35] Epoch 11\\Batch 45500\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:38] Epoch 11\\Batch 45550\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:41] Epoch 11\\Batch 45600\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:44] Epoch 11\\Batch 45650\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:47] Epoch 11\\Batch 45700\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:50] Epoch 11\\Batch 45750\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:52] Epoch 11\\Batch 45800\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:55] Epoch 11\\Batch 45850\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:24:58] Epoch 11\\Batch 45900\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:01] Epoch 11\\Batch 45950\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:04] Epoch 11\\Batch 46000\\ Train Loss:10.176\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:25:07] Epoch 11\\Batch 46050\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:10] Epoch 11\\Batch 46100\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:13] Epoch 11\\Batch 46150\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:15] Epoch 11\\Batch 46200\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:21] Epoch 12\\Batch 50\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:24] Epoch 12\\Batch 100\\ Train Loss:10.240\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:27] Epoch 12\\Batch 150\\ Train Loss:10.274\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:30] Epoch 12\\Batch 200\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:33] Epoch 12\\Batch 250\\ Train Loss:10.215\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:35] Epoch 12\\Batch 300\\ Train Loss:10.228\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:38] Epoch 12\\Batch 350\\ Train Loss:10.230\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:41] Epoch 12\\Batch 400\\ Train Loss:10.227\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:44] Epoch 12\\Batch 450\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:47] Epoch 12\\Batch 500\\ Train Loss:10.200\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:50] Epoch 12\\Batch 550\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:53] Epoch 12\\Batch 600\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:56] Epoch 12\\Batch 650\\ Train Loss:10.179\\ Learning rate:0.00030\n",
      "[2019/03/18 08:25:58] Epoch 12\\Batch 700\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:01] Epoch 12\\Batch 750\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:04] Epoch 12\\Batch 800\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:07] Epoch 12\\Batch 850\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:10] Epoch 12\\Batch 900\\ Train Loss:10.189\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:13] Epoch 12\\Batch 950\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:16] Epoch 12\\Batch 1000\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:19] Epoch 12\\Batch 1050\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:22] Epoch 12\\Batch 1100\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:24] Epoch 12\\Batch 1150\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:27] Epoch 12\\Batch 1200\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:30] Epoch 12\\Batch 1250\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:33] Epoch 12\\Batch 1300\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:36] Epoch 12\\Batch 1350\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:39] Epoch 12\\Batch 1400\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:42] Epoch 12\\Batch 1450\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:45] Epoch 12\\Batch 1500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:47] Epoch 12\\Batch 1550\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:50] Epoch 12\\Batch 1600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:53] Epoch 12\\Batch 1650\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:56] Epoch 12\\Batch 1700\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:26:59] Epoch 12\\Batch 1750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:02] Epoch 12\\Batch 1800\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:05] Epoch 12\\Batch 1850\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:08] Epoch 12\\Batch 1900\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:10] Epoch 12\\Batch 1950\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:13] Epoch 12\\Batch 2000\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:16] Epoch 12\\Batch 2050\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:19] Epoch 12\\Batch 2100\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:22] Epoch 12\\Batch 2150\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:25] Epoch 12\\Batch 2200\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:28] Epoch 12\\Batch 2250\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:31] Epoch 12\\Batch 2300\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:33] Epoch 12\\Batch 2350\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:36] Epoch 12\\Batch 2400\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:39] Epoch 12\\Batch 2450\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:42] Epoch 12\\Batch 2500\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:45] Epoch 12\\Batch 2550\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:48] Epoch 12\\Batch 2600\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:51] Epoch 12\\Batch 2650\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:54] Epoch 12\\Batch 2700\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:56] Epoch 12\\Batch 2750\\ Train Loss:10.185\\ Learning rate:0.00030\n",
      "[2019/03/18 08:27:59] Epoch 12\\Batch 2800\\ Train Loss:10.183\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:02] Epoch 12\\Batch 2850\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:05] Epoch 12\\Batch 2900\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:08] Epoch 12\\Batch 2950\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:11] Epoch 12\\Batch 3000\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5474.000001, 'TP': 1525.0000009999999, 'FP': 1672.0000009999999}\n",
      "[2019/03/18 08:28:26] Epoch 12/ Validation Loss:9.727/ F1_score:0.299/ Precision:0.477/ Recall:0.218\n",
      "[2019/03/18 08:28:29] Epoch 12\\Batch 3050\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:32] Epoch 12\\Batch 3100\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:35] Epoch 12\\Batch 3150\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:38] Epoch 12\\Batch 3200\\ Train Loss:10.178\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:41] Epoch 12\\Batch 3250\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:44] Epoch 12\\Batch 3300\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:47] Epoch 12\\Batch 3350\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:49] Epoch 12\\Batch 3400\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:52] Epoch 12\\Batch 3450\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:55] Epoch 12\\Batch 3500\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:28:58] Epoch 12\\Batch 3550\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:01] Epoch 12\\Batch 3600\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:04] Epoch 12\\Batch 3650\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:07] Epoch 12\\Batch 3700\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:10] Epoch 12\\Batch 3750\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:12] Epoch 12\\Batch 3800\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:15] Epoch 12\\Batch 3850\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:18] Epoch 12\\Batch 3900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:21] Epoch 12\\Batch 3950\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:24] Epoch 12\\Batch 4000\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:27] Epoch 12\\Batch 4050\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:30] Epoch 12\\Batch 4100\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:33] Epoch 12\\Batch 4150\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:35] Epoch 12\\Batch 4200\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:38] Epoch 12\\Batch 4250\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:41] Epoch 12\\Batch 4300\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:44] Epoch 12\\Batch 4350\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:47] Epoch 12\\Batch 4400\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:50] Epoch 12\\Batch 4450\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:53] Epoch 12\\Batch 4500\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:29:56] Epoch 12\\Batch 4550\\ Train Loss:10.167\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:29:59] Epoch 12\\Batch 4600\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:01] Epoch 12\\Batch 4650\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:04] Epoch 12\\Batch 4700\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:07] Epoch 12\\Batch 4750\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:10] Epoch 12\\Batch 4800\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:13] Epoch 12\\Batch 4850\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:16] Epoch 12\\Batch 4900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:19] Epoch 12\\Batch 4950\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:22] Epoch 12\\Batch 5000\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:25] Epoch 12\\Batch 5050\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:27] Epoch 12\\Batch 5100\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:30] Epoch 12\\Batch 5150\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:33] Epoch 12\\Batch 5200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:36] Epoch 12\\Batch 5250\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:39] Epoch 12\\Batch 5300\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:42] Epoch 12\\Batch 5350\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:45] Epoch 12\\Batch 5400\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:48] Epoch 12\\Batch 5450\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:50] Epoch 12\\Batch 5500\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:53] Epoch 12\\Batch 5550\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:56] Epoch 12\\Batch 5600\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:30:59] Epoch 12\\Batch 5650\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:02] Epoch 12\\Batch 5700\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:05] Epoch 12\\Batch 5750\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:08] Epoch 12\\Batch 5800\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:11] Epoch 12\\Batch 5850\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:14] Epoch 12\\Batch 5900\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:17] Epoch 12\\Batch 5950\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:19] Epoch 12\\Batch 6000\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5450.000001, 'TP': 1549.0000009999999, 'FP': 1660.0000009999999}\n",
      "[2019/03/18 08:31:35] Epoch 12/ Validation Loss:9.722/ F1_score:0.303/ Precision:0.483/ Recall:0.221\n",
      "[2019/03/18 08:31:38] Epoch 12\\Batch 6050\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:41] Epoch 12\\Batch 6100\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:44] Epoch 12\\Batch 6150\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:47] Epoch 12\\Batch 6200\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:50] Epoch 12\\Batch 6250\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:52] Epoch 12\\Batch 6300\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:55] Epoch 12\\Batch 6350\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:31:58] Epoch 12\\Batch 6400\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:01] Epoch 12\\Batch 6450\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:04] Epoch 12\\Batch 6500\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:07] Epoch 12\\Batch 6550\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:10] Epoch 12\\Batch 6600\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:13] Epoch 12\\Batch 6650\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:16] Epoch 12\\Batch 6700\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:18] Epoch 12\\Batch 6750\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:21] Epoch 12\\Batch 6800\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:24] Epoch 12\\Batch 6850\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:27] Epoch 12\\Batch 6900\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:30] Epoch 12\\Batch 6950\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:33] Epoch 12\\Batch 7000\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:36] Epoch 12\\Batch 7050\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:39] Epoch 12\\Batch 7100\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:41] Epoch 12\\Batch 7150\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:44] Epoch 12\\Batch 7200\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:47] Epoch 12\\Batch 7250\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:50] Epoch 12\\Batch 7300\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:53] Epoch 12\\Batch 7350\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:56] Epoch 12\\Batch 7400\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:32:59] Epoch 12\\Batch 7450\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:02] Epoch 12\\Batch 7500\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:05] Epoch 12\\Batch 7550\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:07] Epoch 12\\Batch 7600\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:10] Epoch 12\\Batch 7650\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:13] Epoch 12\\Batch 7700\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:16] Epoch 12\\Batch 7750\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:19] Epoch 12\\Batch 7800\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:22] Epoch 12\\Batch 7850\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:25] Epoch 12\\Batch 7900\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:28] Epoch 12\\Batch 7950\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:30] Epoch 12\\Batch 8000\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:33] Epoch 12\\Batch 8050\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:36] Epoch 12\\Batch 8100\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:39] Epoch 12\\Batch 8150\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:42] Epoch 12\\Batch 8200\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:45] Epoch 12\\Batch 8250\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:48] Epoch 12\\Batch 8300\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:51] Epoch 12\\Batch 8350\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:53] Epoch 12\\Batch 8400\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:56] Epoch 12\\Batch 8450\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:33:59] Epoch 12\\Batch 8500\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:02] Epoch 12\\Batch 8550\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:05] Epoch 12\\Batch 8600\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:08] Epoch 12\\Batch 8650\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:11] Epoch 12\\Batch 8700\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:14] Epoch 12\\Batch 8750\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:16] Epoch 12\\Batch 8800\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:19] Epoch 12\\Batch 8850\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:22] Epoch 12\\Batch 8900\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:25] Epoch 12\\Batch 8950\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:28] Epoch 12\\Batch 9000\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5444.000001, 'TP': 1555.0000009999999, 'FP': 1652.0000009999999}\n",
      "[2019/03/18 08:34:43] Epoch 12/ Validation Loss:9.720/ F1_score:0.305/ Precision:0.485/ Recall:0.222\n",
      "[2019/03/18 08:34:46] Epoch 12\\Batch 9050\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:49] Epoch 12\\Batch 9100\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:52] Epoch 12\\Batch 9150\\ Train Loss:10.175\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:34:55] Epoch 12\\Batch 9200\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:34:58] Epoch 12\\Batch 9250\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:01] Epoch 12\\Batch 9300\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:04] Epoch 12\\Batch 9350\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:06] Epoch 12\\Batch 9400\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:09] Epoch 12\\Batch 9450\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:12] Epoch 12\\Batch 9500\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:15] Epoch 12\\Batch 9550\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:18] Epoch 12\\Batch 9600\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:21] Epoch 12\\Batch 9650\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:24] Epoch 12\\Batch 9700\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:27] Epoch 12\\Batch 9750\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:30] Epoch 12\\Batch 9800\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:33] Epoch 12\\Batch 9850\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:35] Epoch 12\\Batch 9900\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:38] Epoch 12\\Batch 9950\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:41] Epoch 12\\Batch 10000\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:44] Epoch 12\\Batch 10050\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:47] Epoch 12\\Batch 10100\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:50] Epoch 12\\Batch 10150\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:53] Epoch 12\\Batch 10200\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:56] Epoch 12\\Batch 10250\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:35:58] Epoch 12\\Batch 10300\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:01] Epoch 12\\Batch 10350\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:04] Epoch 12\\Batch 10400\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:07] Epoch 12\\Batch 10450\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:10] Epoch 12\\Batch 10500\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:13] Epoch 12\\Batch 10550\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:16] Epoch 12\\Batch 10600\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:19] Epoch 12\\Batch 10650\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:21] Epoch 12\\Batch 10700\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:24] Epoch 12\\Batch 10750\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:27] Epoch 12\\Batch 10800\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:30] Epoch 12\\Batch 10850\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:33] Epoch 12\\Batch 10900\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:36] Epoch 12\\Batch 10950\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:39] Epoch 12\\Batch 11000\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:42] Epoch 12\\Batch 11050\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:45] Epoch 12\\Batch 11100\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:47] Epoch 12\\Batch 11150\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:50] Epoch 12\\Batch 11200\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:53] Epoch 12\\Batch 11250\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:56] Epoch 12\\Batch 11300\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:36:59] Epoch 12\\Batch 11350\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:02] Epoch 12\\Batch 11400\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:05] Epoch 12\\Batch 11450\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:08] Epoch 12\\Batch 11500\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:11] Epoch 12\\Batch 11550\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:13] Epoch 12\\Batch 11600\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:16] Epoch 12\\Batch 11650\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:19] Epoch 12\\Batch 11700\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:22] Epoch 12\\Batch 11750\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:25] Epoch 12\\Batch 11800\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:28] Epoch 12\\Batch 11850\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:31] Epoch 12\\Batch 11900\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:34] Epoch 12\\Batch 11950\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:36] Epoch 12\\Batch 12000\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5430.000001, 'TP': 1569.0000009999999, 'FP': 1641.0000009999999}\n",
      "[2019/03/18 08:37:52] Epoch 12/ Validation Loss:9.694/ F1_score:0.307/ Precision:0.489/ Recall:0.224\n",
      "[2019/03/18 08:37:55] Epoch 12\\Batch 12050\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:37:58] Epoch 12\\Batch 12100\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:01] Epoch 12\\Batch 12150\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:04] Epoch 12\\Batch 12200\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:07] Epoch 12\\Batch 12250\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:09] Epoch 12\\Batch 12300\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:12] Epoch 12\\Batch 12350\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:15] Epoch 12\\Batch 12400\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:18] Epoch 12\\Batch 12450\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:21] Epoch 12\\Batch 12500\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:24] Epoch 12\\Batch 12550\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:27] Epoch 12\\Batch 12600\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:30] Epoch 12\\Batch 12650\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:32] Epoch 12\\Batch 12700\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:35] Epoch 12\\Batch 12750\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:38] Epoch 12\\Batch 12800\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:41] Epoch 12\\Batch 12850\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:44] Epoch 12\\Batch 12900\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:47] Epoch 12\\Batch 12950\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:50] Epoch 12\\Batch 13000\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:53] Epoch 12\\Batch 13050\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:56] Epoch 12\\Batch 13100\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:38:58] Epoch 12\\Batch 13150\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:01] Epoch 12\\Batch 13200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:04] Epoch 12\\Batch 13250\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:07] Epoch 12\\Batch 13300\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:10] Epoch 12\\Batch 13350\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:13] Epoch 12\\Batch 13400\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:16] Epoch 12\\Batch 13450\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:19] Epoch 12\\Batch 13500\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:22] Epoch 12\\Batch 13550\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:24] Epoch 12\\Batch 13600\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:27] Epoch 12\\Batch 13650\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:30] Epoch 12\\Batch 13700\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:33] Epoch 12\\Batch 13750\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:36] Epoch 12\\Batch 13800\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:39] Epoch 12\\Batch 13850\\ Train Loss:10.170\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:39:42] Epoch 12\\Batch 13900\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:45] Epoch 12\\Batch 13950\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:47] Epoch 12\\Batch 14000\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:50] Epoch 12\\Batch 14050\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:53] Epoch 12\\Batch 14100\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:56] Epoch 12\\Batch 14150\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:39:59] Epoch 12\\Batch 14200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:02] Epoch 12\\Batch 14250\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:05] Epoch 12\\Batch 14300\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:08] Epoch 12\\Batch 14350\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:11] Epoch 12\\Batch 14400\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:13] Epoch 12\\Batch 14450\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:16] Epoch 12\\Batch 14500\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:19] Epoch 12\\Batch 14550\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:22] Epoch 12\\Batch 14600\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:25] Epoch 12\\Batch 14650\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:28] Epoch 12\\Batch 14700\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:31] Epoch 12\\Batch 14750\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:34] Epoch 12\\Batch 14800\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:36] Epoch 12\\Batch 14850\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:39] Epoch 12\\Batch 14900\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:42] Epoch 12\\Batch 14950\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:40:45] Epoch 12\\Batch 15000\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5427.000001, 'TP': 1572.0000009999999, 'FP': 1612.0000009999999}\n",
      "[2019/03/18 08:41:01] Epoch 12/ Validation Loss:9.704/ F1_score:0.309/ Precision:0.494/ Recall:0.225\n",
      "[2019/03/18 08:41:04] Epoch 12\\Batch 15050\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:07] Epoch 12\\Batch 15100\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:09] Epoch 12\\Batch 15150\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:12] Epoch 12\\Batch 15200\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:15] Epoch 12\\Batch 15250\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:18] Epoch 12\\Batch 15300\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:21] Epoch 12\\Batch 15350\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:24] Epoch 12\\Batch 15400\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:27] Epoch 12\\Batch 15450\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:30] Epoch 12\\Batch 15500\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:33] Epoch 12\\Batch 15550\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:35] Epoch 12\\Batch 15600\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:38] Epoch 12\\Batch 15650\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:41] Epoch 12\\Batch 15700\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:44] Epoch 12\\Batch 15750\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:47] Epoch 12\\Batch 15800\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:50] Epoch 12\\Batch 15850\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:53] Epoch 12\\Batch 15900\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:56] Epoch 12\\Batch 15950\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:41:59] Epoch 12\\Batch 16000\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:01] Epoch 12\\Batch 16050\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:04] Epoch 12\\Batch 16100\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:07] Epoch 12\\Batch 16150\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:10] Epoch 12\\Batch 16200\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:13] Epoch 12\\Batch 16250\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:16] Epoch 12\\Batch 16300\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:19] Epoch 12\\Batch 16350\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:22] Epoch 12\\Batch 16400\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:25] Epoch 12\\Batch 16450\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:27] Epoch 12\\Batch 16500\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:30] Epoch 12\\Batch 16550\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:33] Epoch 12\\Batch 16600\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:36] Epoch 12\\Batch 16650\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:39] Epoch 12\\Batch 16700\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:42] Epoch 12\\Batch 16750\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:45] Epoch 12\\Batch 16800\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:48] Epoch 12\\Batch 16850\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:51] Epoch 12\\Batch 16900\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:53] Epoch 12\\Batch 16950\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:56] Epoch 12\\Batch 17000\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:42:59] Epoch 12\\Batch 17050\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:02] Epoch 12\\Batch 17100\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:05] Epoch 12\\Batch 17150\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:08] Epoch 12\\Batch 17200\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:11] Epoch 12\\Batch 17250\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:14] Epoch 12\\Batch 17300\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:17] Epoch 12\\Batch 17350\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:19] Epoch 12\\Batch 17400\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:22] Epoch 12\\Batch 17450\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:25] Epoch 12\\Batch 17500\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:28] Epoch 12\\Batch 17550\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:31] Epoch 12\\Batch 17600\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:34] Epoch 12\\Batch 17650\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:37] Epoch 12\\Batch 17700\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:40] Epoch 12\\Batch 17750\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:43] Epoch 12\\Batch 17800\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:45] Epoch 12\\Batch 17850\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:48] Epoch 12\\Batch 17900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:51] Epoch 12\\Batch 17950\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:43:54] Epoch 12\\Batch 18000\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5427.000001, 'TP': 1572.0000009999999, 'FP': 1634.0000009999999}\n",
      "[2019/03/18 08:44:09] Epoch 12/ Validation Loss:9.709/ F1_score:0.308/ Precision:0.490/ Recall:0.225\n",
      "[2019/03/18 08:44:12] Epoch 12\\Batch 18050\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:15] Epoch 12\\Batch 18100\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:18] Epoch 12\\Batch 18150\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:21] Epoch 12\\Batch 18200\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:24] Epoch 12\\Batch 18250\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:27] Epoch 12\\Batch 18300\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:30] Epoch 12\\Batch 18350\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:33] Epoch 12\\Batch 18400\\ Train Loss:10.168\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:44:36] Epoch 12\\Batch 18450\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:38] Epoch 12\\Batch 18500\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:41] Epoch 12\\Batch 18550\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:44] Epoch 12\\Batch 18600\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:47] Epoch 12\\Batch 18650\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:50] Epoch 12\\Batch 18700\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:53] Epoch 12\\Batch 18750\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:56] Epoch 12\\Batch 18800\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:44:59] Epoch 12\\Batch 18850\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:02] Epoch 12\\Batch 18900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:04] Epoch 12\\Batch 18950\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:07] Epoch 12\\Batch 19000\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:10] Epoch 12\\Batch 19050\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:13] Epoch 12\\Batch 19100\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:16] Epoch 12\\Batch 19150\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:19] Epoch 12\\Batch 19200\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:22] Epoch 12\\Batch 19250\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:25] Epoch 12\\Batch 19300\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:28] Epoch 12\\Batch 19350\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:30] Epoch 12\\Batch 19400\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:33] Epoch 12\\Batch 19450\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:36] Epoch 12\\Batch 19500\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:39] Epoch 12\\Batch 19550\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:42] Epoch 12\\Batch 19600\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:45] Epoch 12\\Batch 19650\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:48] Epoch 12\\Batch 19700\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:51] Epoch 12\\Batch 19750\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:53] Epoch 12\\Batch 19800\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:56] Epoch 12\\Batch 19850\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:45:59] Epoch 12\\Batch 19900\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:02] Epoch 12\\Batch 19950\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:05] Epoch 12\\Batch 20000\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:08] Epoch 12\\Batch 20050\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:11] Epoch 12\\Batch 20100\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:14] Epoch 12\\Batch 20150\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:17] Epoch 12\\Batch 20200\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:19] Epoch 12\\Batch 20250\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:22] Epoch 12\\Batch 20300\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:25] Epoch 12\\Batch 20350\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:28] Epoch 12\\Batch 20400\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:31] Epoch 12\\Batch 20450\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:34] Epoch 12\\Batch 20500\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:37] Epoch 12\\Batch 20550\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:40] Epoch 12\\Batch 20600\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:43] Epoch 12\\Batch 20650\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:45] Epoch 12\\Batch 20700\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:48] Epoch 12\\Batch 20750\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:51] Epoch 12\\Batch 20800\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:54] Epoch 12\\Batch 20850\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 08:46:57] Epoch 12\\Batch 20900\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:00] Epoch 12\\Batch 20950\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:03] Epoch 12\\Batch 21000\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572, 793]\n",
      "{'FN': 5412.000001, 'TP': 1587.0000009999999, 'FP': 1622.0000009999999}\n",
      "[2019/03/18 08:47:18] Epoch 12/ Validation Loss:9.704/ F1_score:0.311/ Precision:0.495/ Recall:0.227\n",
      "[2019/03/18 08:47:21] Epoch 12\\Batch 21050\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:24] Epoch 12\\Batch 21100\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:27] Epoch 12\\Batch 21150\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:30] Epoch 12\\Batch 21200\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:33] Epoch 12\\Batch 21250\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:36] Epoch 12\\Batch 21300\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:38] Epoch 12\\Batch 21350\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:41] Epoch 12\\Batch 21400\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:44] Epoch 12\\Batch 21450\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:47] Epoch 12\\Batch 21500\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:50] Epoch 12\\Batch 21550\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:53] Epoch 12\\Batch 21600\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:56] Epoch 12\\Batch 21650\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:47:59] Epoch 12\\Batch 21700\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:02] Epoch 12\\Batch 21750\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:04] Epoch 12\\Batch 21800\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:07] Epoch 12\\Batch 21850\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:10] Epoch 12\\Batch 21900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:13] Epoch 12\\Batch 21950\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:16] Epoch 12\\Batch 22000\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:19] Epoch 12\\Batch 22050\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:22] Epoch 12\\Batch 22100\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:25] Epoch 12\\Batch 22150\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:28] Epoch 12\\Batch 22200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:30] Epoch 12\\Batch 22250\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:33] Epoch 12\\Batch 22300\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:36] Epoch 12\\Batch 22350\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:39] Epoch 12\\Batch 22400\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:42] Epoch 12\\Batch 22450\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:45] Epoch 12\\Batch 22500\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:48] Epoch 12\\Batch 22550\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:51] Epoch 12\\Batch 22600\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:54] Epoch 12\\Batch 22650\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:56] Epoch 12\\Batch 22700\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:48:59] Epoch 12\\Batch 22750\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:02] Epoch 12\\Batch 22800\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:05] Epoch 12\\Batch 22850\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:08] Epoch 12\\Batch 22900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:11] Epoch 12\\Batch 22950\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:14] Epoch 12\\Batch 23000\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:17] Epoch 12\\Batch 23050\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:20] Epoch 12\\Batch 23100\\ Train Loss:10.169\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:49:22] Epoch 12\\Batch 23150\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:25] Epoch 12\\Batch 23200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:28] Epoch 12\\Batch 23250\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:31] Epoch 12\\Batch 23300\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:34] Epoch 12\\Batch 23350\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:37] Epoch 12\\Batch 23400\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:40] Epoch 12\\Batch 23450\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:43] Epoch 12\\Batch 23500\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:45] Epoch 12\\Batch 23550\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:48] Epoch 12\\Batch 23600\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:51] Epoch 12\\Batch 23650\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:54] Epoch 12\\Batch 23700\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:49:57] Epoch 12\\Batch 23750\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:00] Epoch 12\\Batch 23800\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:03] Epoch 12\\Batch 23850\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:06] Epoch 12\\Batch 23900\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:09] Epoch 12\\Batch 23950\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:11] Epoch 12\\Batch 24000\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5396.000001, 'TP': 1603.0000009999999, 'FP': 1621.0000009999999}\n",
      "[2019/03/18 08:50:27] Epoch 12/ Validation Loss:9.694/ F1_score:0.314/ Precision:0.497/ Recall:0.229\n",
      "[2019/03/18 08:50:30] Epoch 12\\Batch 24050\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:33] Epoch 12\\Batch 24100\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:36] Epoch 12\\Batch 24150\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:39] Epoch 12\\Batch 24200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:42] Epoch 12\\Batch 24250\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:44] Epoch 12\\Batch 24300\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:47] Epoch 12\\Batch 24350\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:50] Epoch 12\\Batch 24400\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:53] Epoch 12\\Batch 24450\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:56] Epoch 12\\Batch 24500\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:50:59] Epoch 12\\Batch 24550\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:02] Epoch 12\\Batch 24600\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:05] Epoch 12\\Batch 24650\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:08] Epoch 12\\Batch 24700\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:10] Epoch 12\\Batch 24750\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:13] Epoch 12\\Batch 24800\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:16] Epoch 12\\Batch 24850\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:19] Epoch 12\\Batch 24900\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:22] Epoch 12\\Batch 24950\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:25] Epoch 12\\Batch 25000\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:28] Epoch 12\\Batch 25050\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:31] Epoch 12\\Batch 25100\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:34] Epoch 12\\Batch 25150\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:36] Epoch 12\\Batch 25200\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:39] Epoch 12\\Batch 25250\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:42] Epoch 12\\Batch 25300\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:45] Epoch 12\\Batch 25350\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:48] Epoch 12\\Batch 25400\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:51] Epoch 12\\Batch 25450\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:54] Epoch 12\\Batch 25500\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:51:57] Epoch 12\\Batch 25550\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:00] Epoch 12\\Batch 25600\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:02] Epoch 12\\Batch 25650\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:05] Epoch 12\\Batch 25700\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:08] Epoch 12\\Batch 25750\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:11] Epoch 12\\Batch 25800\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:14] Epoch 12\\Batch 25850\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:17] Epoch 12\\Batch 25900\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:20] Epoch 12\\Batch 25950\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:23] Epoch 12\\Batch 26000\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:26] Epoch 12\\Batch 26050\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:28] Epoch 12\\Batch 26100\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:31] Epoch 12\\Batch 26150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:34] Epoch 12\\Batch 26200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:37] Epoch 12\\Batch 26250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:40] Epoch 12\\Batch 26300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:43] Epoch 12\\Batch 26350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:46] Epoch 12\\Batch 26400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:49] Epoch 12\\Batch 26450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:52] Epoch 12\\Batch 26500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:54] Epoch 12\\Batch 26550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:52:57] Epoch 12\\Batch 26600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:00] Epoch 12\\Batch 26650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:03] Epoch 12\\Batch 26700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:06] Epoch 12\\Batch 26750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:09] Epoch 12\\Batch 26800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:12] Epoch 12\\Batch 26850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:15] Epoch 12\\Batch 26900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:18] Epoch 12\\Batch 26950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:20] Epoch 12\\Batch 27000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5463.000001, 'TP': 1536.0000009999999, 'FP': 1652.0000009999999}\n",
      "[2019/03/18 08:53:36] Epoch 12/ Validation Loss:9.705/ F1_score:0.302/ Precision:0.482/ Recall:0.219\n",
      "[2019/03/18 08:53:39] Epoch 12\\Batch 27050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:42] Epoch 12\\Batch 27100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:44] Epoch 12\\Batch 27150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:47] Epoch 12\\Batch 27200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:50] Epoch 12\\Batch 27250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:53] Epoch 12\\Batch 27300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:56] Epoch 12\\Batch 27350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:53:59] Epoch 12\\Batch 27400\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:02] Epoch 12\\Batch 27450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:05] Epoch 12\\Batch 27500\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:08] Epoch 12\\Batch 27550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:10] Epoch 12\\Batch 27600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:13] Epoch 12\\Batch 27650\\ Train Loss:10.163\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:54:16] Epoch 12\\Batch 27700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:19] Epoch 12\\Batch 27750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:22] Epoch 12\\Batch 27800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:25] Epoch 12\\Batch 27850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:28] Epoch 12\\Batch 27900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:31] Epoch 12\\Batch 27950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:33] Epoch 12\\Batch 28000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:36] Epoch 12\\Batch 28050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:39] Epoch 12\\Batch 28100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:42] Epoch 12\\Batch 28150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:45] Epoch 12\\Batch 28200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:48] Epoch 12\\Batch 28250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:51] Epoch 12\\Batch 28300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:54] Epoch 12\\Batch 28350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:57] Epoch 12\\Batch 28400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:54:59] Epoch 12\\Batch 28450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:02] Epoch 12\\Batch 28500\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:05] Epoch 12\\Batch 28550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:08] Epoch 12\\Batch 28600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:11] Epoch 12\\Batch 28650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:14] Epoch 12\\Batch 28700\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:17] Epoch 12\\Batch 28750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:20] Epoch 12\\Batch 28800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:22] Epoch 12\\Batch 28850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:25] Epoch 12\\Batch 28900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:28] Epoch 12\\Batch 28950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:31] Epoch 12\\Batch 29000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:34] Epoch 12\\Batch 29050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:37] Epoch 12\\Batch 29100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:40] Epoch 12\\Batch 29150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:43] Epoch 12\\Batch 29200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:46] Epoch 12\\Batch 29250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:48] Epoch 12\\Batch 29300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:51] Epoch 12\\Batch 29350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:54] Epoch 12\\Batch 29400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:55:57] Epoch 12\\Batch 29450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:00] Epoch 12\\Batch 29500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:03] Epoch 12\\Batch 29550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:06] Epoch 12\\Batch 29600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:09] Epoch 12\\Batch 29650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:12] Epoch 12\\Batch 29700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:14] Epoch 12\\Batch 29750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:17] Epoch 12\\Batch 29800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:20] Epoch 12\\Batch 29850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:23] Epoch 12\\Batch 29900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:26] Epoch 12\\Batch 29950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:29] Epoch 12\\Batch 30000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5440.000001, 'TP': 1559.0000009999999, 'FP': 1634.0000009999999}\n",
      "[2019/03/18 08:56:44] Epoch 12/ Validation Loss:9.688/ F1_score:0.306/ Precision:0.488/ Recall:0.223\n",
      "[2019/03/18 08:56:47] Epoch 12\\Batch 30050\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:50] Epoch 12\\Batch 30100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:53] Epoch 12\\Batch 30150\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:56] Epoch 12\\Batch 30200\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:56:59] Epoch 12\\Batch 30250\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:02] Epoch 12\\Batch 30300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:05] Epoch 12\\Batch 30350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:08] Epoch 12\\Batch 30400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:10] Epoch 12\\Batch 30450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:13] Epoch 12\\Batch 30500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:16] Epoch 12\\Batch 30550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:19] Epoch 12\\Batch 30600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:22] Epoch 12\\Batch 30650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:25] Epoch 12\\Batch 30700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:28] Epoch 12\\Batch 30750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:31] Epoch 12\\Batch 30800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:34] Epoch 12\\Batch 30850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:36] Epoch 12\\Batch 30900\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:39] Epoch 12\\Batch 30950\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:42] Epoch 12\\Batch 31000\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:45] Epoch 12\\Batch 31050\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:48] Epoch 12\\Batch 31100\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:51] Epoch 12\\Batch 31150\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:54] Epoch 12\\Batch 31200\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:57] Epoch 12\\Batch 31250\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:57:59] Epoch 12\\Batch 31300\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:02] Epoch 12\\Batch 31350\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:05] Epoch 12\\Batch 31400\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:08] Epoch 12\\Batch 31450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:11] Epoch 12\\Batch 31500\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:14] Epoch 12\\Batch 31550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:17] Epoch 12\\Batch 31600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:20] Epoch 12\\Batch 31650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:23] Epoch 12\\Batch 31700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:25] Epoch 12\\Batch 31750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:28] Epoch 12\\Batch 31800\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:31] Epoch 12\\Batch 31850\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:34] Epoch 12\\Batch 31900\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:37] Epoch 12\\Batch 31950\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:40] Epoch 12\\Batch 32000\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:43] Epoch 12\\Batch 32050\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:46] Epoch 12\\Batch 32100\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:49] Epoch 12\\Batch 32150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:51] Epoch 12\\Batch 32200\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:54] Epoch 12\\Batch 32250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:58:57] Epoch 12\\Batch 32300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:00] Epoch 12\\Batch 32350\\ Train Loss:10.164\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 08:59:03] Epoch 12\\Batch 32400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:06] Epoch 12\\Batch 32450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:09] Epoch 12\\Batch 32500\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:12] Epoch 12\\Batch 32550\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:15] Epoch 12\\Batch 32600\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:17] Epoch 12\\Batch 32650\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:20] Epoch 12\\Batch 32700\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:23] Epoch 12\\Batch 32750\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:26] Epoch 12\\Batch 32800\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:29] Epoch 12\\Batch 32850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:32] Epoch 12\\Batch 32900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:35] Epoch 12\\Batch 32950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:38] Epoch 12\\Batch 33000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5443.000001, 'TP': 1556.0000009999999, 'FP': 1650.0000009999999}\n",
      "[2019/03/18 08:59:53] Epoch 12/ Validation Loss:9.706/ F1_score:0.305/ Precision:0.485/ Recall:0.222\n",
      "[2019/03/18 08:59:56] Epoch 12\\Batch 33050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 08:59:59] Epoch 12\\Batch 33100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:02] Epoch 12\\Batch 33150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:05] Epoch 12\\Batch 33200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:08] Epoch 12\\Batch 33250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:11] Epoch 12\\Batch 33300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:14] Epoch 12\\Batch 33350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:16] Epoch 12\\Batch 33400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:19] Epoch 12\\Batch 33450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:22] Epoch 12\\Batch 33500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:25] Epoch 12\\Batch 33550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:28] Epoch 12\\Batch 33600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:31] Epoch 12\\Batch 33650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:34] Epoch 12\\Batch 33700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:37] Epoch 12\\Batch 33750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:40] Epoch 12\\Batch 33800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:42] Epoch 12\\Batch 33850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:45] Epoch 12\\Batch 33900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:48] Epoch 12\\Batch 33950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:51] Epoch 12\\Batch 34000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:54] Epoch 12\\Batch 34050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:00:57] Epoch 12\\Batch 34100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:00] Epoch 12\\Batch 34150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:03] Epoch 12\\Batch 34200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:06] Epoch 12\\Batch 34250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:08] Epoch 12\\Batch 34300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:11] Epoch 12\\Batch 34350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:14] Epoch 12\\Batch 34400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:17] Epoch 12\\Batch 34450\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:20] Epoch 12\\Batch 34500\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:23] Epoch 12\\Batch 34550\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:26] Epoch 12\\Batch 34600\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:28] Epoch 12\\Batch 34650\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:31] Epoch 12\\Batch 34700\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:34] Epoch 12\\Batch 34750\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:37] Epoch 12\\Batch 34800\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:40] Epoch 12\\Batch 34850\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:43] Epoch 12\\Batch 34900\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:46] Epoch 12\\Batch 34950\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:49] Epoch 12\\Batch 35000\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:51] Epoch 12\\Batch 35050\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:54] Epoch 12\\Batch 35100\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:01:57] Epoch 12\\Batch 35150\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:00] Epoch 12\\Batch 35200\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:03] Epoch 12\\Batch 35250\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:06] Epoch 12\\Batch 35300\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:09] Epoch 12\\Batch 35350\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:12] Epoch 12\\Batch 35400\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:14] Epoch 12\\Batch 35450\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:17] Epoch 12\\Batch 35500\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:20] Epoch 12\\Batch 35550\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:23] Epoch 12\\Batch 35600\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:26] Epoch 12\\Batch 35650\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:29] Epoch 12\\Batch 35700\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:32] Epoch 12\\Batch 35750\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:34] Epoch 12\\Batch 35800\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:37] Epoch 12\\Batch 35850\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:40] Epoch 12\\Batch 35900\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:43] Epoch 12\\Batch 35950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:02:46] Epoch 12\\Batch 36000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5457.000001, 'TP': 1542.0000009999999, 'FP': 1649.0000009999999}\n",
      "[2019/03/18 09:03:01] Epoch 12/ Validation Loss:9.721/ F1_score:0.303/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 09:03:04] Epoch 12\\Batch 36050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:07] Epoch 12\\Batch 36100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:10] Epoch 12\\Batch 36150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:13] Epoch 12\\Batch 36200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:16] Epoch 12\\Batch 36250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:19] Epoch 12\\Batch 36300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:21] Epoch 12\\Batch 36350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:24] Epoch 12\\Batch 36400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:27] Epoch 12\\Batch 36450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:30] Epoch 12\\Batch 36500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:33] Epoch 12\\Batch 36550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:36] Epoch 12\\Batch 36600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:39] Epoch 12\\Batch 36650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:42] Epoch 12\\Batch 36700\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:44] Epoch 12\\Batch 36750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:47] Epoch 12\\Batch 36800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:50] Epoch 12\\Batch 36850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:53] Epoch 12\\Batch 36900\\ Train Loss:10.163\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:03:56] Epoch 12\\Batch 36950\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:03:59] Epoch 12\\Batch 37000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:02] Epoch 12\\Batch 37050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:05] Epoch 12\\Batch 37100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:07] Epoch 12\\Batch 37150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:10] Epoch 12\\Batch 37200\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:13] Epoch 12\\Batch 37250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:16] Epoch 12\\Batch 37300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:19] Epoch 12\\Batch 37350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:22] Epoch 12\\Batch 37400\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:25] Epoch 12\\Batch 37450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:28] Epoch 12\\Batch 37500\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:30] Epoch 12\\Batch 37550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:33] Epoch 12\\Batch 37600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:36] Epoch 12\\Batch 37650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:39] Epoch 12\\Batch 37700\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:42] Epoch 12\\Batch 37750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:45] Epoch 12\\Batch 37800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:48] Epoch 12\\Batch 37850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:50] Epoch 12\\Batch 37900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:53] Epoch 12\\Batch 37950\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:56] Epoch 12\\Batch 38000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:04:59] Epoch 12\\Batch 38050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:02] Epoch 12\\Batch 38100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:05] Epoch 12\\Batch 38150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:08] Epoch 12\\Batch 38200\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:11] Epoch 12\\Batch 38250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:13] Epoch 12\\Batch 38300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:16] Epoch 12\\Batch 38350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:19] Epoch 12\\Batch 38400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:22] Epoch 12\\Batch 38450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:25] Epoch 12\\Batch 38500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:28] Epoch 12\\Batch 38550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:31] Epoch 12\\Batch 38600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:34] Epoch 12\\Batch 38650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:36] Epoch 12\\Batch 38700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:39] Epoch 12\\Batch 38750\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:42] Epoch 12\\Batch 38800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:45] Epoch 12\\Batch 38850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:48] Epoch 12\\Batch 38900\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:51] Epoch 12\\Batch 38950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:05:54] Epoch 12\\Batch 39000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5420.000001, 'TP': 1579.0000009999999, 'FP': 1636.0000009999999}\n",
      "[2019/03/18 09:06:09] Epoch 12/ Validation Loss:9.729/ F1_score:0.309/ Precision:0.491/ Recall:0.226\n",
      "[2019/03/18 09:06:12] Epoch 12\\Batch 39050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:15] Epoch 12\\Batch 39100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:18] Epoch 12\\Batch 39150\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:21] Epoch 12\\Batch 39200\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:24] Epoch 12\\Batch 39250\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:27] Epoch 12\\Batch 39300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:29] Epoch 12\\Batch 39350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:32] Epoch 12\\Batch 39400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:35] Epoch 12\\Batch 39450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:38] Epoch 12\\Batch 39500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:41] Epoch 12\\Batch 39550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:44] Epoch 12\\Batch 39600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:47] Epoch 12\\Batch 39650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:50] Epoch 12\\Batch 39700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:52] Epoch 12\\Batch 39750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:55] Epoch 12\\Batch 39800\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:06:58] Epoch 12\\Batch 39850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:01] Epoch 12\\Batch 39900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:04] Epoch 12\\Batch 39950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:07] Epoch 12\\Batch 40000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:10] Epoch 12\\Batch 40050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:13] Epoch 12\\Batch 40100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:15] Epoch 12\\Batch 40150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:18] Epoch 12\\Batch 40200\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:21] Epoch 12\\Batch 40250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:24] Epoch 12\\Batch 40300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:27] Epoch 12\\Batch 40350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:30] Epoch 12\\Batch 40400\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:33] Epoch 12\\Batch 40450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:36] Epoch 12\\Batch 40500\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:38] Epoch 12\\Batch 40550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:41] Epoch 12\\Batch 40600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:44] Epoch 12\\Batch 40650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:47] Epoch 12\\Batch 40700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:50] Epoch 12\\Batch 40750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:53] Epoch 12\\Batch 40800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:56] Epoch 12\\Batch 40850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:07:58] Epoch 12\\Batch 40900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:01] Epoch 12\\Batch 40950\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:04] Epoch 12\\Batch 41000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:07] Epoch 12\\Batch 41050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:10] Epoch 12\\Batch 41100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:13] Epoch 12\\Batch 41150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:16] Epoch 12\\Batch 41200\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:19] Epoch 12\\Batch 41250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:21] Epoch 12\\Batch 41300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:24] Epoch 12\\Batch 41350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:27] Epoch 12\\Batch 41400\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:30] Epoch 12\\Batch 41450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:33] Epoch 12\\Batch 41500\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:36] Epoch 12\\Batch 41550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:39] Epoch 12\\Batch 41600\\ Train Loss:10.163\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:08:42] Epoch 12\\Batch 41650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:44] Epoch 12\\Batch 41700\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:47] Epoch 12\\Batch 41750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:50] Epoch 12\\Batch 41800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:53] Epoch 12\\Batch 41850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:56] Epoch 12\\Batch 41900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:08:59] Epoch 12\\Batch 41950\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:02] Epoch 12\\Batch 42000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5417.000001, 'TP': 1582.0000009999999, 'FP': 1624.0000009999999}\n",
      "[2019/03/18 09:09:17] Epoch 12/ Validation Loss:9.705/ F1_score:0.310/ Precision:0.493/ Recall:0.226\n",
      "[2019/03/18 09:09:20] Epoch 12\\Batch 42050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:23] Epoch 12\\Batch 42100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:26] Epoch 12\\Batch 42150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:29] Epoch 12\\Batch 42200\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:32] Epoch 12\\Batch 42250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:35] Epoch 12\\Batch 42300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:37] Epoch 12\\Batch 42350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:40] Epoch 12\\Batch 42400\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:43] Epoch 12\\Batch 42450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:46] Epoch 12\\Batch 42500\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:49] Epoch 12\\Batch 42550\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:52] Epoch 12\\Batch 42600\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:55] Epoch 12\\Batch 42650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:09:57] Epoch 12\\Batch 42700\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:00] Epoch 12\\Batch 42750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:03] Epoch 12\\Batch 42800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:06] Epoch 12\\Batch 42850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:09] Epoch 12\\Batch 42900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:12] Epoch 12\\Batch 42950\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:15] Epoch 12\\Batch 43000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:18] Epoch 12\\Batch 43050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:20] Epoch 12\\Batch 43100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:23] Epoch 12\\Batch 43150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:26] Epoch 12\\Batch 43200\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:29] Epoch 12\\Batch 43250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:32] Epoch 12\\Batch 43300\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:35] Epoch 12\\Batch 43350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:38] Epoch 12\\Batch 43400\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:41] Epoch 12\\Batch 43450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:43] Epoch 12\\Batch 43500\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:46] Epoch 12\\Batch 43550\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:49] Epoch 12\\Batch 43600\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:52] Epoch 12\\Batch 43650\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:55] Epoch 12\\Batch 43700\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:10:58] Epoch 12\\Batch 43750\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:01] Epoch 12\\Batch 43800\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:03] Epoch 12\\Batch 43850\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:06] Epoch 12\\Batch 43900\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:09] Epoch 12\\Batch 43950\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:12] Epoch 12\\Batch 44000\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:15] Epoch 12\\Batch 44050\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:18] Epoch 12\\Batch 44100\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:21] Epoch 12\\Batch 44150\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:23] Epoch 12\\Batch 44200\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:26] Epoch 12\\Batch 44250\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:29] Epoch 12\\Batch 44300\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:32] Epoch 12\\Batch 44350\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:35] Epoch 12\\Batch 44400\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:38] Epoch 12\\Batch 44450\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:41] Epoch 12\\Batch 44500\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:44] Epoch 12\\Batch 44550\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:46] Epoch 12\\Batch 44600\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:49] Epoch 12\\Batch 44650\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:52] Epoch 12\\Batch 44700\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:55] Epoch 12\\Batch 44750\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:11:58] Epoch 12\\Batch 44800\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:01] Epoch 12\\Batch 44850\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:04] Epoch 12\\Batch 44900\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:06] Epoch 12\\Batch 44950\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:09] Epoch 12\\Batch 45000\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5428.000001, 'TP': 1571.0000009999999, 'FP': 1633.0000009999999}\n",
      "[2019/03/18 09:12:25] Epoch 12/ Validation Loss:9.714/ F1_score:0.308/ Precision:0.490/ Recall:0.224\n",
      "[2019/03/18 09:12:28] Epoch 12\\Batch 45050\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:31] Epoch 12\\Batch 45100\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:33] Epoch 12\\Batch 45150\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:36] Epoch 12\\Batch 45200\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:39] Epoch 12\\Batch 45250\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:42] Epoch 12\\Batch 45300\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:45] Epoch 12\\Batch 45350\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:48] Epoch 12\\Batch 45400\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:51] Epoch 12\\Batch 45450\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:54] Epoch 12\\Batch 45500\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:56] Epoch 12\\Batch 45550\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:12:59] Epoch 12\\Batch 45600\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:02] Epoch 12\\Batch 45650\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:05] Epoch 12\\Batch 45700\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:08] Epoch 12\\Batch 45750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:11] Epoch 12\\Batch 45800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:14] Epoch 12\\Batch 45850\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:16] Epoch 12\\Batch 45900\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:19] Epoch 12\\Batch 45950\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:22] Epoch 12\\Batch 46000\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:25] Epoch 12\\Batch 46050\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:28] Epoch 12\\Batch 46100\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:31] Epoch 12\\Batch 46150\\ Train Loss:10.161\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:13:34] Epoch 12\\Batch 46200\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:39] Epoch 13\\Batch 50\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:42] Epoch 13\\Batch 100\\ Train Loss:10.204\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:45] Epoch 13\\Batch 150\\ Train Loss:10.250\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:48] Epoch 13\\Batch 200\\ Train Loss:10.206\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:51] Epoch 13\\Batch 250\\ Train Loss:10.205\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:54] Epoch 13\\Batch 300\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:57] Epoch 13\\Batch 350\\ Train Loss:10.218\\ Learning rate:0.00030\n",
      "[2019/03/18 09:13:59] Epoch 13\\Batch 400\\ Train Loss:10.223\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:02] Epoch 13\\Batch 450\\ Train Loss:10.207\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:05] Epoch 13\\Batch 500\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:08] Epoch 13\\Batch 550\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:11] Epoch 13\\Batch 600\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:14] Epoch 13\\Batch 650\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:17] Epoch 13\\Batch 700\\ Train Loss:10.181\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:20] Epoch 13\\Batch 750\\ Train Loss:10.180\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:22] Epoch 13\\Batch 800\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:25] Epoch 13\\Batch 850\\ Train Loss:10.177\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:28] Epoch 13\\Batch 900\\ Train Loss:10.184\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:31] Epoch 13\\Batch 950\\ Train Loss:10.176\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:34] Epoch 13\\Batch 1000\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:37] Epoch 13\\Batch 1050\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:40] Epoch 13\\Batch 1100\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:42] Epoch 13\\Batch 1150\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:45] Epoch 13\\Batch 1200\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:48] Epoch 13\\Batch 1250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:51] Epoch 13\\Batch 1300\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:54] Epoch 13\\Batch 1350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:14:57] Epoch 13\\Batch 1400\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:00] Epoch 13\\Batch 1450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:03] Epoch 13\\Batch 1500\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:05] Epoch 13\\Batch 1550\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:08] Epoch 13\\Batch 1600\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:11] Epoch 13\\Batch 1650\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:14] Epoch 13\\Batch 1700\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:17] Epoch 13\\Batch 1750\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:20] Epoch 13\\Batch 1800\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:23] Epoch 13\\Batch 1850\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:25] Epoch 13\\Batch 1900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:28] Epoch 13\\Batch 1950\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:31] Epoch 13\\Batch 2000\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:34] Epoch 13\\Batch 2050\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:37] Epoch 13\\Batch 2100\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:40] Epoch 13\\Batch 2150\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:43] Epoch 13\\Batch 2200\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:45] Epoch 13\\Batch 2250\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:48] Epoch 13\\Batch 2300\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:51] Epoch 13\\Batch 2350\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:54] Epoch 13\\Batch 2400\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 09:15:57] Epoch 13\\Batch 2450\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:00] Epoch 13\\Batch 2500\\ Train Loss:10.174\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:03] Epoch 13\\Batch 2550\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:06] Epoch 13\\Batch 2600\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:08] Epoch 13\\Batch 2650\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:11] Epoch 13\\Batch 2700\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:14] Epoch 13\\Batch 2750\\ Train Loss:10.175\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:17] Epoch 13\\Batch 2800\\ Train Loss:10.173\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:20] Epoch 13\\Batch 2850\\ Train Loss:10.172\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:23] Epoch 13\\Batch 2900\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:26] Epoch 13\\Batch 2950\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:28] Epoch 13\\Batch 3000\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5456.000001, 'TP': 1543.0000009999999, 'FP': 1659.0000009999999}\n",
      "[2019/03/18 09:16:44] Epoch 13/ Validation Loss:9.727/ F1_score:0.303/ Precision:0.482/ Recall:0.220\n",
      "[2019/03/18 09:16:47] Epoch 13\\Batch 3050\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:50] Epoch 13\\Batch 3100\\ Train Loss:10.171\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:53] Epoch 13\\Batch 3150\\ Train Loss:10.170\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:55] Epoch 13\\Batch 3200\\ Train Loss:10.169\\ Learning rate:0.00030\n",
      "[2019/03/18 09:16:58] Epoch 13\\Batch 3250\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:01] Epoch 13\\Batch 3300\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:04] Epoch 13\\Batch 3350\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:07] Epoch 13\\Batch 3400\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:10] Epoch 13\\Batch 3450\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:13] Epoch 13\\Batch 3500\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:16] Epoch 13\\Batch 3550\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:18] Epoch 13\\Batch 3600\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:21] Epoch 13\\Batch 3650\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:24] Epoch 13\\Batch 3700\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:27] Epoch 13\\Batch 3750\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:30] Epoch 13\\Batch 3800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:33] Epoch 13\\Batch 3850\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:36] Epoch 13\\Batch 3900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:38] Epoch 13\\Batch 3950\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:41] Epoch 13\\Batch 4000\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:44] Epoch 13\\Batch 4050\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:47] Epoch 13\\Batch 4100\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:50] Epoch 13\\Batch 4150\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:53] Epoch 13\\Batch 4200\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:56] Epoch 13\\Batch 4250\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:17:59] Epoch 13\\Batch 4300\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:01] Epoch 13\\Batch 4350\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:04] Epoch 13\\Batch 4400\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:07] Epoch 13\\Batch 4450\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:10] Epoch 13\\Batch 4500\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:13] Epoch 13\\Batch 4550\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:16] Epoch 13\\Batch 4600\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:19] Epoch 13\\Batch 4650\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:21] Epoch 13\\Batch 4700\\ Train Loss:10.158\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:18:24] Epoch 13\\Batch 4750\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:27] Epoch 13\\Batch 4800\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:30] Epoch 13\\Batch 4850\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:33] Epoch 13\\Batch 4900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:36] Epoch 13\\Batch 4950\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:39] Epoch 13\\Batch 5000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:42] Epoch 13\\Batch 5050\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:44] Epoch 13\\Batch 5100\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:47] Epoch 13\\Batch 5150\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:50] Epoch 13\\Batch 5200\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:53] Epoch 13\\Batch 5250\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:56] Epoch 13\\Batch 5300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:18:59] Epoch 13\\Batch 5350\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:02] Epoch 13\\Batch 5400\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:04] Epoch 13\\Batch 5450\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:07] Epoch 13\\Batch 5500\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:10] Epoch 13\\Batch 5550\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:13] Epoch 13\\Batch 5600\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:16] Epoch 13\\Batch 5650\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:19] Epoch 13\\Batch 5700\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:22] Epoch 13\\Batch 5750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:25] Epoch 13\\Batch 5800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:27] Epoch 13\\Batch 5850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:30] Epoch 13\\Batch 5900\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:33] Epoch 13\\Batch 5950\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:36] Epoch 13\\Batch 6000\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5438.000001, 'TP': 1561.0000009999999, 'FP': 1642.0000009999999}\n",
      "[2019/03/18 09:19:51] Epoch 13/ Validation Loss:9.740/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 09:19:54] Epoch 13\\Batch 6050\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:19:57] Epoch 13\\Batch 6100\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:00] Epoch 13\\Batch 6150\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:03] Epoch 13\\Batch 6200\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:06] Epoch 13\\Batch 6250\\ Train Loss:10.166\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:09] Epoch 13\\Batch 6300\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:12] Epoch 13\\Batch 6350\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:15] Epoch 13\\Batch 6400\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:18] Epoch 13\\Batch 6450\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:20] Epoch 13\\Batch 6500\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:23] Epoch 13\\Batch 6550\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:26] Epoch 13\\Batch 6600\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:29] Epoch 13\\Batch 6650\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:32] Epoch 13\\Batch 6700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:35] Epoch 13\\Batch 6750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:38] Epoch 13\\Batch 6800\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:40] Epoch 13\\Batch 6850\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:43] Epoch 13\\Batch 6900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:46] Epoch 13\\Batch 6950\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:49] Epoch 13\\Batch 7000\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:52] Epoch 13\\Batch 7050\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:55] Epoch 13\\Batch 7100\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:20:58] Epoch 13\\Batch 7150\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:00] Epoch 13\\Batch 7200\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:03] Epoch 13\\Batch 7250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:06] Epoch 13\\Batch 7300\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:09] Epoch 13\\Batch 7350\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:12] Epoch 13\\Batch 7400\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:15] Epoch 13\\Batch 7450\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:18] Epoch 13\\Batch 7500\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:21] Epoch 13\\Batch 7550\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:23] Epoch 13\\Batch 7600\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:26] Epoch 13\\Batch 7650\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:29] Epoch 13\\Batch 7700\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:32] Epoch 13\\Batch 7750\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:35] Epoch 13\\Batch 7800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:38] Epoch 13\\Batch 7850\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:41] Epoch 13\\Batch 7900\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:43] Epoch 13\\Batch 7950\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:46] Epoch 13\\Batch 8000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:49] Epoch 13\\Batch 8050\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:52] Epoch 13\\Batch 8100\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:55] Epoch 13\\Batch 8150\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:21:58] Epoch 13\\Batch 8200\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:01] Epoch 13\\Batch 8250\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:04] Epoch 13\\Batch 8300\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:07] Epoch 13\\Batch 8350\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:09] Epoch 13\\Batch 8400\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:12] Epoch 13\\Batch 8450\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:15] Epoch 13\\Batch 8500\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:18] Epoch 13\\Batch 8550\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:21] Epoch 13\\Batch 8600\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:24] Epoch 13\\Batch 8650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:27] Epoch 13\\Batch 8700\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:30] Epoch 13\\Batch 8750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:33] Epoch 13\\Batch 8800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:36] Epoch 13\\Batch 8850\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:39] Epoch 13\\Batch 8900\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:41] Epoch 13\\Batch 8950\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:22:44] Epoch 13\\Batch 9000\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5428.000001, 'TP': 1571.0000009999999, 'FP': 1632.0000009999999}\n",
      "[2019/03/18 09:23:00] Epoch 13/ Validation Loss:9.712/ F1_score:0.308/ Precision:0.490/ Recall:0.224\n",
      "[2019/03/18 09:23:03] Epoch 13\\Batch 9050\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:06] Epoch 13\\Batch 9100\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:09] Epoch 13\\Batch 9150\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:12] Epoch 13\\Batch 9200\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:14] Epoch 13\\Batch 9250\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:17] Epoch 13\\Batch 9300\\ Train Loss:10.162\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:23:20] Epoch 13\\Batch 9350\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:23] Epoch 13\\Batch 9400\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:26] Epoch 13\\Batch 9450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:29] Epoch 13\\Batch 9500\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:32] Epoch 13\\Batch 9550\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:35] Epoch 13\\Batch 9600\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:38] Epoch 13\\Batch 9650\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:41] Epoch 13\\Batch 9700\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:44] Epoch 13\\Batch 9750\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:47] Epoch 13\\Batch 9800\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:49] Epoch 13\\Batch 9850\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:52] Epoch 13\\Batch 9900\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:55] Epoch 13\\Batch 9950\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:23:58] Epoch 13\\Batch 10000\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:01] Epoch 13\\Batch 10050\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:04] Epoch 13\\Batch 10100\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:07] Epoch 13\\Batch 10150\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:10] Epoch 13\\Batch 10200\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:13] Epoch 13\\Batch 10250\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:16] Epoch 13\\Batch 10300\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:19] Epoch 13\\Batch 10350\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:22] Epoch 13\\Batch 10400\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:24] Epoch 13\\Batch 10450\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:27] Epoch 13\\Batch 10500\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:30] Epoch 13\\Batch 10550\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:33] Epoch 13\\Batch 10600\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:36] Epoch 13\\Batch 10650\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:39] Epoch 13\\Batch 10700\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:42] Epoch 13\\Batch 10750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:45] Epoch 13\\Batch 10800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:48] Epoch 13\\Batch 10850\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:51] Epoch 13\\Batch 10900\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:54] Epoch 13\\Batch 10950\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:56] Epoch 13\\Batch 11000\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:24:59] Epoch 13\\Batch 11050\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:02] Epoch 13\\Batch 11100\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:05] Epoch 13\\Batch 11150\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:08] Epoch 13\\Batch 11200\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:11] Epoch 13\\Batch 11250\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:14] Epoch 13\\Batch 11300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:17] Epoch 13\\Batch 11350\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:20] Epoch 13\\Batch 11400\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:23] Epoch 13\\Batch 11450\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:26] Epoch 13\\Batch 11500\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:29] Epoch 13\\Batch 11550\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:32] Epoch 13\\Batch 11600\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:34] Epoch 13\\Batch 11650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:37] Epoch 13\\Batch 11700\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:40] Epoch 13\\Batch 11750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:43] Epoch 13\\Batch 11800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:46] Epoch 13\\Batch 11850\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:49] Epoch 13\\Batch 11900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:52] Epoch 13\\Batch 11950\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:25:55] Epoch 13\\Batch 12000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5415.000001, 'TP': 1584.0000009999999, 'FP': 1646.0000009999999}\n",
      "[2019/03/18 09:26:10] Epoch 13/ Validation Loss:9.702/ F1_score:0.310/ Precision:0.490/ Recall:0.226\n",
      "[2019/03/18 09:26:13] Epoch 13\\Batch 12050\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:16] Epoch 13\\Batch 12100\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:19] Epoch 13\\Batch 12150\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:22] Epoch 13\\Batch 12200\\ Train Loss:10.162\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:25] Epoch 13\\Batch 12250\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:28] Epoch 13\\Batch 12300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:31] Epoch 13\\Batch 12350\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:34] Epoch 13\\Batch 12400\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:37] Epoch 13\\Batch 12450\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:40] Epoch 13\\Batch 12500\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:43] Epoch 13\\Batch 12550\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:45] Epoch 13\\Batch 12600\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:48] Epoch 13\\Batch 12650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:51] Epoch 13\\Batch 12700\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:54] Epoch 13\\Batch 12750\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:26:57] Epoch 13\\Batch 12800\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:00] Epoch 13\\Batch 12850\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:03] Epoch 13\\Batch 12900\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:06] Epoch 13\\Batch 12950\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:09] Epoch 13\\Batch 13000\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:12] Epoch 13\\Batch 13050\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:15] Epoch 13\\Batch 13100\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:17] Epoch 13\\Batch 13150\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:20] Epoch 13\\Batch 13200\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:23] Epoch 13\\Batch 13250\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:26] Epoch 13\\Batch 13300\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:29] Epoch 13\\Batch 13350\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:32] Epoch 13\\Batch 13400\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:35] Epoch 13\\Batch 13450\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:38] Epoch 13\\Batch 13500\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:41] Epoch 13\\Batch 13550\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:44] Epoch 13\\Batch 13600\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:47] Epoch 13\\Batch 13650\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:49] Epoch 13\\Batch 13700\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:52] Epoch 13\\Batch 13750\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:55] Epoch 13\\Batch 13800\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:27:58] Epoch 13\\Batch 13850\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:01] Epoch 13\\Batch 13900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:04] Epoch 13\\Batch 13950\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:07] Epoch 13\\Batch 14000\\ Train Loss:10.160\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:28:10] Epoch 13\\Batch 14050\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:13] Epoch 13\\Batch 14100\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:16] Epoch 13\\Batch 14150\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:19] Epoch 13\\Batch 14200\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:21] Epoch 13\\Batch 14250\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:24] Epoch 13\\Batch 14300\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:27] Epoch 13\\Batch 14350\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:30] Epoch 13\\Batch 14400\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:33] Epoch 13\\Batch 14450\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:36] Epoch 13\\Batch 14500\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:39] Epoch 13\\Batch 14550\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:42] Epoch 13\\Batch 14600\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:45] Epoch 13\\Batch 14650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:48] Epoch 13\\Batch 14700\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:51] Epoch 13\\Batch 14750\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:53] Epoch 13\\Batch 14800\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:56] Epoch 13\\Batch 14850\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:28:59] Epoch 13\\Batch 14900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:02] Epoch 13\\Batch 14950\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:05] Epoch 13\\Batch 15000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5449.000001, 'TP': 1550.0000009999999, 'FP': 1636.0000009999999}\n",
      "[2019/03/18 09:29:20] Epoch 13/ Validation Loss:9.701/ F1_score:0.304/ Precision:0.487/ Recall:0.221\n",
      "[2019/03/18 09:29:23] Epoch 13\\Batch 15050\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:26] Epoch 13\\Batch 15100\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:29] Epoch 13\\Batch 15150\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:32] Epoch 13\\Batch 15200\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:35] Epoch 13\\Batch 15250\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:38] Epoch 13\\Batch 15300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:41] Epoch 13\\Batch 15350\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:44] Epoch 13\\Batch 15400\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:47] Epoch 13\\Batch 15450\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:50] Epoch 13\\Batch 15500\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:52] Epoch 13\\Batch 15550\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:55] Epoch 13\\Batch 15600\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:29:58] Epoch 13\\Batch 15650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:01] Epoch 13\\Batch 15700\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:04] Epoch 13\\Batch 15750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:07] Epoch 13\\Batch 15800\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:10] Epoch 13\\Batch 15850\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:13] Epoch 13\\Batch 15900\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:16] Epoch 13\\Batch 15950\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:19] Epoch 13\\Batch 16000\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:21] Epoch 13\\Batch 16050\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:24] Epoch 13\\Batch 16100\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:27] Epoch 13\\Batch 16150\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:30] Epoch 13\\Batch 16200\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:33] Epoch 13\\Batch 16250\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:36] Epoch 13\\Batch 16300\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:39] Epoch 13\\Batch 16350\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:42] Epoch 13\\Batch 16400\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:45] Epoch 13\\Batch 16450\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:48] Epoch 13\\Batch 16500\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:51] Epoch 13\\Batch 16550\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:53] Epoch 13\\Batch 16600\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:56] Epoch 13\\Batch 16650\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:30:59] Epoch 13\\Batch 16700\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:02] Epoch 13\\Batch 16750\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:05] Epoch 13\\Batch 16800\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:08] Epoch 13\\Batch 16850\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:11] Epoch 13\\Batch 16900\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:14] Epoch 13\\Batch 16950\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:17] Epoch 13\\Batch 17000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:20] Epoch 13\\Batch 17050\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:23] Epoch 13\\Batch 17100\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:25] Epoch 13\\Batch 17150\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:28] Epoch 13\\Batch 17200\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:31] Epoch 13\\Batch 17250\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:34] Epoch 13\\Batch 17300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:37] Epoch 13\\Batch 17350\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:40] Epoch 13\\Batch 17400\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:43] Epoch 13\\Batch 17450\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:46] Epoch 13\\Batch 17500\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:49] Epoch 13\\Batch 17550\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:52] Epoch 13\\Batch 17600\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:55] Epoch 13\\Batch 17650\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:31:57] Epoch 13\\Batch 17700\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:00] Epoch 13\\Batch 17750\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:03] Epoch 13\\Batch 17800\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:06] Epoch 13\\Batch 17850\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:09] Epoch 13\\Batch 17900\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:12] Epoch 13\\Batch 17950\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:15] Epoch 13\\Batch 18000\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5438.000001, 'TP': 1561.0000009999999, 'FP': 1639.0000009999999}\n",
      "[2019/03/18 09:32:30] Epoch 13/ Validation Loss:9.710/ F1_score:0.306/ Precision:0.488/ Recall:0.223\n",
      "[2019/03/18 09:32:33] Epoch 13\\Batch 18050\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:36] Epoch 13\\Batch 18100\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:39] Epoch 13\\Batch 18150\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:42] Epoch 13\\Batch 18200\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:45] Epoch 13\\Batch 18250\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:48] Epoch 13\\Batch 18300\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:51] Epoch 13\\Batch 18350\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:54] Epoch 13\\Batch 18400\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:32:57] Epoch 13\\Batch 18450\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:00] Epoch 13\\Batch 18500\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:02] Epoch 13\\Batch 18550\\ Train Loss:10.157\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:33:05] Epoch 13\\Batch 18600\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:08] Epoch 13\\Batch 18650\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:11] Epoch 13\\Batch 18700\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:14] Epoch 13\\Batch 18750\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:17] Epoch 13\\Batch 18800\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:20] Epoch 13\\Batch 18850\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:23] Epoch 13\\Batch 18900\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:26] Epoch 13\\Batch 18950\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:29] Epoch 13\\Batch 19000\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:32] Epoch 13\\Batch 19050\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:34] Epoch 13\\Batch 19100\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:37] Epoch 13\\Batch 19150\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:40] Epoch 13\\Batch 19200\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:43] Epoch 13\\Batch 19250\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:46] Epoch 13\\Batch 19300\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:49] Epoch 13\\Batch 19350\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:52] Epoch 13\\Batch 19400\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:55] Epoch 13\\Batch 19450\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:33:58] Epoch 13\\Batch 19500\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:01] Epoch 13\\Batch 19550\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:04] Epoch 13\\Batch 19600\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:06] Epoch 13\\Batch 19650\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:09] Epoch 13\\Batch 19700\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:12] Epoch 13\\Batch 19750\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:15] Epoch 13\\Batch 19800\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:18] Epoch 13\\Batch 19850\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:21] Epoch 13\\Batch 19900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:24] Epoch 13\\Batch 19950\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:27] Epoch 13\\Batch 20000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:30] Epoch 13\\Batch 20050\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:33] Epoch 13\\Batch 20100\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:36] Epoch 13\\Batch 20150\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:38] Epoch 13\\Batch 20200\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:41] Epoch 13\\Batch 20250\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:44] Epoch 13\\Batch 20300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:47] Epoch 13\\Batch 20350\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:50] Epoch 13\\Batch 20400\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:53] Epoch 13\\Batch 20450\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:56] Epoch 13\\Batch 20500\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:34:59] Epoch 13\\Batch 20550\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:02] Epoch 13\\Batch 20600\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:05] Epoch 13\\Batch 20650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:08] Epoch 13\\Batch 20700\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:11] Epoch 13\\Batch 20750\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:13] Epoch 13\\Batch 20800\\ Train Loss:10.161\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:16] Epoch 13\\Batch 20850\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:19] Epoch 13\\Batch 20900\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:22] Epoch 13\\Batch 20950\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:25] Epoch 13\\Batch 21000\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572, 793]\n",
      "{'FN': 5444.000001, 'TP': 1555.0000009999999, 'FP': 1637.0000009999999}\n",
      "[2019/03/18 09:35:41] Epoch 13/ Validation Loss:9.709/ F1_score:0.305/ Precision:0.487/ Recall:0.222\n",
      "[2019/03/18 09:35:44] Epoch 13\\Batch 21050\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:47] Epoch 13\\Batch 21100\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:49] Epoch 13\\Batch 21150\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:52] Epoch 13\\Batch 21200\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:55] Epoch 13\\Batch 21250\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:35:58] Epoch 13\\Batch 21300\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:01] Epoch 13\\Batch 21350\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:04] Epoch 13\\Batch 21400\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:07] Epoch 13\\Batch 21450\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:10] Epoch 13\\Batch 21500\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:13] Epoch 13\\Batch 21550\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:16] Epoch 13\\Batch 21600\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:19] Epoch 13\\Batch 21650\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:22] Epoch 13\\Batch 21700\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:24] Epoch 13\\Batch 21750\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:27] Epoch 13\\Batch 21800\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:30] Epoch 13\\Batch 21850\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:33] Epoch 13\\Batch 21900\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:36] Epoch 13\\Batch 21950\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:39] Epoch 13\\Batch 22000\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:42] Epoch 13\\Batch 22050\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:45] Epoch 13\\Batch 22100\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:48] Epoch 13\\Batch 22150\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:51] Epoch 13\\Batch 22200\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:53] Epoch 13\\Batch 22250\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:56] Epoch 13\\Batch 22300\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:36:59] Epoch 13\\Batch 22350\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:02] Epoch 13\\Batch 22400\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:05] Epoch 13\\Batch 22450\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:08] Epoch 13\\Batch 22500\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:11] Epoch 13\\Batch 22550\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:14] Epoch 13\\Batch 22600\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:17] Epoch 13\\Batch 22650\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:20] Epoch 13\\Batch 22700\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:22] Epoch 13\\Batch 22750\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:25] Epoch 13\\Batch 22800\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:28] Epoch 13\\Batch 22850\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:31] Epoch 13\\Batch 22900\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:34] Epoch 13\\Batch 22950\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:37] Epoch 13\\Batch 23000\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:40] Epoch 13\\Batch 23050\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:43] Epoch 13\\Batch 23100\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:45] Epoch 13\\Batch 23150\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:48] Epoch 13\\Batch 23200\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:51] Epoch 13\\Batch 23250\\ Train Loss:10.158\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:37:54] Epoch 13\\Batch 23300\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:37:57] Epoch 13\\Batch 23350\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:00] Epoch 13\\Batch 23400\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:03] Epoch 13\\Batch 23450\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:05] Epoch 13\\Batch 23500\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:08] Epoch 13\\Batch 23550\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:11] Epoch 13\\Batch 23600\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:14] Epoch 13\\Batch 23650\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:17] Epoch 13\\Batch 23700\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:20] Epoch 13\\Batch 23750\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:23] Epoch 13\\Batch 23800\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:26] Epoch 13\\Batch 23850\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:28] Epoch 13\\Batch 23900\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:31] Epoch 13\\Batch 23950\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:34] Epoch 13\\Batch 24000\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5416.000001, 'TP': 1583.0000009999999, 'FP': 1624.0000009999999}\n",
      "[2019/03/18 09:38:50] Epoch 13/ Validation Loss:9.671/ F1_score:0.310/ Precision:0.494/ Recall:0.226\n",
      "[2019/03/18 09:38:53] Epoch 13\\Batch 24050\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:56] Epoch 13\\Batch 24100\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:38:58] Epoch 13\\Batch 24150\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:01] Epoch 13\\Batch 24200\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:04] Epoch 13\\Batch 24250\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:07] Epoch 13\\Batch 24300\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:10] Epoch 13\\Batch 24350\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:13] Epoch 13\\Batch 24400\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:16] Epoch 13\\Batch 24450\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:19] Epoch 13\\Batch 24500\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:22] Epoch 13\\Batch 24550\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:24] Epoch 13\\Batch 24600\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:27] Epoch 13\\Batch 24650\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:30] Epoch 13\\Batch 24700\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:33] Epoch 13\\Batch 24750\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:36] Epoch 13\\Batch 24800\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:39] Epoch 13\\Batch 24850\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:42] Epoch 13\\Batch 24900\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:45] Epoch 13\\Batch 24950\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:47] Epoch 13\\Batch 25000\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:50] Epoch 13\\Batch 25050\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:53] Epoch 13\\Batch 25100\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:56] Epoch 13\\Batch 25150\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:39:59] Epoch 13\\Batch 25200\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:02] Epoch 13\\Batch 25250\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:05] Epoch 13\\Batch 25300\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:08] Epoch 13\\Batch 25350\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:10] Epoch 13\\Batch 25400\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:13] Epoch 13\\Batch 25450\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:16] Epoch 13\\Batch 25500\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:19] Epoch 13\\Batch 25550\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:22] Epoch 13\\Batch 25600\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:25] Epoch 13\\Batch 25650\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:28] Epoch 13\\Batch 25700\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:31] Epoch 13\\Batch 25750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:33] Epoch 13\\Batch 25800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:36] Epoch 13\\Batch 25850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:39] Epoch 13\\Batch 25900\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:42] Epoch 13\\Batch 25950\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:45] Epoch 13\\Batch 26000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:48] Epoch 13\\Batch 26050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:51] Epoch 13\\Batch 26100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:54] Epoch 13\\Batch 26150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:56] Epoch 13\\Batch 26200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:40:59] Epoch 13\\Batch 26250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:02] Epoch 13\\Batch 26300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:05] Epoch 13\\Batch 26350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:08] Epoch 13\\Batch 26400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:11] Epoch 13\\Batch 26450\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:14] Epoch 13\\Batch 26500\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:17] Epoch 13\\Batch 26550\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:19] Epoch 13\\Batch 26600\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:22] Epoch 13\\Batch 26650\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:25] Epoch 13\\Batch 26700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:28] Epoch 13\\Batch 26750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:31] Epoch 13\\Batch 26800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:34] Epoch 13\\Batch 26850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:37] Epoch 13\\Batch 26900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:40] Epoch 13\\Batch 26950\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:41:42] Epoch 13\\Batch 27000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5441.000001, 'TP': 1558.0000009999999, 'FP': 1639.0000009999999}\n",
      "[2019/03/18 09:41:58] Epoch 13/ Validation Loss:9.700/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 09:42:01] Epoch 13\\Batch 27050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:04] Epoch 13\\Batch 27100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:07] Epoch 13\\Batch 27150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:09] Epoch 13\\Batch 27200\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:12] Epoch 13\\Batch 27250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:15] Epoch 13\\Batch 27300\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:18] Epoch 13\\Batch 27350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:21] Epoch 13\\Batch 27400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:24] Epoch 13\\Batch 27450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:27] Epoch 13\\Batch 27500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:29] Epoch 13\\Batch 27550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:32] Epoch 13\\Batch 27600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:35] Epoch 13\\Batch 27650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:38] Epoch 13\\Batch 27700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:41] Epoch 13\\Batch 27750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:44] Epoch 13\\Batch 27800\\ Train Loss:10.153\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:42:47] Epoch 13\\Batch 27850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:50] Epoch 13\\Batch 27900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:52] Epoch 13\\Batch 27950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:55] Epoch 13\\Batch 28000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:42:58] Epoch 13\\Batch 28050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:01] Epoch 13\\Batch 28100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:04] Epoch 13\\Batch 28150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:07] Epoch 13\\Batch 28200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:10] Epoch 13\\Batch 28250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:13] Epoch 13\\Batch 28300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:15] Epoch 13\\Batch 28350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:18] Epoch 13\\Batch 28400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:21] Epoch 13\\Batch 28450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:24] Epoch 13\\Batch 28500\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:27] Epoch 13\\Batch 28550\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:30] Epoch 13\\Batch 28600\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:33] Epoch 13\\Batch 28650\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:36] Epoch 13\\Batch 28700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:39] Epoch 13\\Batch 28750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:41] Epoch 13\\Batch 28800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:44] Epoch 13\\Batch 28850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:47] Epoch 13\\Batch 28900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:50] Epoch 13\\Batch 28950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:53] Epoch 13\\Batch 29000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:56] Epoch 13\\Batch 29050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:43:59] Epoch 13\\Batch 29100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:02] Epoch 13\\Batch 29150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:04] Epoch 13\\Batch 29200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:07] Epoch 13\\Batch 29250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:10] Epoch 13\\Batch 29300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:13] Epoch 13\\Batch 29350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:16] Epoch 13\\Batch 29400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:19] Epoch 13\\Batch 29450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:22] Epoch 13\\Batch 29500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:24] Epoch 13\\Batch 29550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:27] Epoch 13\\Batch 29600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:30] Epoch 13\\Batch 29650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:33] Epoch 13\\Batch 29700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:36] Epoch 13\\Batch 29750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:39] Epoch 13\\Batch 29800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:42] Epoch 13\\Batch 29850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:45] Epoch 13\\Batch 29900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:47] Epoch 13\\Batch 29950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:44:50] Epoch 13\\Batch 30000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5426.000001, 'TP': 1573.0000009999999, 'FP': 1633.0000009999999}\n",
      "[2019/03/18 09:45:06] Epoch 13/ Validation Loss:9.690/ F1_score:0.308/ Precision:0.491/ Recall:0.225\n",
      "[2019/03/18 09:45:09] Epoch 13\\Batch 30050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:12] Epoch 13\\Batch 30100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:14] Epoch 13\\Batch 30150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:17] Epoch 13\\Batch 30200\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:20] Epoch 13\\Batch 30250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:23] Epoch 13\\Batch 30300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:26] Epoch 13\\Batch 30350\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:29] Epoch 13\\Batch 30400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:32] Epoch 13\\Batch 30450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:35] Epoch 13\\Batch 30500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:37] Epoch 13\\Batch 30550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:40] Epoch 13\\Batch 30600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:43] Epoch 13\\Batch 30650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:46] Epoch 13\\Batch 30700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:49] Epoch 13\\Batch 30750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:52] Epoch 13\\Batch 30800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:55] Epoch 13\\Batch 30850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:45:58] Epoch 13\\Batch 30900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:00] Epoch 13\\Batch 30950\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:03] Epoch 13\\Batch 31000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:06] Epoch 13\\Batch 31050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:09] Epoch 13\\Batch 31100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:12] Epoch 13\\Batch 31150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:15] Epoch 13\\Batch 31200\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:18] Epoch 13\\Batch 31250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:20] Epoch 13\\Batch 31300\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:23] Epoch 13\\Batch 31350\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:26] Epoch 13\\Batch 31400\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:29] Epoch 13\\Batch 31450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:32] Epoch 13\\Batch 31500\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:35] Epoch 13\\Batch 31550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:38] Epoch 13\\Batch 31600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:40] Epoch 13\\Batch 31650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:43] Epoch 13\\Batch 31700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:46] Epoch 13\\Batch 31750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:49] Epoch 13\\Batch 31800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:52] Epoch 13\\Batch 31850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:55] Epoch 13\\Batch 31900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:46:58] Epoch 13\\Batch 31950\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:00] Epoch 13\\Batch 32000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:03] Epoch 13\\Batch 32050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:06] Epoch 13\\Batch 32100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:09] Epoch 13\\Batch 32150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:12] Epoch 13\\Batch 32200\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:15] Epoch 13\\Batch 32250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:18] Epoch 13\\Batch 32300\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:20] Epoch 13\\Batch 32350\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:23] Epoch 13\\Batch 32400\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:26] Epoch 13\\Batch 32450\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:29] Epoch 13\\Batch 32500\\ Train Loss:10.153\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:47:32] Epoch 13\\Batch 32550\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:35] Epoch 13\\Batch 32600\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:38] Epoch 13\\Batch 32650\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:41] Epoch 13\\Batch 32700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:43] Epoch 13\\Batch 32750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:46] Epoch 13\\Batch 32800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:49] Epoch 13\\Batch 32850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:52] Epoch 13\\Batch 32900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:55] Epoch 13\\Batch 32950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:47:58] Epoch 13\\Batch 33000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5433.000001, 'TP': 1566.0000009999999, 'FP': 1653.0000009999999}\n",
      "[2019/03/18 09:48:13] Epoch 13/ Validation Loss:9.703/ F1_score:0.307/ Precision:0.486/ Recall:0.224\n",
      "[2019/03/18 09:48:16] Epoch 13\\Batch 33050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:19] Epoch 13\\Batch 33100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:22] Epoch 13\\Batch 33150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:25] Epoch 13\\Batch 33200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:28] Epoch 13\\Batch 33250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:30] Epoch 13\\Batch 33300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:33] Epoch 13\\Batch 33350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:36] Epoch 13\\Batch 33400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:39] Epoch 13\\Batch 33450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:42] Epoch 13\\Batch 33500\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:45] Epoch 13\\Batch 33550\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:48] Epoch 13\\Batch 33600\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:50] Epoch 13\\Batch 33650\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:53] Epoch 13\\Batch 33700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:56] Epoch 13\\Batch 33750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:48:59] Epoch 13\\Batch 33800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:02] Epoch 13\\Batch 33850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:05] Epoch 13\\Batch 33900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:08] Epoch 13\\Batch 33950\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:11] Epoch 13\\Batch 34000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:13] Epoch 13\\Batch 34050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:16] Epoch 13\\Batch 34100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:19] Epoch 13\\Batch 34150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:22] Epoch 13\\Batch 34200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:25] Epoch 13\\Batch 34250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:28] Epoch 13\\Batch 34300\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:31] Epoch 13\\Batch 34350\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:33] Epoch 13\\Batch 34400\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:36] Epoch 13\\Batch 34450\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:39] Epoch 13\\Batch 34500\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:42] Epoch 13\\Batch 34550\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:45] Epoch 13\\Batch 34600\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:48] Epoch 13\\Batch 34650\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:51] Epoch 13\\Batch 34700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:53] Epoch 13\\Batch 34750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:56] Epoch 13\\Batch 34800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:49:59] Epoch 13\\Batch 34850\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:02] Epoch 13\\Batch 34900\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:05] Epoch 13\\Batch 34950\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:08] Epoch 13\\Batch 35000\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:11] Epoch 13\\Batch 35050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:14] Epoch 13\\Batch 35100\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:16] Epoch 13\\Batch 35150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:19] Epoch 13\\Batch 35200\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:22] Epoch 13\\Batch 35250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:25] Epoch 13\\Batch 35300\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:28] Epoch 13\\Batch 35350\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:31] Epoch 13\\Batch 35400\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:34] Epoch 13\\Batch 35450\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:36] Epoch 13\\Batch 35500\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:39] Epoch 13\\Batch 35550\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:42] Epoch 13\\Batch 35600\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:45] Epoch 13\\Batch 35650\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:48] Epoch 13\\Batch 35700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:51] Epoch 13\\Batch 35750\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:54] Epoch 13\\Batch 35800\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:57] Epoch 13\\Batch 35850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:50:59] Epoch 13\\Batch 35900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:02] Epoch 13\\Batch 35950\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:05] Epoch 13\\Batch 36000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5425.000001, 'TP': 1574.0000009999999, 'FP': 1633.0000009999999}\n",
      "[2019/03/18 09:51:20] Epoch 13/ Validation Loss:9.701/ F1_score:0.308/ Precision:0.491/ Recall:0.225\n",
      "[2019/03/18 09:51:23] Epoch 13\\Batch 36050\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:26] Epoch 13\\Batch 36100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:29] Epoch 13\\Batch 36150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:32] Epoch 13\\Batch 36200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:35] Epoch 13\\Batch 36250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:38] Epoch 13\\Batch 36300\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:41] Epoch 13\\Batch 36350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:43] Epoch 13\\Batch 36400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:46] Epoch 13\\Batch 36450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:49] Epoch 13\\Batch 36500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:52] Epoch 13\\Batch 36550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:55] Epoch 13\\Batch 36600\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:51:58] Epoch 13\\Batch 36650\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:01] Epoch 13\\Batch 36700\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:04] Epoch 13\\Batch 36750\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:06] Epoch 13\\Batch 36800\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:09] Epoch 13\\Batch 36850\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:12] Epoch 13\\Batch 36900\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:15] Epoch 13\\Batch 36950\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:18] Epoch 13\\Batch 37000\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:21] Epoch 13\\Batch 37050\\ Train Loss:10.151\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:52:24] Epoch 13\\Batch 37100\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:26] Epoch 13\\Batch 37150\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:29] Epoch 13\\Batch 37200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:32] Epoch 13\\Batch 37250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:35] Epoch 13\\Batch 37300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:38] Epoch 13\\Batch 37350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:41] Epoch 13\\Batch 37400\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:44] Epoch 13\\Batch 37450\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:46] Epoch 13\\Batch 37500\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:49] Epoch 13\\Batch 37550\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:52] Epoch 13\\Batch 37600\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:55] Epoch 13\\Batch 37650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:52:58] Epoch 13\\Batch 37700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:01] Epoch 13\\Batch 37750\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:04] Epoch 13\\Batch 37800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:07] Epoch 13\\Batch 37850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:09] Epoch 13\\Batch 37900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:12] Epoch 13\\Batch 37950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:15] Epoch 13\\Batch 38000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:18] Epoch 13\\Batch 38050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:21] Epoch 13\\Batch 38100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:24] Epoch 13\\Batch 38150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:27] Epoch 13\\Batch 38200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:29] Epoch 13\\Batch 38250\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:32] Epoch 13\\Batch 38300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:35] Epoch 13\\Batch 38350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:38] Epoch 13\\Batch 38400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:41] Epoch 13\\Batch 38450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:44] Epoch 13\\Batch 38500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:47] Epoch 13\\Batch 38550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:49] Epoch 13\\Batch 38600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:52] Epoch 13\\Batch 38650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:55] Epoch 13\\Batch 38700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:53:58] Epoch 13\\Batch 38750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:01] Epoch 13\\Batch 38800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:04] Epoch 13\\Batch 38850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:07] Epoch 13\\Batch 38900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:09] Epoch 13\\Batch 38950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:12] Epoch 13\\Batch 39000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5420.000001, 'TP': 1579.0000009999999, 'FP': 1644.0000009999999}\n",
      "[2019/03/18 09:54:28] Epoch 13/ Validation Loss:9.714/ F1_score:0.309/ Precision:0.490/ Recall:0.226\n",
      "[2019/03/18 09:54:31] Epoch 13\\Batch 39050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:34] Epoch 13\\Batch 39100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:37] Epoch 13\\Batch 39150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:39] Epoch 13\\Batch 39200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:42] Epoch 13\\Batch 39250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:45] Epoch 13\\Batch 39300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:48] Epoch 13\\Batch 39350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:51] Epoch 13\\Batch 39400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:54] Epoch 13\\Batch 39450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:54:57] Epoch 13\\Batch 39500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:00] Epoch 13\\Batch 39550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:03] Epoch 13\\Batch 39600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:05] Epoch 13\\Batch 39650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:08] Epoch 13\\Batch 39700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:11] Epoch 13\\Batch 39750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:14] Epoch 13\\Batch 39800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:17] Epoch 13\\Batch 39850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:20] Epoch 13\\Batch 39900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:23] Epoch 13\\Batch 39950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:26] Epoch 13\\Batch 40000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:28] Epoch 13\\Batch 40050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:31] Epoch 13\\Batch 40100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:34] Epoch 13\\Batch 40150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:37] Epoch 13\\Batch 40200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:40] Epoch 13\\Batch 40250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:43] Epoch 13\\Batch 40300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:46] Epoch 13\\Batch 40350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:48] Epoch 13\\Batch 40400\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:51] Epoch 13\\Batch 40450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:54] Epoch 13\\Batch 40500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:55:57] Epoch 13\\Batch 40550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:00] Epoch 13\\Batch 40600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:03] Epoch 13\\Batch 40650\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:06] Epoch 13\\Batch 40700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:08] Epoch 13\\Batch 40750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:11] Epoch 13\\Batch 40800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:14] Epoch 13\\Batch 40850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:17] Epoch 13\\Batch 40900\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:20] Epoch 13\\Batch 40950\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:23] Epoch 13\\Batch 41000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:26] Epoch 13\\Batch 41050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:28] Epoch 13\\Batch 41100\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:31] Epoch 13\\Batch 41150\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:34] Epoch 13\\Batch 41200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:37] Epoch 13\\Batch 41250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:40] Epoch 13\\Batch 41300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:43] Epoch 13\\Batch 41350\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:46] Epoch 13\\Batch 41400\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:49] Epoch 13\\Batch 41450\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:51] Epoch 13\\Batch 41500\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:54] Epoch 13\\Batch 41550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:56:57] Epoch 13\\Batch 41600\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:00] Epoch 13\\Batch 41650\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:03] Epoch 13\\Batch 41700\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:06] Epoch 13\\Batch 41750\\ Train Loss:10.152\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 09:57:09] Epoch 13\\Batch 41800\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:11] Epoch 13\\Batch 41850\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:14] Epoch 13\\Batch 41900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:17] Epoch 13\\Batch 41950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:20] Epoch 13\\Batch 42000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5404.000001, 'TP': 1595.0000009999999, 'FP': 1635.0000009999999}\n",
      "[2019/03/18 09:57:35] Epoch 13/ Validation Loss:9.700/ F1_score:0.312/ Precision:0.494/ Recall:0.228\n",
      "[2019/03/18 09:57:38] Epoch 13\\Batch 42050\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:41] Epoch 13\\Batch 42100\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:44] Epoch 13\\Batch 42150\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:47] Epoch 13\\Batch 42200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:50] Epoch 13\\Batch 42250\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:53] Epoch 13\\Batch 42300\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:56] Epoch 13\\Batch 42350\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:57:58] Epoch 13\\Batch 42400\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:01] Epoch 13\\Batch 42450\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:04] Epoch 13\\Batch 42500\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:07] Epoch 13\\Batch 42550\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:10] Epoch 13\\Batch 42600\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:13] Epoch 13\\Batch 42650\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:16] Epoch 13\\Batch 42700\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:19] Epoch 13\\Batch 42750\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:21] Epoch 13\\Batch 42800\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:24] Epoch 13\\Batch 42850\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:27] Epoch 13\\Batch 42900\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:30] Epoch 13\\Batch 42950\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:33] Epoch 13\\Batch 43000\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:36] Epoch 13\\Batch 43050\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:39] Epoch 13\\Batch 43100\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:42] Epoch 13\\Batch 43150\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:44] Epoch 13\\Batch 43200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:47] Epoch 13\\Batch 43250\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:50] Epoch 13\\Batch 43300\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:53] Epoch 13\\Batch 43350\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:56] Epoch 13\\Batch 43400\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:58:59] Epoch 13\\Batch 43450\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:02] Epoch 13\\Batch 43500\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:05] Epoch 13\\Batch 43550\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:07] Epoch 13\\Batch 43600\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:10] Epoch 13\\Batch 43650\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:13] Epoch 13\\Batch 43700\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:16] Epoch 13\\Batch 43750\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:19] Epoch 13\\Batch 43800\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:22] Epoch 13\\Batch 43850\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:25] Epoch 13\\Batch 43900\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:27] Epoch 13\\Batch 43950\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:30] Epoch 13\\Batch 44000\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:33] Epoch 13\\Batch 44050\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:36] Epoch 13\\Batch 44100\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:39] Epoch 13\\Batch 44150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:42] Epoch 13\\Batch 44200\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:45] Epoch 13\\Batch 44250\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:48] Epoch 13\\Batch 44300\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:50] Epoch 13\\Batch 44350\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:53] Epoch 13\\Batch 44400\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:56] Epoch 13\\Batch 44450\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 09:59:59] Epoch 13\\Batch 44500\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:02] Epoch 13\\Batch 44550\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:05] Epoch 13\\Batch 44600\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:08] Epoch 13\\Batch 44650\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:11] Epoch 13\\Batch 44700\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:13] Epoch 13\\Batch 44750\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:16] Epoch 13\\Batch 44800\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:19] Epoch 13\\Batch 44850\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:22] Epoch 13\\Batch 44900\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:25] Epoch 13\\Batch 44950\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:28] Epoch 13\\Batch 45000\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5436.000001, 'TP': 1563.0000009999999, 'FP': 1645.0000009999999}\n",
      "[2019/03/18 10:00:43] Epoch 13/ Validation Loss:9.701/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 10:00:46] Epoch 13\\Batch 45050\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:49] Epoch 13\\Batch 45100\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:52] Epoch 13\\Batch 45150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:55] Epoch 13\\Batch 45200\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:00:58] Epoch 13\\Batch 45250\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:01] Epoch 13\\Batch 45300\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:03] Epoch 13\\Batch 45350\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:06] Epoch 13\\Batch 45400\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:09] Epoch 13\\Batch 45450\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:12] Epoch 13\\Batch 45500\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:15] Epoch 13\\Batch 45550\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:18] Epoch 13\\Batch 45600\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:21] Epoch 13\\Batch 45650\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:24] Epoch 13\\Batch 45700\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:26] Epoch 13\\Batch 45750\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:29] Epoch 13\\Batch 45800\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:32] Epoch 13\\Batch 45850\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:35] Epoch 13\\Batch 45900\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:38] Epoch 13\\Batch 45950\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:41] Epoch 13\\Batch 46000\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:44] Epoch 13\\Batch 46050\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:47] Epoch 13\\Batch 46100\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:49] Epoch 13\\Batch 46150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:52] Epoch 13\\Batch 46200\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:01:58] Epoch 14\\Batch 50\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:01] Epoch 14\\Batch 100\\ Train Loss:10.194\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:02:04] Epoch 14\\Batch 150\\ Train Loss:10.243\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:07] Epoch 14\\Batch 200\\ Train Loss:10.203\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:10] Epoch 14\\Batch 250\\ Train Loss:10.198\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:12] Epoch 14\\Batch 300\\ Train Loss:10.210\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:15] Epoch 14\\Batch 350\\ Train Loss:10.211\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:18] Epoch 14\\Batch 400\\ Train Loss:10.209\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:21] Epoch 14\\Batch 450\\ Train Loss:10.193\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:24] Epoch 14\\Batch 500\\ Train Loss:10.182\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:27] Epoch 14\\Batch 550\\ Train Loss:10.165\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:30] Epoch 14\\Batch 600\\ Train Loss:10.168\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:32] Epoch 14\\Batch 650\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:35] Epoch 14\\Batch 700\\ Train Loss:10.164\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:38] Epoch 14\\Batch 750\\ Train Loss:10.163\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:41] Epoch 14\\Batch 800\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:44] Epoch 14\\Batch 850\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:47] Epoch 14\\Batch 900\\ Train Loss:10.167\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:50] Epoch 14\\Batch 950\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:53] Epoch 14\\Batch 1000\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:56] Epoch 14\\Batch 1050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:02:58] Epoch 14\\Batch 1100\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:01] Epoch 14\\Batch 1150\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:04] Epoch 14\\Batch 1200\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:07] Epoch 14\\Batch 1250\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:10] Epoch 14\\Batch 1300\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:13] Epoch 14\\Batch 1350\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:16] Epoch 14\\Batch 1400\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:18] Epoch 14\\Batch 1450\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:21] Epoch 14\\Batch 1500\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:24] Epoch 14\\Batch 1550\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:27] Epoch 14\\Batch 1600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:30] Epoch 14\\Batch 1650\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:33] Epoch 14\\Batch 1700\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:36] Epoch 14\\Batch 1750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:39] Epoch 14\\Batch 1800\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:41] Epoch 14\\Batch 1850\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:44] Epoch 14\\Batch 1900\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:47] Epoch 14\\Batch 1950\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:50] Epoch 14\\Batch 2000\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:53] Epoch 14\\Batch 2050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:56] Epoch 14\\Batch 2100\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:03:59] Epoch 14\\Batch 2150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:02] Epoch 14\\Batch 2200\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:04] Epoch 14\\Batch 2250\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:07] Epoch 14\\Batch 2300\\ Train Loss:10.160\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:10] Epoch 14\\Batch 2350\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:13] Epoch 14\\Batch 2400\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:16] Epoch 14\\Batch 2450\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:19] Epoch 14\\Batch 2500\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:22] Epoch 14\\Batch 2550\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:25] Epoch 14\\Batch 2600\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:27] Epoch 14\\Batch 2650\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:30] Epoch 14\\Batch 2700\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:33] Epoch 14\\Batch 2750\\ Train Loss:10.159\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:36] Epoch 14\\Batch 2800\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:39] Epoch 14\\Batch 2850\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:42] Epoch 14\\Batch 2900\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:45] Epoch 14\\Batch 2950\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 10:04:48] Epoch 14\\Batch 3000\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5432.000001, 'TP': 1567.0000009999999, 'FP': 1641.0000009999999}\n",
      "[2019/03/18 10:05:03] Epoch 14/ Validation Loss:9.705/ F1_score:0.307/ Precision:0.488/ Recall:0.224\n",
      "[2019/03/18 10:05:06] Epoch 14\\Batch 3050\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:09] Epoch 14\\Batch 3100\\ Train Loss:10.158\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:12] Epoch 14\\Batch 3150\\ Train Loss:10.157\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:15] Epoch 14\\Batch 3200\\ Train Loss:10.156\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:18] Epoch 14\\Batch 3250\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:20] Epoch 14\\Batch 3300\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:23] Epoch 14\\Batch 3350\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:26] Epoch 14\\Batch 3400\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:29] Epoch 14\\Batch 3450\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:32] Epoch 14\\Batch 3500\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:35] Epoch 14\\Batch 3550\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:38] Epoch 14\\Batch 3600\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:41] Epoch 14\\Batch 3650\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:43] Epoch 14\\Batch 3700\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:46] Epoch 14\\Batch 3750\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:49] Epoch 14\\Batch 3800\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:52] Epoch 14\\Batch 3850\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:55] Epoch 14\\Batch 3900\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:05:58] Epoch 14\\Batch 3950\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:01] Epoch 14\\Batch 4000\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:04] Epoch 14\\Batch 4050\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:06] Epoch 14\\Batch 4100\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:09] Epoch 14\\Batch 4150\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:12] Epoch 14\\Batch 4200\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:15] Epoch 14\\Batch 4250\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:18] Epoch 14\\Batch 4300\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:21] Epoch 14\\Batch 4350\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:24] Epoch 14\\Batch 4400\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:27] Epoch 14\\Batch 4450\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:29] Epoch 14\\Batch 4500\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:32] Epoch 14\\Batch 4550\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:35] Epoch 14\\Batch 4600\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:38] Epoch 14\\Batch 4650\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:41] Epoch 14\\Batch 4700\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:44] Epoch 14\\Batch 4750\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:47] Epoch 14\\Batch 4800\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:50] Epoch 14\\Batch 4850\\ Train Loss:10.148\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:06:52] Epoch 14\\Batch 4900\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:55] Epoch 14\\Batch 4950\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:06:58] Epoch 14\\Batch 5000\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:01] Epoch 14\\Batch 5050\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:04] Epoch 14\\Batch 5100\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:07] Epoch 14\\Batch 5150\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:10] Epoch 14\\Batch 5200\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:13] Epoch 14\\Batch 5250\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:15] Epoch 14\\Batch 5300\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:18] Epoch 14\\Batch 5350\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:21] Epoch 14\\Batch 5400\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:24] Epoch 14\\Batch 5450\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:27] Epoch 14\\Batch 5500\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:30] Epoch 14\\Batch 5550\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:33] Epoch 14\\Batch 5600\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:36] Epoch 14\\Batch 5650\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:39] Epoch 14\\Batch 5700\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:41] Epoch 14\\Batch 5750\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:44] Epoch 14\\Batch 5800\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:47] Epoch 14\\Batch 5850\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:50] Epoch 14\\Batch 5900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:53] Epoch 14\\Batch 5950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:07:56] Epoch 14\\Batch 6000\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5406.000001, 'TP': 1593.0000009999999, 'FP': 1630.0000009999999}\n",
      "[2019/03/18 10:08:11] Epoch 14/ Validation Loss:9.691/ F1_score:0.312/ Precision:0.494/ Recall:0.228\n",
      "[2019/03/18 10:08:14] Epoch 14\\Batch 6050\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:17] Epoch 14\\Batch 6100\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:20] Epoch 14\\Batch 6150\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:23] Epoch 14\\Batch 6200\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:26] Epoch 14\\Batch 6250\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:29] Epoch 14\\Batch 6300\\ Train Loss:10.154\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:31] Epoch 14\\Batch 6350\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:34] Epoch 14\\Batch 6400\\ Train Loss:10.155\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:37] Epoch 14\\Batch 6450\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:40] Epoch 14\\Batch 6500\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:43] Epoch 14\\Batch 6550\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:46] Epoch 14\\Batch 6600\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:49] Epoch 14\\Batch 6650\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:52] Epoch 14\\Batch 6700\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:54] Epoch 14\\Batch 6750\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:08:57] Epoch 14\\Batch 6800\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:00] Epoch 14\\Batch 6850\\ Train Loss:10.153\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:03] Epoch 14\\Batch 6900\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:06] Epoch 14\\Batch 6950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:09] Epoch 14\\Batch 7000\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:12] Epoch 14\\Batch 7050\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:15] Epoch 14\\Batch 7100\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:17] Epoch 14\\Batch 7150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:20] Epoch 14\\Batch 7200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:23] Epoch 14\\Batch 7250\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:26] Epoch 14\\Batch 7300\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:29] Epoch 14\\Batch 7350\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:32] Epoch 14\\Batch 7400\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:35] Epoch 14\\Batch 7450\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:37] Epoch 14\\Batch 7500\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:40] Epoch 14\\Batch 7550\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:43] Epoch 14\\Batch 7600\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:46] Epoch 14\\Batch 7650\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:49] Epoch 14\\Batch 7700\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:52] Epoch 14\\Batch 7750\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:55] Epoch 14\\Batch 7800\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:09:58] Epoch 14\\Batch 7850\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:00] Epoch 14\\Batch 7900\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:03] Epoch 14\\Batch 7950\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:06] Epoch 14\\Batch 8000\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:09] Epoch 14\\Batch 8050\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:12] Epoch 14\\Batch 8100\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:15] Epoch 14\\Batch 8150\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:18] Epoch 14\\Batch 8200\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:21] Epoch 14\\Batch 8250\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:23] Epoch 14\\Batch 8300\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:26] Epoch 14\\Batch 8350\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:29] Epoch 14\\Batch 8400\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:32] Epoch 14\\Batch 8450\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:35] Epoch 14\\Batch 8500\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:38] Epoch 14\\Batch 8550\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:41] Epoch 14\\Batch 8600\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:44] Epoch 14\\Batch 8650\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:46] Epoch 14\\Batch 8700\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:49] Epoch 14\\Batch 8750\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:52] Epoch 14\\Batch 8800\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:55] Epoch 14\\Batch 8850\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:10:58] Epoch 14\\Batch 8900\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:01] Epoch 14\\Batch 8950\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:04] Epoch 14\\Batch 9000\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5436.000001, 'TP': 1563.0000009999999, 'FP': 1647.0000009999999}\n",
      "[2019/03/18 10:11:19] Epoch 14/ Validation Loss:9.700/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 10:11:22] Epoch 14\\Batch 9050\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:25] Epoch 14\\Batch 9100\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:28] Epoch 14\\Batch 9150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:31] Epoch 14\\Batch 9200\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:33] Epoch 14\\Batch 9250\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:36] Epoch 14\\Batch 9300\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:39] Epoch 14\\Batch 9350\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:42] Epoch 14\\Batch 9400\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:45] Epoch 14\\Batch 9450\\ Train Loss:10.150\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:11:48] Epoch 14\\Batch 9500\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:51] Epoch 14\\Batch 9550\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:54] Epoch 14\\Batch 9600\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:56] Epoch 14\\Batch 9650\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:11:59] Epoch 14\\Batch 9700\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:02] Epoch 14\\Batch 9750\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:05] Epoch 14\\Batch 9800\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:08] Epoch 14\\Batch 9850\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:11] Epoch 14\\Batch 9900\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:14] Epoch 14\\Batch 9950\\ Train Loss:10.152\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:17] Epoch 14\\Batch 10000\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:19] Epoch 14\\Batch 10050\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:22] Epoch 14\\Batch 10100\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:25] Epoch 14\\Batch 10150\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:28] Epoch 14\\Batch 10200\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:31] Epoch 14\\Batch 10250\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:34] Epoch 14\\Batch 10300\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:37] Epoch 14\\Batch 10350\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:40] Epoch 14\\Batch 10400\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:42] Epoch 14\\Batch 10450\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:45] Epoch 14\\Batch 10500\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:48] Epoch 14\\Batch 10550\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:51] Epoch 14\\Batch 10600\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:54] Epoch 14\\Batch 10650\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:12:57] Epoch 14\\Batch 10700\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:00] Epoch 14\\Batch 10750\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:03] Epoch 14\\Batch 10800\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:05] Epoch 14\\Batch 10850\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:08] Epoch 14\\Batch 10900\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:11] Epoch 14\\Batch 10950\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:14] Epoch 14\\Batch 11000\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:17] Epoch 14\\Batch 11050\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:20] Epoch 14\\Batch 11100\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:23] Epoch 14\\Batch 11150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:26] Epoch 14\\Batch 11200\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:28] Epoch 14\\Batch 11250\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:31] Epoch 14\\Batch 11300\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:34] Epoch 14\\Batch 11350\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:37] Epoch 14\\Batch 11400\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:40] Epoch 14\\Batch 11450\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:43] Epoch 14\\Batch 11500\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:46] Epoch 14\\Batch 11550\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:49] Epoch 14\\Batch 11600\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:51] Epoch 14\\Batch 11650\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:54] Epoch 14\\Batch 11700\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:13:57] Epoch 14\\Batch 11750\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:00] Epoch 14\\Batch 11800\\ Train Loss:10.151\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:03] Epoch 14\\Batch 11850\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:06] Epoch 14\\Batch 11900\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:09] Epoch 14\\Batch 11950\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:12] Epoch 14\\Batch 12000\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5431.000001, 'TP': 1568.0000009999999, 'FP': 1662.0000009999999}\n",
      "[2019/03/18 10:14:27] Epoch 14/ Validation Loss:9.698/ F1_score:0.307/ Precision:0.485/ Recall:0.224\n",
      "[2019/03/18 10:14:30] Epoch 14\\Batch 12050\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:33] Epoch 14\\Batch 12100\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:36] Epoch 14\\Batch 12150\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:38] Epoch 14\\Batch 12200\\ Train Loss:10.150\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:41] Epoch 14\\Batch 12250\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:44] Epoch 14\\Batch 12300\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:47] Epoch 14\\Batch 12350\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:50] Epoch 14\\Batch 12400\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:53] Epoch 14\\Batch 12450\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:56] Epoch 14\\Batch 12500\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:14:59] Epoch 14\\Batch 12550\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:01] Epoch 14\\Batch 12600\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:04] Epoch 14\\Batch 12650\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:07] Epoch 14\\Batch 12700\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:10] Epoch 14\\Batch 12750\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:13] Epoch 14\\Batch 12800\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:16] Epoch 14\\Batch 12850\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:19] Epoch 14\\Batch 12900\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:22] Epoch 14\\Batch 12950\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:24] Epoch 14\\Batch 13000\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:27] Epoch 14\\Batch 13050\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:30] Epoch 14\\Batch 13100\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:33] Epoch 14\\Batch 13150\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:36] Epoch 14\\Batch 13200\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:39] Epoch 14\\Batch 13250\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:42] Epoch 14\\Batch 13300\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:45] Epoch 14\\Batch 13350\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:47] Epoch 14\\Batch 13400\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:50] Epoch 14\\Batch 13450\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:53] Epoch 14\\Batch 13500\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:56] Epoch 14\\Batch 13550\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:15:59] Epoch 14\\Batch 13600\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:02] Epoch 14\\Batch 13650\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:05] Epoch 14\\Batch 13700\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:08] Epoch 14\\Batch 13750\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:10] Epoch 14\\Batch 13800\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:13] Epoch 14\\Batch 13850\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:16] Epoch 14\\Batch 13900\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:19] Epoch 14\\Batch 13950\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:22] Epoch 14\\Batch 14000\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:25] Epoch 14\\Batch 14050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:28] Epoch 14\\Batch 14100\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:31] Epoch 14\\Batch 14150\\ Train Loss:10.147\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:16:33] Epoch 14\\Batch 14200\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:36] Epoch 14\\Batch 14250\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:39] Epoch 14\\Batch 14300\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:42] Epoch 14\\Batch 14350\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:45] Epoch 14\\Batch 14400\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:48] Epoch 14\\Batch 14450\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:51] Epoch 14\\Batch 14500\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:54] Epoch 14\\Batch 14550\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:56] Epoch 14\\Batch 14600\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:16:59] Epoch 14\\Batch 14650\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:02] Epoch 14\\Batch 14700\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:05] Epoch 14\\Batch 14750\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:08] Epoch 14\\Batch 14800\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:11] Epoch 14\\Batch 14850\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:14] Epoch 14\\Batch 14900\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:17] Epoch 14\\Batch 14950\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:20] Epoch 14\\Batch 15000\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [793]\n",
      "{'FN': 5459.000001, 'TP': 1540.0000009999999, 'FP': 1651.0000009999999}\n",
      "[2019/03/18 10:17:35] Epoch 14/ Validation Loss:9.698/ F1_score:0.302/ Precision:0.483/ Recall:0.220\n",
      "[2019/03/18 10:17:38] Epoch 14\\Batch 15050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:41] Epoch 14\\Batch 15100\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:43] Epoch 14\\Batch 15150\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:46] Epoch 14\\Batch 15200\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:49] Epoch 14\\Batch 15250\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:52] Epoch 14\\Batch 15300\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:55] Epoch 14\\Batch 15350\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:17:58] Epoch 14\\Batch 15400\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:01] Epoch 14\\Batch 15450\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:04] Epoch 14\\Batch 15500\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:07] Epoch 14\\Batch 15550\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:09] Epoch 14\\Batch 15600\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:12] Epoch 14\\Batch 15650\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:15] Epoch 14\\Batch 15700\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:18] Epoch 14\\Batch 15750\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:21] Epoch 14\\Batch 15800\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:24] Epoch 14\\Batch 15850\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:27] Epoch 14\\Batch 15900\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:30] Epoch 14\\Batch 15950\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:32] Epoch 14\\Batch 16000\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:35] Epoch 14\\Batch 16050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:38] Epoch 14\\Batch 16100\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:41] Epoch 14\\Batch 16150\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:44] Epoch 14\\Batch 16200\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:47] Epoch 14\\Batch 16250\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:50] Epoch 14\\Batch 16300\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:53] Epoch 14\\Batch 16350\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:55] Epoch 14\\Batch 16400\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:18:58] Epoch 14\\Batch 16450\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:01] Epoch 14\\Batch 16500\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:04] Epoch 14\\Batch 16550\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:07] Epoch 14\\Batch 16600\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:10] Epoch 14\\Batch 16650\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:13] Epoch 14\\Batch 16700\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:16] Epoch 14\\Batch 16750\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:18] Epoch 14\\Batch 16800\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:21] Epoch 14\\Batch 16850\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:24] Epoch 14\\Batch 16900\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:27] Epoch 14\\Batch 16950\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:30] Epoch 14\\Batch 17000\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:33] Epoch 14\\Batch 17050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:36] Epoch 14\\Batch 17100\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:39] Epoch 14\\Batch 17150\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:41] Epoch 14\\Batch 17200\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:44] Epoch 14\\Batch 17250\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:47] Epoch 14\\Batch 17300\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:50] Epoch 14\\Batch 17350\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:53] Epoch 14\\Batch 17400\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:56] Epoch 14\\Batch 17450\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:19:59] Epoch 14\\Batch 17500\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:02] Epoch 14\\Batch 17550\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:04] Epoch 14\\Batch 17600\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:07] Epoch 14\\Batch 17650\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:10] Epoch 14\\Batch 17700\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:13] Epoch 14\\Batch 17750\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:16] Epoch 14\\Batch 17800\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:19] Epoch 14\\Batch 17850\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:22] Epoch 14\\Batch 17900\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:25] Epoch 14\\Batch 17950\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:27] Epoch 14\\Batch 18000\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5450.000001, 'TP': 1549.0000009999999, 'FP': 1669.0000009999999}\n",
      "[2019/03/18 10:20:43] Epoch 14/ Validation Loss:9.703/ F1_score:0.303/ Precision:0.481/ Recall:0.221\n",
      "[2019/03/18 10:20:46] Epoch 14\\Batch 18050\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:49] Epoch 14\\Batch 18100\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:52] Epoch 14\\Batch 18150\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:55] Epoch 14\\Batch 18200\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:20:57] Epoch 14\\Batch 18250\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:00] Epoch 14\\Batch 18300\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:03] Epoch 14\\Batch 18350\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:06] Epoch 14\\Batch 18400\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:09] Epoch 14\\Batch 18450\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:12] Epoch 14\\Batch 18500\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:15] Epoch 14\\Batch 18550\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:18] Epoch 14\\Batch 18600\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:20] Epoch 14\\Batch 18650\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:23] Epoch 14\\Batch 18700\\ Train Loss:10.145\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:21:26] Epoch 14\\Batch 18750\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:29] Epoch 14\\Batch 18800\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:32] Epoch 14\\Batch 18850\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:35] Epoch 14\\Batch 18900\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:38] Epoch 14\\Batch 18950\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:40] Epoch 14\\Batch 19000\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:43] Epoch 14\\Batch 19050\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:46] Epoch 14\\Batch 19100\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:49] Epoch 14\\Batch 19150\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:52] Epoch 14\\Batch 19200\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:55] Epoch 14\\Batch 19250\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:21:58] Epoch 14\\Batch 19300\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:01] Epoch 14\\Batch 19350\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:03] Epoch 14\\Batch 19400\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:06] Epoch 14\\Batch 19450\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:09] Epoch 14\\Batch 19500\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:12] Epoch 14\\Batch 19550\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:15] Epoch 14\\Batch 19600\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:18] Epoch 14\\Batch 19650\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:21] Epoch 14\\Batch 19700\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:24] Epoch 14\\Batch 19750\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:26] Epoch 14\\Batch 19800\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:29] Epoch 14\\Batch 19850\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:32] Epoch 14\\Batch 19900\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:35] Epoch 14\\Batch 19950\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:38] Epoch 14\\Batch 20000\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:41] Epoch 14\\Batch 20050\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:44] Epoch 14\\Batch 20100\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:47] Epoch 14\\Batch 20150\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:49] Epoch 14\\Batch 20200\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:52] Epoch 14\\Batch 20250\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:55] Epoch 14\\Batch 20300\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:22:58] Epoch 14\\Batch 20350\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:01] Epoch 14\\Batch 20400\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:04] Epoch 14\\Batch 20450\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:07] Epoch 14\\Batch 20500\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:10] Epoch 14\\Batch 20550\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:12] Epoch 14\\Batch 20600\\ Train Loss:10.149\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:15] Epoch 14\\Batch 20650\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:18] Epoch 14\\Batch 20700\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:21] Epoch 14\\Batch 20750\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:24] Epoch 14\\Batch 20800\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:27] Epoch 14\\Batch 20850\\ Train Loss:10.148\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:30] Epoch 14\\Batch 20900\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:33] Epoch 14\\Batch 20950\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:35] Epoch 14\\Batch 21000\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572, 793]\n",
      "{'FN': 5416.000001, 'TP': 1583.0000009999999, 'FP': 1648.0000009999999}\n",
      "[2019/03/18 10:23:51] Epoch 14/ Validation Loss:9.710/ F1_score:0.309/ Precision:0.490/ Recall:0.226\n",
      "[2019/03/18 10:23:54] Epoch 14\\Batch 21050\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:23:57] Epoch 14\\Batch 21100\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:00] Epoch 14\\Batch 21150\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:02] Epoch 14\\Batch 21200\\ Train Loss:10.147\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:05] Epoch 14\\Batch 21250\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:08] Epoch 14\\Batch 21300\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:11] Epoch 14\\Batch 21350\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:14] Epoch 14\\Batch 21400\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:17] Epoch 14\\Batch 21450\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:20] Epoch 14\\Batch 21500\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:23] Epoch 14\\Batch 21550\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:26] Epoch 14\\Batch 21600\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:28] Epoch 14\\Batch 21650\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:31] Epoch 14\\Batch 21700\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:34] Epoch 14\\Batch 21750\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:37] Epoch 14\\Batch 21800\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:40] Epoch 14\\Batch 21850\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:43] Epoch 14\\Batch 21900\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:46] Epoch 14\\Batch 21950\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:49] Epoch 14\\Batch 22000\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:51] Epoch 14\\Batch 22050\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:54] Epoch 14\\Batch 22100\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:24:57] Epoch 14\\Batch 22150\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:00] Epoch 14\\Batch 22200\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:03] Epoch 14\\Batch 22250\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:06] Epoch 14\\Batch 22300\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:09] Epoch 14\\Batch 22350\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:12] Epoch 14\\Batch 22400\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:14] Epoch 14\\Batch 22450\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:17] Epoch 14\\Batch 22500\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:20] Epoch 14\\Batch 22550\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:23] Epoch 14\\Batch 22600\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:26] Epoch 14\\Batch 22650\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:29] Epoch 14\\Batch 22700\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:32] Epoch 14\\Batch 22750\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:34] Epoch 14\\Batch 22800\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:37] Epoch 14\\Batch 22850\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:40] Epoch 14\\Batch 22900\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:43] Epoch 14\\Batch 22950\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:46] Epoch 14\\Batch 23000\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:49] Epoch 14\\Batch 23050\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:52] Epoch 14\\Batch 23100\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:55] Epoch 14\\Batch 23150\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:25:57] Epoch 14\\Batch 23200\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:00] Epoch 14\\Batch 23250\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:03] Epoch 14\\Batch 23300\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:06] Epoch 14\\Batch 23350\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:09] Epoch 14\\Batch 23400\\ Train Loss:10.145\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:26:12] Epoch 14\\Batch 23450\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:15] Epoch 14\\Batch 23500\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:18] Epoch 14\\Batch 23550\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:21] Epoch 14\\Batch 23600\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:23] Epoch 14\\Batch 23650\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:26] Epoch 14\\Batch 23700\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:29] Epoch 14\\Batch 23750\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:32] Epoch 14\\Batch 23800\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:35] Epoch 14\\Batch 23850\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:38] Epoch 14\\Batch 23900\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:41] Epoch 14\\Batch 23950\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "[2019/03/18 10:26:44] Epoch 14\\Batch 24000\\ Train Loss:10.146\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5412.000001, 'TP': 1587.0000009999999, 'FP': 1640.0000009999999}\n",
      "[2019/03/18 10:26:59] Epoch 14/ Validation Loss:9.690/ F1_score:0.310/ Precision:0.492/ Recall:0.227\n",
      "[2019/03/18 10:27:02] Epoch 14\\Batch 24050\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:05] Epoch 14\\Batch 24100\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:08] Epoch 14\\Batch 24150\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:11] Epoch 14\\Batch 24200\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:13] Epoch 14\\Batch 24250\\ Train Loss:10.145\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:16] Epoch 14\\Batch 24300\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:19] Epoch 14\\Batch 24350\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:22] Epoch 14\\Batch 24400\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:25] Epoch 14\\Batch 24450\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:28] Epoch 14\\Batch 24500\\ Train Loss:10.144\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:31] Epoch 14\\Batch 24550\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:34] Epoch 14\\Batch 24600\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:36] Epoch 14\\Batch 24650\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:39] Epoch 14\\Batch 24700\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:42] Epoch 14\\Batch 24750\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:45] Epoch 14\\Batch 24800\\ Train Loss:10.143\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:48] Epoch 14\\Batch 24850\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:51] Epoch 14\\Batch 24900\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:54] Epoch 14\\Batch 24950\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:57] Epoch 14\\Batch 25000\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:27:59] Epoch 14\\Batch 25050\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:02] Epoch 14\\Batch 25100\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:05] Epoch 14\\Batch 25150\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:08] Epoch 14\\Batch 25200\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:11] Epoch 14\\Batch 25250\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:14] Epoch 14\\Batch 25300\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:17] Epoch 14\\Batch 25350\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:20] Epoch 14\\Batch 25400\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:22] Epoch 14\\Batch 25450\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:25] Epoch 14\\Batch 25500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:28] Epoch 14\\Batch 25550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:31] Epoch 14\\Batch 25600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:34] Epoch 14\\Batch 25650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:37] Epoch 14\\Batch 25700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:40] Epoch 14\\Batch 25750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:43] Epoch 14\\Batch 25800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:45] Epoch 14\\Batch 25850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:48] Epoch 14\\Batch 25900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:51] Epoch 14\\Batch 25950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:54] Epoch 14\\Batch 26000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:28:57] Epoch 14\\Batch 26050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:00] Epoch 14\\Batch 26100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:03] Epoch 14\\Batch 26150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:06] Epoch 14\\Batch 26200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:08] Epoch 14\\Batch 26250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:11] Epoch 14\\Batch 26300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:14] Epoch 14\\Batch 26350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:17] Epoch 14\\Batch 26400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:20] Epoch 14\\Batch 26450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:23] Epoch 14\\Batch 26500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:26] Epoch 14\\Batch 26550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:29] Epoch 14\\Batch 26600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:31] Epoch 14\\Batch 26650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:34] Epoch 14\\Batch 26700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:37] Epoch 14\\Batch 26750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:40] Epoch 14\\Batch 26800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:43] Epoch 14\\Batch 26850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:46] Epoch 14\\Batch 26900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:49] Epoch 14\\Batch 26950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:29:52] Epoch 14\\Batch 27000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5451.000001, 'TP': 1548.0000009999999, 'FP': 1671.0000009999999}\n",
      "[2019/03/18 10:30:07] Epoch 14/ Validation Loss:9.707/ F1_score:0.303/ Precision:0.481/ Recall:0.221\n",
      "[2019/03/18 10:30:10] Epoch 14\\Batch 27050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:13] Epoch 14\\Batch 27100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:16] Epoch 14\\Batch 27150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:19] Epoch 14\\Batch 27200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:22] Epoch 14\\Batch 27250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:24] Epoch 14\\Batch 27300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:27] Epoch 14\\Batch 27350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:30] Epoch 14\\Batch 27400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:33] Epoch 14\\Batch 27450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:36] Epoch 14\\Batch 27500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:39] Epoch 14\\Batch 27550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:42] Epoch 14\\Batch 27600\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:45] Epoch 14\\Batch 27650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:47] Epoch 14\\Batch 27700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:50] Epoch 14\\Batch 27750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:53] Epoch 14\\Batch 27800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:56] Epoch 14\\Batch 27850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:30:59] Epoch 14\\Batch 27900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:02] Epoch 14\\Batch 27950\\ Train Loss:10.140\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:31:05] Epoch 14\\Batch 28000\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:08] Epoch 14\\Batch 28050\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:10] Epoch 14\\Batch 28100\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:13] Epoch 14\\Batch 28150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:16] Epoch 14\\Batch 28200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:19] Epoch 14\\Batch 28250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:22] Epoch 14\\Batch 28300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:25] Epoch 14\\Batch 28350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:28] Epoch 14\\Batch 28400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:31] Epoch 14\\Batch 28450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:33] Epoch 14\\Batch 28500\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:36] Epoch 14\\Batch 28550\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:39] Epoch 14\\Batch 28600\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:42] Epoch 14\\Batch 28650\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:45] Epoch 14\\Batch 28700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:48] Epoch 14\\Batch 28750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:51] Epoch 14\\Batch 28800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:54] Epoch 14\\Batch 28850\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:56] Epoch 14\\Batch 28900\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:31:59] Epoch 14\\Batch 28950\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:02] Epoch 14\\Batch 29000\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:05] Epoch 14\\Batch 29050\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:08] Epoch 14\\Batch 29100\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:11] Epoch 14\\Batch 29150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:14] Epoch 14\\Batch 29200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:17] Epoch 14\\Batch 29250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:19] Epoch 14\\Batch 29300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:22] Epoch 14\\Batch 29350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:25] Epoch 14\\Batch 29400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:28] Epoch 14\\Batch 29450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:31] Epoch 14\\Batch 29500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:34] Epoch 14\\Batch 29550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:37] Epoch 14\\Batch 29600\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:40] Epoch 14\\Batch 29650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:42] Epoch 14\\Batch 29700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:45] Epoch 14\\Batch 29750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:48] Epoch 14\\Batch 29800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:51] Epoch 14\\Batch 29850\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:54] Epoch 14\\Batch 29900\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:32:57] Epoch 14\\Batch 29950\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:00] Epoch 14\\Batch 30000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5425.000001, 'TP': 1574.0000009999999, 'FP': 1637.0000009999999}\n",
      "[2019/03/18 10:33:15] Epoch 14/ Validation Loss:9.691/ F1_score:0.308/ Precision:0.490/ Recall:0.225\n",
      "[2019/03/18 10:33:18] Epoch 14\\Batch 30050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:21] Epoch 14\\Batch 30100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:24] Epoch 14\\Batch 30150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:27] Epoch 14\\Batch 30200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:30] Epoch 14\\Batch 30250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:32] Epoch 14\\Batch 30300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:35] Epoch 14\\Batch 30350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:38] Epoch 14\\Batch 30400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:41] Epoch 14\\Batch 30450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:44] Epoch 14\\Batch 30500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:47] Epoch 14\\Batch 30550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:50] Epoch 14\\Batch 30600\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:53] Epoch 14\\Batch 30650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:55] Epoch 14\\Batch 30700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:33:58] Epoch 14\\Batch 30750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:01] Epoch 14\\Batch 30800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:04] Epoch 14\\Batch 30850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:07] Epoch 14\\Batch 30900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:10] Epoch 14\\Batch 30950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:13] Epoch 14\\Batch 31000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:16] Epoch 14\\Batch 31050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:18] Epoch 14\\Batch 31100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:21] Epoch 14\\Batch 31150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:24] Epoch 14\\Batch 31200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:27] Epoch 14\\Batch 31250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:30] Epoch 14\\Batch 31300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:33] Epoch 14\\Batch 31350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:36] Epoch 14\\Batch 31400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:39] Epoch 14\\Batch 31450\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:41] Epoch 14\\Batch 31500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:44] Epoch 14\\Batch 31550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:47] Epoch 14\\Batch 31600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:50] Epoch 14\\Batch 31650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:53] Epoch 14\\Batch 31700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:56] Epoch 14\\Batch 31750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:34:59] Epoch 14\\Batch 31800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:02] Epoch 14\\Batch 31850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:04] Epoch 14\\Batch 31900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:07] Epoch 14\\Batch 31950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:10] Epoch 14\\Batch 32000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:13] Epoch 14\\Batch 32050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:16] Epoch 14\\Batch 32100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:19] Epoch 14\\Batch 32150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:22] Epoch 14\\Batch 32200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:25] Epoch 14\\Batch 32250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:27] Epoch 14\\Batch 32300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:30] Epoch 14\\Batch 32350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:33] Epoch 14\\Batch 32400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:36] Epoch 14\\Batch 32450\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:39] Epoch 14\\Batch 32500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:42] Epoch 14\\Batch 32550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:45] Epoch 14\\Batch 32600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:47] Epoch 14\\Batch 32650\\ Train Loss:10.141\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:35:50] Epoch 14\\Batch 32700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:53] Epoch 14\\Batch 32750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:56] Epoch 14\\Batch 32800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:35:59] Epoch 14\\Batch 32850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:02] Epoch 14\\Batch 32900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:05] Epoch 14\\Batch 32950\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:08] Epoch 14\\Batch 33000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5440.000001, 'TP': 1559.0000009999999, 'FP': 1644.0000009999999}\n",
      "[2019/03/18 10:36:23] Epoch 14/ Validation Loss:9.687/ F1_score:0.306/ Precision:0.487/ Recall:0.223\n",
      "[2019/03/18 10:36:26] Epoch 14\\Batch 33050\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:29] Epoch 14\\Batch 33100\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:32] Epoch 14\\Batch 33150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:35] Epoch 14\\Batch 33200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:38] Epoch 14\\Batch 33250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:40] Epoch 14\\Batch 33300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:43] Epoch 14\\Batch 33350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:46] Epoch 14\\Batch 33400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:49] Epoch 14\\Batch 33450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:52] Epoch 14\\Batch 33500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:55] Epoch 14\\Batch 33550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:36:58] Epoch 14\\Batch 33600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:01] Epoch 14\\Batch 33650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:03] Epoch 14\\Batch 33700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:06] Epoch 14\\Batch 33750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:09] Epoch 14\\Batch 33800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:12] Epoch 14\\Batch 33850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:15] Epoch 14\\Batch 33900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:18] Epoch 14\\Batch 33950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:21] Epoch 14\\Batch 34000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:24] Epoch 14\\Batch 34050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:26] Epoch 14\\Batch 34100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:29] Epoch 14\\Batch 34150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:32] Epoch 14\\Batch 34200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:35] Epoch 14\\Batch 34250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:38] Epoch 14\\Batch 34300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:41] Epoch 14\\Batch 34350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:44] Epoch 14\\Batch 34400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:47] Epoch 14\\Batch 34450\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:49] Epoch 14\\Batch 34500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:52] Epoch 14\\Batch 34550\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:55] Epoch 14\\Batch 34600\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:37:58] Epoch 14\\Batch 34650\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:01] Epoch 14\\Batch 34700\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:04] Epoch 14\\Batch 34750\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:07] Epoch 14\\Batch 34800\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:10] Epoch 14\\Batch 34850\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:12] Epoch 14\\Batch 34900\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:15] Epoch 14\\Batch 34950\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:18] Epoch 14\\Batch 35000\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:21] Epoch 14\\Batch 35050\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:24] Epoch 14\\Batch 35100\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:27] Epoch 14\\Batch 35150\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:30] Epoch 14\\Batch 35200\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:33] Epoch 14\\Batch 35250\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:35] Epoch 14\\Batch 35300\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:38] Epoch 14\\Batch 35350\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:41] Epoch 14\\Batch 35400\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:44] Epoch 14\\Batch 35450\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:47] Epoch 14\\Batch 35500\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:50] Epoch 14\\Batch 35550\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:53] Epoch 14\\Batch 35600\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:56] Epoch 14\\Batch 35650\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:38:58] Epoch 14\\Batch 35700\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:01] Epoch 14\\Batch 35750\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:04] Epoch 14\\Batch 35800\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:07] Epoch 14\\Batch 35850\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:10] Epoch 14\\Batch 35900\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:13] Epoch 14\\Batch 35950\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:16] Epoch 14\\Batch 36000\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5424.000001, 'TP': 1575.0000009999999, 'FP': 1632.0000009999999}\n",
      "[2019/03/18 10:39:31] Epoch 14/ Validation Loss:9.687/ F1_score:0.309/ Precision:0.491/ Recall:0.225\n",
      "[2019/03/18 10:39:34] Epoch 14\\Batch 36050\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:37] Epoch 14\\Batch 36100\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:40] Epoch 14\\Batch 36150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:43] Epoch 14\\Batch 36200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:45] Epoch 14\\Batch 36250\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:48] Epoch 14\\Batch 36300\\ Train Loss:10.142\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:51] Epoch 14\\Batch 36350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:54] Epoch 14\\Batch 36400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:39:57] Epoch 14\\Batch 36450\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:00] Epoch 14\\Batch 36500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:03] Epoch 14\\Batch 36550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:06] Epoch 14\\Batch 36600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:08] Epoch 14\\Batch 36650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:11] Epoch 14\\Batch 36700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:14] Epoch 14\\Batch 36750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:17] Epoch 14\\Batch 36800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:20] Epoch 14\\Batch 36850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:23] Epoch 14\\Batch 36900\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:26] Epoch 14\\Batch 36950\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:29] Epoch 14\\Batch 37000\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:31] Epoch 14\\Batch 37050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:34] Epoch 14\\Batch 37100\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:37] Epoch 14\\Batch 37150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:40] Epoch 14\\Batch 37200\\ Train Loss:10.140\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:40:43] Epoch 14\\Batch 37250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:46] Epoch 14\\Batch 37300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:49] Epoch 14\\Batch 37350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:52] Epoch 14\\Batch 37400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:54] Epoch 14\\Batch 37450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:40:57] Epoch 14\\Batch 37500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:00] Epoch 14\\Batch 37550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:03] Epoch 14\\Batch 37600\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:06] Epoch 14\\Batch 37650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:09] Epoch 14\\Batch 37700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:12] Epoch 14\\Batch 37750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:15] Epoch 14\\Batch 37800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:17] Epoch 14\\Batch 37850\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:20] Epoch 14\\Batch 37900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:23] Epoch 14\\Batch 37950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:26] Epoch 14\\Batch 38000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:29] Epoch 14\\Batch 38050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:32] Epoch 14\\Batch 38100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:35] Epoch 14\\Batch 38150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:38] Epoch 14\\Batch 38200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:40] Epoch 14\\Batch 38250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:43] Epoch 14\\Batch 38300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:46] Epoch 14\\Batch 38350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:49] Epoch 14\\Batch 38400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:52] Epoch 14\\Batch 38450\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:55] Epoch 14\\Batch 38500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:41:58] Epoch 14\\Batch 38550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:01] Epoch 14\\Batch 38600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:03] Epoch 14\\Batch 38650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:06] Epoch 14\\Batch 38700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:09] Epoch 14\\Batch 38750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:12] Epoch 14\\Batch 38800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:15] Epoch 14\\Batch 38850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:18] Epoch 14\\Batch 38900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:21] Epoch 14\\Batch 38950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:24] Epoch 14\\Batch 39000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5419.000001, 'TP': 1580.0000009999999, 'FP': 1658.0000009999999}\n",
      "[2019/03/18 10:42:39] Epoch 14/ Validation Loss:9.718/ F1_score:0.309/ Precision:0.488/ Recall:0.226\n",
      "[2019/03/18 10:42:42] Epoch 14\\Batch 39050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:45] Epoch 14\\Batch 39100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:48] Epoch 14\\Batch 39150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:51] Epoch 14\\Batch 39200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:54] Epoch 14\\Batch 39250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:56] Epoch 14\\Batch 39300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:42:59] Epoch 14\\Batch 39350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:02] Epoch 14\\Batch 39400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:05] Epoch 14\\Batch 39450\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:08] Epoch 14\\Batch 39500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:11] Epoch 14\\Batch 39550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:14] Epoch 14\\Batch 39600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:17] Epoch 14\\Batch 39650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:19] Epoch 14\\Batch 39700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:22] Epoch 14\\Batch 39750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:25] Epoch 14\\Batch 39800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:28] Epoch 14\\Batch 39850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:31] Epoch 14\\Batch 39900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:34] Epoch 14\\Batch 39950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:37] Epoch 14\\Batch 40000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:40] Epoch 14\\Batch 40050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:42] Epoch 14\\Batch 40100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:45] Epoch 14\\Batch 40150\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:48] Epoch 14\\Batch 40200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:51] Epoch 14\\Batch 40250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:54] Epoch 14\\Batch 40300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:43:57] Epoch 14\\Batch 40350\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:00] Epoch 14\\Batch 40400\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:03] Epoch 14\\Batch 40450\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:05] Epoch 14\\Batch 40500\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:08] Epoch 14\\Batch 40550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:11] Epoch 14\\Batch 40600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:14] Epoch 14\\Batch 40650\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:17] Epoch 14\\Batch 40700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:20] Epoch 14\\Batch 40750\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:23] Epoch 14\\Batch 40800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:26] Epoch 14\\Batch 40850\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:28] Epoch 14\\Batch 40900\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:31] Epoch 14\\Batch 40950\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:34] Epoch 14\\Batch 41000\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:37] Epoch 14\\Batch 41050\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:40] Epoch 14\\Batch 41100\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:43] Epoch 14\\Batch 41150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:46] Epoch 14\\Batch 41200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:49] Epoch 14\\Batch 41250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:51] Epoch 14\\Batch 41300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:54] Epoch 14\\Batch 41350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:44:57] Epoch 14\\Batch 41400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:00] Epoch 14\\Batch 41450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:03] Epoch 14\\Batch 41500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:06] Epoch 14\\Batch 41550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:09] Epoch 14\\Batch 41600\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:12] Epoch 14\\Batch 41650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:14] Epoch 14\\Batch 41700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:17] Epoch 14\\Batch 41750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:20] Epoch 14\\Batch 41800\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:23] Epoch 14\\Batch 41850\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:26] Epoch 14\\Batch 41900\\ Train Loss:10.140\\ Learning rate:0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/18 10:45:29] Epoch 14\\Batch 41950\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:32] Epoch 14\\Batch 42000\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5412.000001, 'TP': 1587.0000009999999, 'FP': 1625.0000009999999}\n",
      "[2019/03/18 10:45:47] Epoch 14/ Validation Loss:9.682/ F1_score:0.311/ Precision:0.494/ Recall:0.227\n",
      "[2019/03/18 10:45:50] Epoch 14\\Batch 42050\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:53] Epoch 14\\Batch 42100\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:56] Epoch 14\\Batch 42150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:45:59] Epoch 14\\Batch 42200\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:02] Epoch 14\\Batch 42250\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:04] Epoch 14\\Batch 42300\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:07] Epoch 14\\Batch 42350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:10] Epoch 14\\Batch 42400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:13] Epoch 14\\Batch 42450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:16] Epoch 14\\Batch 42500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:19] Epoch 14\\Batch 42550\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:22] Epoch 14\\Batch 42600\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:25] Epoch 14\\Batch 42650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:27] Epoch 14\\Batch 42700\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:30] Epoch 14\\Batch 42750\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:33] Epoch 14\\Batch 42800\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:36] Epoch 14\\Batch 42850\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:39] Epoch 14\\Batch 42900\\ Train Loss:10.141\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:42] Epoch 14\\Batch 42950\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:45] Epoch 14\\Batch 43000\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:48] Epoch 14\\Batch 43050\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:51] Epoch 14\\Batch 43100\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:53] Epoch 14\\Batch 43150\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:56] Epoch 14\\Batch 43200\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:46:59] Epoch 14\\Batch 43250\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:02] Epoch 14\\Batch 43300\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:05] Epoch 14\\Batch 43350\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:08] Epoch 14\\Batch 43400\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:11] Epoch 14\\Batch 43450\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:14] Epoch 14\\Batch 43500\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:16] Epoch 14\\Batch 43550\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:19] Epoch 14\\Batch 43600\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:22] Epoch 14\\Batch 43650\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:25] Epoch 14\\Batch 43700\\ Train Loss:10.140\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:28] Epoch 14\\Batch 43750\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:31] Epoch 14\\Batch 43800\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:34] Epoch 14\\Batch 43850\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:37] Epoch 14\\Batch 43900\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:39] Epoch 14\\Batch 43950\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:42] Epoch 14\\Batch 44000\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:45] Epoch 14\\Batch 44050\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:48] Epoch 14\\Batch 44100\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:51] Epoch 14\\Batch 44150\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:54] Epoch 14\\Batch 44200\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:47:57] Epoch 14\\Batch 44250\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:00] Epoch 14\\Batch 44300\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:02] Epoch 14\\Batch 44350\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:05] Epoch 14\\Batch 44400\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:08] Epoch 14\\Batch 44450\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:11] Epoch 14\\Batch 44500\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:14] Epoch 14\\Batch 44550\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:17] Epoch 14\\Batch 44600\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:20] Epoch 14\\Batch 44650\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:23] Epoch 14\\Batch 44700\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:26] Epoch 14\\Batch 44750\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:28] Epoch 14\\Batch 44800\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:31] Epoch 14\\Batch 44850\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:34] Epoch 14\\Batch 44900\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:37] Epoch 14\\Batch 44950\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:48:40] Epoch 14\\Batch 45000\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [572]\n",
      "{'FN': 5419.000001, 'TP': 1580.0000009999999, 'FP': 1633.0000009999999}\n",
      "[2019/03/18 10:48:55] Epoch 14/ Validation Loss:9.678/ F1_score:0.309/ Precision:0.492/ Recall:0.226\n",
      "[2019/03/18 10:48:58] Epoch 14\\Batch 45050\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:01] Epoch 14\\Batch 45100\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:04] Epoch 14\\Batch 45150\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:07] Epoch 14\\Batch 45200\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:10] Epoch 14\\Batch 45250\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:13] Epoch 14\\Batch 45300\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:16] Epoch 14\\Batch 45350\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:18] Epoch 14\\Batch 45400\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:21] Epoch 14\\Batch 45450\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:24] Epoch 14\\Batch 45500\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:27] Epoch 14\\Batch 45550\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:30] Epoch 14\\Batch 45600\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:33] Epoch 14\\Batch 45650\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:36] Epoch 14\\Batch 45700\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:39] Epoch 14\\Batch 45750\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:41] Epoch 14\\Batch 45800\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:44] Epoch 14\\Batch 45850\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:47] Epoch 14\\Batch 45900\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:50] Epoch 14\\Batch 45950\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:53] Epoch 14\\Batch 46000\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:56] Epoch 14\\Batch 46050\\ Train Loss:10.138\\ Learning rate:0.00030\n",
      "[2019/03/18 10:49:59] Epoch 14\\Batch 46100\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:50:02] Epoch 14\\Batch 46150\\ Train Loss:10.139\\ Learning rate:0.00030\n",
      "[2019/03/18 10:50:04] Epoch 14\\Batch 46200\\ Train Loss:10.139\\ Learning rate:0.00030\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
