{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "import package successful...\n"
     ]
    }
   ],
   "source": [
    "# import some packages\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tflearn.data_utils import pad_sequences\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import codecs\n",
    "print(\"import package successful...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成所需的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/data/chenhy/data/ieee_zhihu_cup/question_train_set3.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-06e2e1b71f9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 读取训练数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/data/chenhy/data/ieee_zhihu_cup/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_data_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'question_train_set3.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'question_topic_train_set3.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'/data/chenhy/data/ieee_zhihu_cup/question_train_set3.txt' does not exist"
     ]
    }
   ],
   "source": [
    "# 读取训练数据\n",
    "base_path='D:/zhihu_data/data/ieee_zhihu_cup2/'\n",
    "train_data_x=pd.read_csv(base_path+'question_train_set3.txt',sep='\\t', encoding=\"utf-8\")\n",
    "train_data_y=pd.read_csv(base_path+'question_topic_train_set3.txt',sep='\\t', encoding=\"utf-8\")\n",
    "\n",
    "train_data_x=train_data_x.fillna('')\n",
    "train_data_y=train_data_y.fillna('')\n",
    "\n",
    "print(\"train_data_x:\",train_data_x.shape)\n",
    "print(\"train_data_y:\",train_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_desc_char</th>\n",
       "      <th>topic_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c324,c39,c40,c155,c180,c180,c181,c17,c4,c1153,...</td>\n",
       "      <td>7739004195693774975,3738968195649774859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c44,c110,c101,c286,c106,c150,c101,c892,c632,c1...</td>\n",
       "      <td>-3149765934180654494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c15,c768,c769,c1363,c650,c1218,c2361,c11,c90,c...</td>\n",
       "      <td>-760432988437306018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c473,c1528,c528,c428,c295,c15,c101,c188,c146,c...</td>\n",
       "      <td>-6758942141122113907,3195914392210930723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c190,c147,c105,c219,c220,c101,c647,c219,c220,c...</td>\n",
       "      <td>3804601920633030746,4797226510592237555,435133...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title_desc_char  \\\n",
       "0  c324,c39,c40,c155,c180,c180,c181,c17,c4,c1153,...   \n",
       "1  c44,c110,c101,c286,c106,c150,c101,c892,c632,c1...   \n",
       "2  c15,c768,c769,c1363,c650,c1218,c2361,c11,c90,c...   \n",
       "3  c473,c1528,c528,c428,c295,c15,c101,c188,c146,c...   \n",
       "4  c190,c147,c105,c219,c220,c101,c647,c219,c220,c...   \n",
       "\n",
       "                                           topic_ids  \n",
       "0            7739004195693774975,3738968195649774859  \n",
       "1                               -3149765934180654494  \n",
       "2                                -760432988437306018  \n",
       "3           -6758942141122113907,3195914392210930723  \n",
       "4  3804601920633030746,4797226510592237555,435133...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 组成新的文件\n",
    "\n",
    "#连接title和desc\n",
    "train_data_x['title_desc_char'] = train_data_x.apply(lambda t: t['title_char'] + ',SEP,' + t['desc_char'], axis=1)\n",
    "train_desc = pd.concat([train_data_x['title_desc_char'], train_data_y['topic_ids']],axis=1)\n",
    "\n",
    "file = 'train-title-desc.txt' #c324,c39,c40,c155,SEP,c180,c180,c181,c17,c4,c5\\t7739004195693774975,3738968195649774859\n",
    "train_desc.to_csv(file, encoding='utf-8', sep='\\t', index=None)\n",
    "\n",
    "train_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成word2index和label2index，此处的word为char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PAD\n",
      "1 UNK\n",
      "2 SEP\n",
      "3 </s>\n",
      "4 c17\n",
      "5 c101\n",
      "6 c11\n",
      "7 c4\n",
      "8 c147\n",
      "9 c85\n",
      "vocabulary of char generated....\n"
     ]
    }
   ],
   "source": [
    "#创建word2index表和embedding表\n",
    "word_embedding_object=open(base_path+'unused_current/char_embedding.txt')\n",
    "lines_wv=word_embedding_object.readlines()\n",
    "word_embedding_object.close()\n",
    "char_list=[]\n",
    "char_list.extend(['PAD','UNK','SEP'])\n",
    "PAD_ID=0\n",
    "UNK_ID=1\n",
    "EMBED_SIZE = 100\n",
    "bound = 0.5\n",
    "embedding = [None] * (int(lines_wv[0].split(' ')[0]) + 3) #长度\n",
    "embedding[0] = np.zeros(100)\n",
    "embedding[1] = np.random.rand(100)\n",
    "embedding[2] = embedding[0] + 0.1\n",
    "for i, line in enumerate(lines_wv):\n",
    "    if i==0: continue\n",
    "    char_embedding_list=line.split(\" \")\n",
    "    char_token=char_embedding_list[0]\n",
    "    char_list.append(char_token)\n",
    "    embedding[i+2] = char_embedding_list[1:1+EMBED_SIZE]\n",
    "\n",
    "embedding = np.array(embedding,dtype=np.float32)\n",
    "# write to vocab.txt under data/ieee_zhihu_cup2\n",
    "vocab_path='vocab.txt'\n",
    "\n",
    "vocab_char_object=open(vocab_path,'w')\n",
    "\n",
    "word2index={}\n",
    "for i, char in enumerate(char_list):\n",
    "    if i<10:print(i,char)\n",
    "    word2index[char]=i\n",
    "    vocab_char_object.write(char+\"\\n\")\n",
    "vocab_char_object.close()\n",
    "print(\"vocabulary of char generated....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7476760589625268543 2308\n",
      "4697014490911193675 1746\n",
      "-4653836020042332281 1579\n",
      "-8175048003539471998 1475\n",
      "-8377411942628634656 1382\n",
      "-7046289575185911002 1338\n",
      "-5932391056759866388 1283\n",
      "2787171473654490487 1145\n",
      "-7129272008741138808 1085\n",
      "2587540952280802350 1079\n",
      "-4931965624608608932 1079\n",
      "-6748914495015758455 1049\n",
      "-5513826101327857645 993\n",
      "2347973810368732059 970\n",
      "9069451131871918127 958\n",
      "-8132909213241034354 904\n",
      "-3517637179126242000 867\n",
      "-5872443091340192918 834\n",
      "-3522198575349379632 830\n",
      "1127459907694805235 829\n",
      "generate label dict successful...\n"
     ]
    }
   ],
   "source": [
    " # generate labels list, and save to file system \n",
    "c_labels=Counter()\n",
    "train_data_y_small=train_data_y[0:100000]#.sample(frac=0.1)\n",
    "for index, row in train_data_y_small.iterrows():\n",
    "    topic_ids=row['topic_ids']\n",
    "    topic_list=topic_ids.split(',')\n",
    "    c_labels.update(topic_list)\n",
    "\n",
    "label_list=c_labels.most_common()\n",
    "label2index={}\n",
    "label_target_object=open('label_set.txt','w')\n",
    "for i, label_freq in enumerate(label_list):\n",
    "    label,freq=label_freq\n",
    "    label2index[label]=i\n",
    "    label_target_object.write(label+\"\\n\")\n",
    "    if i<20: print(label,freq)\n",
    "label_target_object.close()\n",
    "print(\"generate label dict successful...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=100,\n",
    "                             traning_data_path='train-title-desc.txt',multi_label_flag=True,use_seq2seq=False,seq2seq_label_length=6):  # n_words=100000,\n",
    "    \"\"\"\n",
    "    input: a file path\n",
    "    :return: train, test, valid. where train=(trainX, trainY). where\n",
    "                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n",
    "    \"\"\"\n",
    "    # 1.load a zhihu data from file\n",
    "    # example:\"w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492\"\n",
    "    print(\"load_data.started...\")\n",
    "    print(\"load_data_multilabel_new.training_data_path:\",traning_data_path)\n",
    "    zhihu_f = codecs.open(traning_data_path, 'r', 'utf8') #-zhihu4-only-title.txt\n",
    "    lines = zhihu_f.readlines(101)\n",
    "    # 2.transform X as indices\n",
    "    # 3.transform  y as scalar\n",
    "    X = []\n",
    "    Y = []\n",
    "    Y_decoder_input=[] #ADD 2017-06-15\n",
    "    for i, line in enumerate(lines):\n",
    "        x, y = line.split('\\t') #x='w17314 w5521 w7729 w767 w10147 w111'\n",
    "        y=y.strip().replace('\\n','')\n",
    "        x = x.strip()\n",
    "        if i<1:\n",
    "            print(i,\"x0:\",x) #get raw x\n",
    "        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n",
    "        x=x.split(\",\")\n",
    "        x = [vocabulary_word2index.get(e,0) for e in x] #if can't find the word, set the index as '0'.(equal to PAD_ID = 0)\n",
    "        if i<2:\n",
    "            print(i,\"x1:\",x) #word to index\n",
    "        if use_seq2seq:        # 1)prepare label for seq2seq format(ADD _GO,_END,_PAD for seq2seq)\n",
    "            ys = y.replace('\\n', '').split(\",\")  # ys is a list\n",
    "            _PAD_INDEX=vocabulary_word2index_label[_PAD]\n",
    "            ys_mulithot_list=[PAD_ID]*seq2seq_label_length #[3,2,11,14,1]\n",
    "            ys_decoder_input=[PAD_ID]*seq2seq_label_length\n",
    "            # below is label.\n",
    "            for j,y in enumerate(ys):\n",
    "                if j<seq2seq_label_length-1:\n",
    "                    ys_mulithot_list[j]=vocabulary_word2index_label[y]\n",
    "            if len(ys)>seq2seq_label_length-1:\n",
    "                ys_mulithot_list[seq2seq_label_length-1]=vocabulary_word2index_label[_END]#ADD END TOKEN\n",
    "            else:\n",
    "                ys_mulithot_list[len(ys)] = vocabulary_word2index_label[_END]\n",
    "\n",
    "            # below is input for decoder.\n",
    "            ys_decoder_input[0]=vocabulary_word2index_label[_GO]\n",
    "            for j,y in enumerate(ys):\n",
    "                if j < seq2seq_label_length - 1:\n",
    "                    ys_decoder_input[j+1]=vocabulary_word2index_label[y]\n",
    "            if i<10:\n",
    "                print(i,\"ys:==========>0\", ys)\n",
    "                print(i,\"ys_mulithot_list:==============>1\", ys_mulithot_list)\n",
    "                print(i,\"ys_decoder_input:==============>2\", ys_decoder_input)\n",
    "        else:\n",
    "            if multi_label_flag: # 2)prepare multi-label format for classification\n",
    "                ys = y.replace('\\n', '').split(\",\")  # ys is a list\n",
    "                ys_index=[]\n",
    "                for y in ys:\n",
    "                    y_index = vocabulary_word2index_label[y]\n",
    "                    ys_index.append(y_index)\n",
    "                ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\n",
    "            else:                #3)prepare single label format for classification\n",
    "                ys_mulithot_list=vocabulary_word2index_label[y]\n",
    "        if i<=3:\n",
    "            print(\"ys_index:\")\n",
    "            #print(ys_index)\n",
    "            print(i,\"y:\",y,\" ;ys_mulithot_list:\",ys_mulithot_list) #,\" ;ys_decoder_input:\",ys_decoder_input)\n",
    "        X.append(x)\n",
    "        Y.append(ys_mulithot_list)\n",
    "        if use_seq2seq:\n",
    "            Y_decoder_input.append(ys_decoder_input) #decoder input\n",
    "        #if i>50000:\n",
    "        #    break\n",
    "    # 4.split to train,test and valid data\n",
    "    number_examples = len(X)\n",
    "    print(\"number_examples:\",number_examples) #\n",
    "    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\n",
    "    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\n",
    "    if use_seq2seq:\n",
    "        train=train+(Y_decoder_input[0:int((1 - valid_portion) * number_examples)],)\n",
    "        test=test+(Y_decoder_input[int((1 - valid_portion) * number_examples) + 1:],)\n",
    "    # 5.return\n",
    "    print(\"load_data.ended...\")\n",
    "    return train, test, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data.started...\n",
      "load_data_multilabel_new.training_data_path: train-title-desc.txt\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dd318205192d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_data_multilabel_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel2index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_seq2seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-56f5dea98c23>\u001b[0m in \u001b[0;36mload_data_multilabel_new\u001b[1;34m(vocabulary_word2index, vocabulary_word2index_label, valid_portion, max_training_data, traning_data_path, multi_label_flag, use_seq2seq, seq2seq_label_length)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"load_data_multilabel_new.training_data_path:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraning_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mzhihu_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraning_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#-zhihu4-only-title.txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzhihu_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 2.transform X as indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# 3.transform  y as scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\codecs.py\u001b[0m in \u001b[0;36mreadlines\u001b[1;34m(self, sizehint)\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizehint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizehint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\codecs.py\u001b[0m in \u001b[0;36mreadlines\u001b[1;34m(self, sizehint, keepends)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeepends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\codecs.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size, chars, firstline)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;31m# we need more data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                 \u001b[0mnewdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mnewdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_data_multilabel_new(word2index, label2index, use_seq2seq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
