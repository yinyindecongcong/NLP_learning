{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义TextRNN结构，使用双向的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN:\n",
    "    def __init__(self, batch_size, num_classes, vocab_size, sentence_len, embed_size, \n",
    "                 learning_rate, decay_steps, decay_rate, is_training):\n",
    "        #1.定义超参数\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = embed_size #lstm层的维度\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "        \n",
    "        #2.输入\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sentence_len], 'input_x')\n",
    "#         self.input_y = tf.placeholder(tf.int32, [None], 'input_y') #单个标签\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], 'input_y') #多个标签\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        #3.初始化全连接层参数\n",
    "        self.init_weight()\n",
    "        \n",
    "        #4.网络结构\n",
    "        self.logits = self.inference() #[batch_size, num_classes]\n",
    "        \n",
    "        #5.损失函数\n",
    "        self.loss_val = self.loss()\n",
    "        \n",
    "        #6.优化器\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], dtype=tf.float32)\n",
    "        self.W = tf.get_variable('W', [self.hidden_size * 2, self.num_classes], dtype=tf.float32) #双向LSTM，输出concat，所以此处为2倍\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        # a.embedding\n",
    "        self.sentence_embed = tf.nn.embedding_lookup(self.Embedding, self.input_x) #[batch_size, sentence_len, embed_size]\n",
    "        \n",
    "        # b.bidiretional lstm\n",
    "        self.fw_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size) #前向单元\n",
    "        self.bw_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size) #后向单元\n",
    "#         if self.dropout_keep_prob is not None:\n",
    "#             self.fw_cell = tf.contrib.rnn.DropoutWrapper(self.fw_cell, output_keep_prob=) \n",
    "                #input_keep_prob是对输入而言，output_keep_prb是对lstm各层而言\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(self.fw_cell, self.bw_cell, self.sentence_embed, dtype=tf.float32)\n",
    "        #输入为 [batch_size, sentence_len, embed_size]，输出为大小为2的元组，每个元素为[batch_size, sentence_len, hidden_size]\n",
    "        \n",
    "        # c.concat\n",
    "        fw_output = outputs[0][:,-1,:]\n",
    "        bw_output = outputs[1][:,-1,:] #[batch_size, 1, hidden_size]\n",
    "        final_output = tf.concat([fw_output, bw_output], axis=1) #[batch_size, 1, hidden_size*2]\n",
    "        final_output = tf.reshape(final_output, [-1, self.hidden_size*2]) #[batch_size, hidden_size * 2]\n",
    "        \n",
    "        # d.full_connection\n",
    "        logits = tf.matmul(final_output, self.W) + self.b #[batch_size, num_classes]\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, l2_lambda=0.0001):\n",
    "#         loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "        #先将label转化为one-hot形式，再对logits计算softmax，最后计算交叉熵\n",
    "        loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "        loss1 = tf.reduce_mean(tf.reduce_sum(loss1, axis=1))\n",
    "        loss2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) * l2_lambda\n",
    "        return loss1 + loss2\n",
    "    \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_step, learning_rate, optimizer='Adam')\n",
    "        return train_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=5\n",
    "    learning_rate=0.01\n",
    "    batch_size=5\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextRNN(batch_size, num_classes, vocab_size, sequence_length, embed_size, \n",
    "                     learning_rate, decay_step, decay_rate, True)\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 2,size=(batch_size, num_classes), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, logits, _ = sess.run([model.loss_val, model.logits, model.train_op],\n",
    "                                            feed_dict={model.input_x: input_x, model.input_y: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob})\n",
    "            logits = np.argsort(logits)\n",
    "            print('****label****\\n', input_y)\n",
    "            print('****pre_y****\\n', logits, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'W:0' shape=(200, 5) dtype=float32_ref>, <tf.Variable 'b:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/fw/basic_lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/fw/basic_lstm_cell/bias:0' shape=(400,) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/bw/basic_lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/bw/basic_lstm_cell/bias:0' shape=(400,) dtype=float32_ref>]\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 2 0 3 1]\n",
      " [4 2 0 3 1]\n",
      " [4 2 0 3 1]\n",
      " [4 2 0 3 1]\n",
      " [4 2 0 3 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 2 0 3 1]\n",
      " [4 2 0 3 1]\n",
      " [4 2 0 3 1]\n",
      " [4 2 0 3 1]\n",
      " [4 2 0 3 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 2 0 1 3]\n",
      " [4 2 0 1 3]\n",
      " [4 2 0 1 3]\n",
      " [4 2 0 1 3]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 2 1 3]\n",
      " [4 2 0 1 3]\n",
      " [4 2 0 1 3]\n",
      " [0 2 4 1 3]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 2 1 3]\n",
      " [4 2 0 1 3]\n",
      " [4 0 2 1 3]\n",
      " [0 1 2 4 3]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 2 1 3]\n",
      " [4 2 0 1 3]\n",
      " [4 0 2 1 3]\n",
      " [0 1 2 3 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 1 2 3]\n",
      " [4 2 0 1 3]\n",
      " [4 0 2 1 3]\n",
      " [1 0 2 3 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 1 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [0 1 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 1 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 1 4 2 3]\n",
      " [0 1 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 1 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 0 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 1 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 0 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 1 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 0 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 1 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 0 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 1 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 0 3 2 4]\n",
      " [4 2 0 1 3]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 4 1 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 3 0 2 4]\n",
      " [4 2 0 3 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 1 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 3 0 2 4]\n",
      " [4 2 0 3 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 1 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [1 3 0 2 4]\n",
      " [4 2 0 3 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[0 1 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 1 2 3]\n",
      " [3 1 0 2 4]\n",
      " [4 2 0 3 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[1 0 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 2 1 3]\n",
      " [3 1 0 2 4]\n",
      " [4 2 3 0 1]] \n",
      "\n",
      "****label****\n",
      " [[0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 1 0 1 0]]\n",
      "****pre_y****\n",
      " [[1 0 4 2 3]\n",
      " [4 2 0 1 3]\n",
      " [0 4 2 1 3]\n",
      " [3 1 2 0 4]\n",
      " [4 2 3 0 1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#定义超参数\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 1024, 'batch_size')\n",
    "tf.app.flags.DEFINE_integer('num_classes', 1999, 'num_classes')\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 100, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 1000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 10, 'number of epoch')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"testrnn_multilabel_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"textrnn_multilabel_checkpoint/data_cache.pik\",\"data chche for the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = '/data/chenhy/data/ieee_zhihu_cup/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    vocab_size = len(word2index)\n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "\n",
    "    print(\"train_X[0:5]:\", train_X[0:5])\n",
    "    print(\"train_Y[0:5]:\", train_y[0:5])\n",
    "    train_y_short = get_target_label_short(train_y[0])\n",
    "    print(\"train_y_short:\", train_y_short)\n",
    "    \n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextRNN(FLAGS.batch_size, FLAGS.num_classes, vocab_size, FLAGS.sentence_len,FLAGS.embed_size, \n",
    "                        FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.is_training)\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            log(\"restore from checkpoint\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "        else:\n",
    "            log('init variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "#             #是否使用embedding\n",
    "#             print('assign pre-trained embedding')\n",
    "#             embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "#             sess.run(embedding_assign)\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = sess.run(model.epoch_step)\n",
    "                loss, counter = 0., 0.\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    loss_tmp,  _ = sess.run([model.loss_val, model.train_op], \n",
    "                                                    feed_dict={model.input_x: train_X[start:end, :FLAGS.sentence_len], model.input_y: train_y[start:end],\n",
    "                                                               model.dropout_keep_prob: 1})\n",
    "                    loss, counter = loss + loss_tmp, counter + 1\n",
    "                    if counter % 200 == 0:\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\"%(epoch, counter, loss/float(counter)))\n",
    "                    if counter % 3000 == 0:\n",
    "                        print('run model on validation data...')\n",
    "                        loss_valid, f1_score, precision, recall = do_eval(sess, model, vaild_X, valid_y)\n",
    "                        log(\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"%(epoch, loss_valid, f1_score, precision, recall))\n",
    "                        #save the checkpoint\n",
    "                        save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                        saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些辅助函数\n",
    "def get_target_label_short(eval_y):\n",
    "    res = [idx for idx in range(len(eval_y)) if eval_y[idx] > 0] #结果如：[45,100,1555]\n",
    "    return res\n",
    "\n",
    "def get_label_using_logits(logits, top_number=5):\n",
    "    predict_y = [idx for idx in range(len(logits)) if logits[idx] >= 0.5]\n",
    "    if len(predict_y) == 0: predict_y = [np.argmax(logits)]\n",
    "    return predict_y\n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    #embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y\n",
    "\n",
    "def do_eval(sess, model, eval_X, eval_y):\n",
    "    test_X, test_y = eval_X[:3000], eval_y[:3000]\n",
    "    num_of_data = len(test_y)\n",
    "    batch_size = 1\n",
    "    loss, F1, p, r = 0., 0., 0., 0.\n",
    "    label_dict_confuse = {'TP':0.000001, 'FN':0.000001, 'FP':0.000001}\n",
    "    for start in range(num_of_data):\n",
    "        end = start + 1\n",
    "        lo,logits = sess.run([model.loss_val, model.logits], \n",
    "                        feed_dict={model.input_x: test_X[start:end,:FLAGS.sentence_len], model.input_y: test_y[start:end],\n",
    "                                   model.dropout_keep_prob:1.0})\n",
    "        loss += lo\n",
    "        pre = get_label_using_logits(logits[0])\n",
    "        label = get_target_label_short(test_y[start])\n",
    "#         pre = np.argsort(logits[0])[-5:]\n",
    "#         label = [i for i in range(len(test_y[start])) if test_y[start][i] > 0]\n",
    "        if start == 0: print('label:',label, 'predict:', pre)\n",
    "        inter = len([x for x in pre if x in label])\n",
    "        label_dict_confuse['TP'] += inter\n",
    "        label_dict_confuse['FN'] += len(label) - inter\n",
    "        label_dict_confuse['FP'] += len(pre) - inter\n",
    "    print(label_dict_confuse)\n",
    "    p = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FP'])\n",
    "    r = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FN'])\n",
    "    if p + r == 0: return loss/num_of_data, 0, 0, 0\n",
    "    F1 = (2 * p * r)/(p + r)\n",
    "    return loss/num_of_data, F1, p, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X[0:5]: [[ 832   60  256 1172 3407  516   96  138  103 1108   16    3   96  177\n",
      "    22   11  672   53   18 1560 1560   15   65   12  180   10  342  173\n",
      "    13  103  141  707  191   12  342  173   15   13   22   11  229  264\n",
      "   163 1362  135 1249  156  156  731  115   84   10  808 1713  103  141\n",
      "   229  264  788  421  103  141   12   95  316   10  808 1713  103  141\n",
      "    12 2413 1227   15 1397  997   22  116  301  489   12   18  858   99\n",
      "   596   98   26  646  813   10  386 1093  197  767   22   11 1179 1849\n",
      "   593   84   22   11   94  102  322   60  190  220  583  355   10  153\n",
      "   103  141  192  153   12   10 1244  466  116  103  141  189   58  130\n",
      "    95  316   12  788  421  699   16    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 270  154  166  504   53  154   89   60   13  210  259  589   12  151\n",
      "    18 1682   10  154  150  113   97   26  145   67   33   16    3   61\n",
      "    18  118  151   18  948   10  151   18  550   80  123  159   93  154\n",
      "    12  150  186  104  174  537   10  116   66   41   12  117  195   10\n",
      "   104   66   41  116  154   12  117  195   16   95  316  104  590  605\n",
      "   610   19   59   19  202  502  214  516  398   16    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 186  163  284  109  108   32  437   41  232  555  121  177   47   19\n",
      "    67   33  188  309  576  948   12  670  775   16    3    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  32  143  194  453  537  903  450  608   95   10  332   15   47   97\n",
      "   892   75  231  143  238  450   10   97  444  444  314  278   12  166\n",
      "   410   16    3  177  257  338  159   32  143  194   12   18  207   56\n",
      "    75  453  537  903   75  450  608   95   10   75  389  113   12  629\n",
      "    56  342  173   99   13  157  255  629  361   10 1238  268   13  537\n",
      "   903 1950   75   10 2004 1790   53   87   92  188  309  281   75  381\n",
      "  1950   75   12  701  841   10 1917  602  703  763 2205 2574  815  909\n",
      "   909  133   84   26   22   11  177  257  103  301 2739 2162   10   32\n",
      "   143  285  397  593  576  608   54   18  208 1078   10  880 1828  454\n",
      "  1083   10  202  502   32  143  194   75  124  451  173 1426  817   22\n",
      "    11  746  194  399  609   71   13  121  177   10  444  264  424  399\n",
      "  2436  553  241  632  629 1302  847  255   10  125  542  158  724  724\n",
      "   593  650   78  236   10  502  159  168   13  650   53   26   75  389\n",
      "    12  399  609   78  236   10  200   93  612  593  972  807  628  305\n",
      "   461   10 2060 2372]\n",
      " [ 224  461 3115  912  560  833  224  333  224 1499  359  106   83   52\n",
      "    36   30   35   21  253  372  414   96  138   16    3  434  135   13\n",
      "   300  352 1078  351  614  345  189  129   18   62  295   10  230  160\n",
      "   108  437  774 1292  372   18  118  347   22  224  461  130 3115  912\n",
      "   130  560  833  130  224  333  130  224 1499  359  106   83   52   36\n",
      "    30   35   21  253  372  414   96  138   16    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "train_Y[0:5]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_y_short: [1473]\n",
      "[2019/03/21 20:11:12] init variables\n",
      "[2019/03/21 20:14:19] Epoch 0\\Batch 200\\ Train Loss:43.160\n",
      "[2019/03/21 20:18:35] Epoch 0\\Batch 400\\ Train Loss:30.421\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
