{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义FastText模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastText:\n",
    "    def __init__(self, label_size, batch_size, num_sampled, sentence_len, vocab_size, \n",
    "                 embed_size, learning_rate, decay_rate, decay_steps, is_training):\n",
    "        #init all hyperparameter\n",
    "        self.label_size = label_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.sentence_len = sentence_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        #add placeholder \n",
    "        self.sentence = tf.placeholder(dtype=tf.int32, shape=[None, sentence_len], name='sentence') #x\n",
    "        self.labels = tf.placeholder(dtype=tf.int32, shape=[None], name='label') #y\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, name='Global_step')\n",
    "        self.epoch_step = tf.Variable(0, dtype=tf.int32, name='Epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        self.decay_rate, self.decay_steps = decay_rate, decay_steps\n",
    "        \n",
    "        self.instantiate_weights()\n",
    "        self.logits = self.inference()\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "        self.predictions = tf.argmax(self.logits, axis=1, name='predictions')\n",
    "        correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32), self.labels)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='Accuracy')\n",
    "        \n",
    "    def instantiate_weights(self):\n",
    "        self.Embedding = tf.get_variable(name='Embedding', shape=[self.vocab_size, self.embed_size])\n",
    "        self.W = tf.get_variable(name='W', shape=[self.embed_size, self.label_size])\n",
    "        self.b = tf.get_variable(name='b', shape=[self.label_size])\n",
    "        \n",
    "    def inference(self):\n",
    "        sentence_embeddings = tf.nn.embedding_lookup(self.Embedding, self.sentence) #每个单词查表得到词向量，[None, sentence_len, embed_size]\n",
    "        self.sentence_embeddings = tf.reduce_mean(sentence_embeddings, axis=1) #求平均，[None, embed_size]\n",
    "        logits = tf.matmul(self.sentence_embeddings, self.W) + self.b #线性分类器\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, l2_lambda=0.01):\n",
    "        #nce loss\n",
    "        if self.is_training:\n",
    "            labels = tf.reshape(self.labels, shape=[-1])\n",
    "            labels = tf.expand_dims(labels, dim=1)\n",
    "            loss = tf.reduce_mean(tf.nn.nce_loss(weights=tf.transpose(self.W), \n",
    "                                                 biases=self.b, \n",
    "                                                 labels=labels, \n",
    "                                                 inputs=self.sentence_embeddings, \n",
    "                                                 num_sampled=self.num_sampled, \n",
    "                                                 num_classes=self.label_size, \n",
    "                                                 partition_strategy='div'))\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=19\n",
    "    learning_rate=0.01\n",
    "    batch_size=8\n",
    "    decay_steps=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=1\n",
    "    \n",
    "    model = FastText(num_classes, batch_size, 5, sequence_length, vocab_size, \n",
    "                     embed_size, learning_rate, decay_rate, decay_steps, is_training)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 19,size=(batch_size), dtype=np.int32)\n",
    "        for i in range(150):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, acc, predict, _ = sess.run([model.loss_val, model.accuracy, model.predictions, model.train_op],\n",
    "                                            feed_dict={model.sentence: input_x, model.labels: input_y})\n",
    "            print('loss:',loss, 'acc:', acc, 'label:', input_y, 'predict:', predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.61115 acc: 0.0 label: [15 18 12 17 13  6 14  3] predict: [16 16 16 16 16 16 16 16]\n",
      "loss: 5.94749 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [16 14 14 16 14 14 14 16]\n",
      "loss: 6.67814 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [14 14 14 14 14 14 14 14]\n",
      "loss: 7.99756 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [14 14 14 14 14 14 14 14]\n",
      "loss: 7.34867 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [14 14 14 14 14 14 14 14]\n",
      "loss: 7.05527 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [14 14 14 14 14 14 14 14]\n",
      "loss: 5.87476 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [14 14 14 17 14 17 14 14]\n",
      "loss: 4.77102 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 4.19791 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 4.71784 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 7.00724 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 6.59517 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 4.84415 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 4.03674 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 3.5464 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 4.23115 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 5.27526 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 3.19686 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 6.46253 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 10.503 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 5.81224 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 6.12614 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 3.41509 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 1.66921 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 3.1552 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 3.32922 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 2.44812 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 1.70662 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 4.85184 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 12 17 17 17]\n",
      "loss: 0.769923 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 2.52116 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 1.46757 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 4.99517 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.727452 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 1.27151 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 4.11229 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 1.29243 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 4.23412 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17 17]\n",
      "loss: 2.39863 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.865456 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.954344 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.661448 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.736332 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.594321 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.826869 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 1.48176 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 4.19676 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 13 17 17 17]\n",
      "loss: 0.772987 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 1.12783 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 4.36004 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 2.79705 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 1.7415 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.481503 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 2.9434 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 0.611638 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 17]\n",
      "loss: 1.13516 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 16]\n",
      "loss: 3.25819 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 16]\n",
      "loss: 1.31734 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [16 17 17 17 17 17 17 16]\n",
      "loss: 0.719019 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [16 17 17 17 17 17 17 16]\n",
      "loss: 1.31028 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [16 17 17 17 17 17 17 16]\n",
      "loss: 3.36578 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [16 17 17 17 17 17 17 16]\n",
      "loss: 1.99345 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [16 17 17 17 17 17 17 16]\n",
      "loss: 4.81084 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 16]\n",
      "loss: 0.664757 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 16]\n",
      "loss: 2.3441 acc: 0.125 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17 16]\n",
      "loss: 0.562877 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17  3]\n",
      "loss: 2.8854 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17  3]\n",
      "loss: 0.714858 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17  3]\n",
      "loss: 0.604307 acc: 0.25 label: [15 18 12 17 13  6 14  3] predict: [17 17 17 17 17 17 17  3]\n",
      "loss: 1.81408 acc: 0.375 label: [15 18 12 17 13  6 14  3] predict: [17 17 12 17 17 17 17  3]\n",
      "loss: 0.679461 acc: 0.625 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 17  3]\n",
      "loss: 1.49288 acc: 0.625 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 17  3]\n",
      "loss: 0.617227 acc: 0.625 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 17  3]\n",
      "loss: 1.0478 acc: 0.625 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 17  3]\n",
      "loss: 1.47451 acc: 0.625 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 17  3]\n",
      "loss: 0.937882 acc: 0.75 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 14  3]\n",
      "loss: 2.5452 acc: 0.75 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 14  3]\n",
      "loss: 0.904331 acc: 0.75 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 14  3]\n",
      "loss: 1.12695 acc: 0.75 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 14  3]\n",
      "loss: 0.489028 acc: 0.75 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 14  3]\n",
      "loss: 0.440091 acc: 0.75 label: [15 18 12 17 13  6 14  3] predict: [15 17 12 17 17  6 14  3]\n",
      "loss: 1.05026 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.929758 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.88924 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.80544 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.901026 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.881004 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.691098 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 1.4791 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.590046 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.964286 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 1.01787 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.32074 acc: 0.875 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 17  6 14  3]\n",
      "loss: 0.587193 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.525358 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.725717 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.254037 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.558443 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.08465 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.357801 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.843171 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.575029 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.783192 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.29666 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.377832 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.11085 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.320394 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.09144 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.28393 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.327157 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.38753 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.29629 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.919225 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.610105 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.84108 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.403709 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.09705 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.712108 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.716365 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.598112 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.366437 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.313477 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.949813 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.677417 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.815233 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.508673 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.490231 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.811874 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.57712 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.408941 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.19411 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.182423 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.176699 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.18728 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.542199 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.832917 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.723045 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.422403 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.535977 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 1.21587 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.784967 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.900259 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.42676 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.736953 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.660558 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.44497 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.409336 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.317885 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.523429 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n",
      "loss: 0.975948 acc: 1.0 label: [15 18 12 17 13  6 14  3] predict: [15 18 12 17 13  6 14  3]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import word2vec\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define hyperparameter\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('label_size', 1999, 'number of label')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128, 'batch size for training')\n",
    "tf.app.flags.DEFINE_integer('num_sampled', 50, 'number of noise sample')\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 200, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 20000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 15, '')\n",
    "tf.app.flags.DEFINE_integer('validation_every', 1, 'Validate every validate_every epochs.')\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"D:/zhihu_data/data/ieee_zhihu_cup2/fast_text_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"D:/zhihu_data/data/ieee_zhihu_cup2/fast_text_checkpoint/data_cache.pik\",\"data chche for the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define main\n",
    "\n",
    "#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data & training (4.validation) \n",
    "\n",
    "def main(_):\n",
    "\t#1.加载数据\n",
    "\tbase_path = 'D:/zhihu_data/data/ieee_zhihu_cup2/'\n",
    "\tcache_file_h5py = base_path + 'data.h5'\n",
    "\tcache_file_pickle = base_path + 'vocab_label.pik'\n",
    "\tword2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y,embedding_final = load_data(cache_file_h5py, cache_file_pickle)\n",
    "\t\n",
    "\tindex2word = {index: word for word, index in word2index.items()}\n",
    "\tindex2label = {index: label for label, index in label2index.items()}\n",
    "\tvocab_size = len(word2index)\n",
    "\t\n",
    "\tprint(\"train_X.shape:\", np.array(train_X).shape)\n",
    "\tprint(\"train_y.shape:\", np.array(train_y).shape)\n",
    "\tprint(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "\tprint(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "\tprint(\"test_X[0]:\", test_X[0])  \n",
    "\tprint(\"test_X[1]:\", test_X[1])\n",
    "\tprint(\"test_y[0]:\", test_y[0])  \n",
    "\n",
    "\t#2.创建session\n",
    "\tconfig = tf.ConfigProto()\n",
    "\tconfig.gpu_options.allow_growth = True\n",
    "\twith tf.Session(config=config) as sess:\n",
    "\t\tmodel = FastText(FLAGS.label_size, FLAGS.batch_size, FLAGS.num_sampled, FLAGS.sentence_len, \n",
    "\t\t\t\t\t\tvocab_size, FLAGS.embed_size, FLAGS.learning_rate, FLAGS.decay_rate, FLAGS.decay_steps, FLAGS.is_training)\n",
    "\t\tsaver = tf.train.Saver()\n",
    "\t\tif os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "\t\t\tprint('restore model from checkpoint')\n",
    "\t\t\tsaver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "\t\telse:\n",
    "\t\t\tprint('initialize variables')\n",
    "\t\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\t\tprint('assign pre-trained embedding')\n",
    "\t\t\tembedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "\t\t\tsess.run(embedding_assign)\n",
    "\t\t\tcurr_epoch = sess.run(model.epoch_step)\n",
    "\t\t\t\n",
    "\t\t\t#3.训练\n",
    "\t\t\tnum_of_data = len(train_y)\n",
    "\t\t\tbatch_size = FLAGS.batch_size\n",
    "\t\t\tfor epoch in range(FLAGS.num_epoch):\n",
    "\t\t\t\tloss, acc, counter = 0.0, 0.0, 0\n",
    "\t\t\t\tfor start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "\t\t\t\t\tif (epoch == 0 and counter == 0):\n",
    "\t\t\t\t\t\tprint('train_X[start, end]:', train_X[start:end])\n",
    "\t\t\t\t\t\tprint('train_y[start, end]:', train_y[start:end])\n",
    "\t\t\t\t\tl,a,_ = sess.run([model.loss_val, model.accuracy, model.train_op], \n",
    "\t\t\t\t\t\t\t\tfeed_dict={model.sentence: train_X[start:end], model.labels: train_y[start:end]})\n",
    "\t\t\t\t\tloss, acc, counter = loss+l, acc+a, counter+1\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif (counter % 500 == 0):\n",
    "\t\t\t\t\t\tprint(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\\ Train Accuracy:%.3f\"%(epoch, counter, loss/float(counter), acc/float(counter)))\n",
    "\t\t\t\t\n",
    "\t\t\t\t#4.验证，每迭代完FLAGS.validation_every轮，在验证集上跑一次\n",
    "\t\t\t\tprint(epoch,FLAGS.validation_every,(epoch % FLAGS.validation_every==0))\n",
    "\t\t\t\tif epoch % FLAGS.validation_every == 0:\n",
    "\t\t\t\t\tprint('run model on validation data...')\n",
    "\t\t\t\t\tloss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "\t\t\t\t\tprint(\"Epoch %d\\ Validation Loss:%.3f/ Validation Accuracy:%.3f\"%(epoch, loss_valid, acc_valid))\n",
    "\t\t\t\t\t#save the checkpoint\n",
    "\t\t\t\t\tsave_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "\t\t\t\t\tsaver.save(sess, save_path, global_step=model.epoch_step)\n",
    "\t\t\t\tsess.run(model.epoch_increment)\n",
    "\t\tloss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "\t\tprint(\"Epoch %d\\ Validation Loss:%.3f/ Validation Accuracy:%.3f\"%(epoch, loss_valid, acc_valid))\n",
    "\t\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "\tif not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "\t\traise RuntimeError('No such file!!')\n",
    "\t\n",
    "\tprint('cache files exist, going to load in...')\n",
    "\tprint('loading h5_file...')\n",
    "\th5_file = h5py.File(h5_file_path, 'r')\n",
    "\tprint('h5_file.keys:', h5_file.keys())\n",
    "\ttrain_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "\tvaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "\ttest_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "\tembedding_final = h5_file['embedding']\n",
    "\t\n",
    "\tprint('loading pickle file')\n",
    "\tword2index, label2index = None, None\n",
    "\twith open(pik_file_path, 'rb') as pkl:\n",
    "\t\tword2index,label2index = pickle.load(pkl)\n",
    "\tprint('cache files load successful!')\n",
    "\treturn word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y, embedding_final\n",
    "\t\n",
    "def do_eval(sess, model, test_X, test_y, batch_size):\n",
    "\tnum_of_data = len(test_y)\n",
    "\tloss, acc, counter = 0.0, 0.0, 0\n",
    "\tfor start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "\t\tl,a = sess.run([model.loss_val, model.accuracy], \n",
    "\t\t\t\t\t\tfeed_dict={model.sentence: test_X[start:end], model.labels: test_y[start:end]})\n",
    "\t\tloss, acc, counter = loss+l, acc+a, counter+1\n",
    "\treturn loss/float(counter), acc/float(counter)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X.shape: (2959966, 200)\n",
      "train_y.shape: (2959966,)\n",
      "test_X.shape: (20000, 200)\n",
      "test_y.shape: (20000,)\n",
      "test_X[0]: [ 579  343 1173 1843    5  583  292 1173 1843    5 1180 1299  989   10\n",
      "    2   68  153  168  531  109  260  217  277   81   59   81  116  514\n",
      "    6  221  253  224  154  718  553    4  806  538  732  264   74    6\n",
      "  221  224  154  326   11  167  136    4  257  145   37   74  175  214\n",
      "   11   57  110  221    6  364   89   20 4050 2344    4  257   78    9\n",
      "  991  326  221   89  699  133   11  597  679 1957  824  884  871 1957\n",
      "  824    4  178   87   87   78  196   52  552   69   47   20   12   37\n",
      " 1371   89    6  755  779   81  667  597    4  586  878    6   35   93\n",
      "    7  719  285  937   35  162   13   11    7 1371   89   35    4  201\n",
      "   68   81   97 1533   81  667  597    9  991  326   35  343  704   16\n",
      "    5   99   13    9  991  654  583  292    4   13  221    6  795  230\n",
      "   11   11  350   12  495  235    7  990  625  718  553  297  215  954\n",
      "  549    4   12  165  198   67   93    9  166  110  146    4   81   86\n",
      "   93  141   87 1146  118  224  154   93  147    9   20    4   81  407\n",
      "   92  116  514   12]\n",
      "test_X[1]: [  52   61   27  505  319  131  491 1514 1514    9  110   24  325    8\n",
      "   28  424  601  664  152  128  838 1292 1047  549   10    2    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "test_y[0]: 808\n",
      "initialize variables\n",
      "assign pre-trained embedding\n",
      "train_X[start, end]: [[  90  132  541 ...    0    0    0]\n",
      " [ 217  190   48 ...   50  336  167]\n",
      " [  11   54  162 ...    0    0    0]\n",
      " ...\n",
      " [ 149  348  815 ...    0    0    0]\n",
      " [ 377  665 1478 ...    0    0    0]\n",
      " [  52   61   27 ...    0    0    0]]\n",
      "train_y[start, end]: [1345  510  135 1699  991  108  921 1695 1004  703  311  350 1418  573\n",
      "  156 1792  106  607 1606  497 1574 1089 1250  359 1630   30  735 1831\n",
      " 1682 1259 1428 1547  183 1695 1057  363 1168  349 1679   33  731  380\n",
      "   81 1076 1196 1275 1957 1969 1491 1435 1911  140 1673  967 1212 1877\n",
      "  485  280  508 1302  797  334 1423 1863   33  857 1927  117 1287    3\n",
      " 1968  581 1276 1793 1000   55    8 1365   31 1945   97 1835 1460 1973\n",
      "  809  947  151  160  941 1482 1902   70  102  816  491  269  694  174\n",
      "  984  695 1075  376 1110 1939  564  465  283  612 1042  610  686  913\n",
      " 1827  836 1194 1215  549  996 1266 1114 1491 1872  801 1388 1433 1911\n",
      "  362 1171]\n",
      "Epoch 0\\Batch 500\\ Train Loss:52.809\\ Train Accuracy:0.001\n",
      "Epoch 0\\Batch 1000\\ Train Loss:30.481\\ Train Accuracy:0.001\n",
      "Epoch 0\\Batch 1500\\ Train Loss:22.003\\ Train Accuracy:0.001\n",
      "Epoch 0\\Batch 2000\\ Train Loss:17.689\\ Train Accuracy:0.001\n",
      "Epoch 0\\Batch 2500\\ Train Loss:15.098\\ Train Accuracy:0.002\n",
      "Epoch 0\\Batch 3000\\ Train Loss:13.370\\ Train Accuracy:0.002\n",
      "Epoch 0\\Batch 3500\\ Train Loss:12.105\\ Train Accuracy:0.003\n",
      "Epoch 0\\Batch 4000\\ Train Loss:11.131\\ Train Accuracy:0.003\n",
      "Epoch 0\\Batch 4500\\ Train Loss:10.346\\ Train Accuracy:0.004\n",
      "Epoch 0\\Batch 5000\\ Train Loss:9.702\\ Train Accuracy:0.005\n",
      "Epoch 0\\Batch 5500\\ Train Loss:9.159\\ Train Accuracy:0.006\n",
      "Epoch 0\\Batch 6000\\ Train Loss:8.698\\ Train Accuracy:0.007\n",
      "Epoch 0\\Batch 6500\\ Train Loss:8.295\\ Train Accuracy:0.009\n",
      "Epoch 0\\Batch 7000\\ Train Loss:7.942\\ Train Accuracy:0.010\n",
      "Epoch 0\\Batch 7500\\ Train Loss:7.629\\ Train Accuracy:0.012\n",
      "Epoch 0\\Batch 8000\\ Train Loss:7.350\\ Train Accuracy:0.014\n",
      "Epoch 0\\Batch 8500\\ Train Loss:7.098\\ Train Accuracy:0.016\n",
      "Epoch 0\\Batch 9000\\ Train Loss:6.869\\ Train Accuracy:0.018\n",
      "Epoch 0\\Batch 9500\\ Train Loss:6.663\\ Train Accuracy:0.019\n",
      "Epoch 0\\Batch 10000\\ Train Loss:6.474\\ Train Accuracy:0.021\n",
      "Epoch 0\\Batch 10500\\ Train Loss:6.301\\ Train Accuracy:0.023\n",
      "Epoch 0\\Batch 11000\\ Train Loss:6.142\\ Train Accuracy:0.025\n",
      "Epoch 0\\Batch 11500\\ Train Loss:5.995\\ Train Accuracy:0.027\n",
      "Epoch 0\\Batch 12000\\ Train Loss:5.858\\ Train Accuracy:0.030\n",
      "Epoch 0\\Batch 12500\\ Train Loss:5.731\\ Train Accuracy:0.032\n",
      "Epoch 0\\Batch 13000\\ Train Loss:5.613\\ Train Accuracy:0.035\n",
      "Epoch 0\\Batch 13500\\ Train Loss:5.501\\ Train Accuracy:0.037\n",
      "Epoch 0\\Batch 14000\\ Train Loss:5.398\\ Train Accuracy:0.039\n",
      "Epoch 0\\Batch 14500\\ Train Loss:5.301\\ Train Accuracy:0.041\n",
      "Epoch 0\\Batch 15000\\ Train Loss:5.208\\ Train Accuracy:0.043\n",
      "Epoch 0\\Batch 15500\\ Train Loss:5.123\\ Train Accuracy:0.045\n",
      "Epoch 0\\Batch 16000\\ Train Loss:5.042\\ Train Accuracy:0.047\n",
      "Epoch 0\\Batch 16500\\ Train Loss:4.965\\ Train Accuracy:0.050\n",
      "Epoch 0\\Batch 17000\\ Train Loss:4.892\\ Train Accuracy:0.052\n",
      "Epoch 0\\Batch 17500\\ Train Loss:4.822\\ Train Accuracy:0.054\n",
      "Epoch 0\\Batch 18000\\ Train Loss:4.756\\ Train Accuracy:0.056\n",
      "Epoch 0\\Batch 18500\\ Train Loss:4.694\\ Train Accuracy:0.058\n",
      "Epoch 0\\Batch 19000\\ Train Loss:4.634\\ Train Accuracy:0.059\n",
      "Epoch 0\\Batch 19500\\ Train Loss:4.578\\ Train Accuracy:0.061\n",
      "Epoch 0\\Batch 20000\\ Train Loss:4.525\\ Train Accuracy:0.063\n",
      "Epoch 0\\Batch 20500\\ Train Loss:4.471\\ Train Accuracy:0.065\n",
      "Epoch 0\\Batch 21000\\ Train Loss:4.420\\ Train Accuracy:0.067\n",
      "Epoch 0\\Batch 21500\\ Train Loss:4.371\\ Train Accuracy:0.069\n",
      "Epoch 0\\Batch 22000\\ Train Loss:4.324\\ Train Accuracy:0.071\n",
      "Epoch 0\\Batch 22500\\ Train Loss:4.280\\ Train Accuracy:0.073\n",
      "Epoch 0\\Batch 23000\\ Train Loss:4.237\\ Train Accuracy:0.075\n",
      "0 1 True\n",
      "run model on validation data...\n",
      "Epoch 0\\ Validation Loss:2.329/ Validation Accuracy:0.149\n",
      "Epoch 1\\Batch 500\\ Train Loss:2.298\\ Train Accuracy:0.152\n",
      "Epoch 1\\Batch 1000\\ Train Loss:2.297\\ Train Accuracy:0.156\n",
      "Epoch 1\\Batch 1500\\ Train Loss:2.293\\ Train Accuracy:0.159\n",
      "Epoch 1\\Batch 2000\\ Train Loss:2.290\\ Train Accuracy:0.162\n",
      "Epoch 1\\Batch 2500\\ Train Loss:2.287\\ Train Accuracy:0.161\n",
      "Epoch 1\\Batch 3000\\ Train Loss:2.284\\ Train Accuracy:0.161\n",
      "Epoch 1\\Batch 3500\\ Train Loss:2.282\\ Train Accuracy:0.162\n",
      "Epoch 1\\Batch 4000\\ Train Loss:2.279\\ Train Accuracy:0.163\n",
      "Epoch 1\\Batch 4500\\ Train Loss:2.276\\ Train Accuracy:0.164\n",
      "Epoch 1\\Batch 5000\\ Train Loss:2.271\\ Train Accuracy:0.166\n",
      "Epoch 1\\Batch 5500\\ Train Loss:2.269\\ Train Accuracy:0.167\n",
      "Epoch 1\\Batch 6000\\ Train Loss:2.266\\ Train Accuracy:0.168\n",
      "Epoch 1\\Batch 6500\\ Train Loss:2.263\\ Train Accuracy:0.168\n",
      "Epoch 1\\Batch 7000\\ Train Loss:2.261\\ Train Accuracy:0.169\n",
      "Epoch 1\\Batch 7500\\ Train Loss:2.260\\ Train Accuracy:0.169\n",
      "Epoch 1\\Batch 8000\\ Train Loss:2.256\\ Train Accuracy:0.170\n",
      "Epoch 1\\Batch 8500\\ Train Loss:2.254\\ Train Accuracy:0.170\n",
      "Epoch 1\\Batch 9000\\ Train Loss:2.252\\ Train Accuracy:0.170\n",
      "Epoch 1\\Batch 9500\\ Train Loss:2.249\\ Train Accuracy:0.171\n",
      "Epoch 1\\Batch 10000\\ Train Loss:2.247\\ Train Accuracy:0.172\n",
      "Epoch 1\\Batch 10500\\ Train Loss:2.245\\ Train Accuracy:0.172\n",
      "Epoch 1\\Batch 11000\\ Train Loss:2.243\\ Train Accuracy:0.173\n",
      "Epoch 1\\Batch 11500\\ Train Loss:2.241\\ Train Accuracy:0.173\n",
      "Epoch 1\\Batch 12000\\ Train Loss:2.240\\ Train Accuracy:0.173\n",
      "Epoch 1\\Batch 12500\\ Train Loss:2.238\\ Train Accuracy:0.174\n",
      "Epoch 1\\Batch 13000\\ Train Loss:2.236\\ Train Accuracy:0.174\n",
      "Epoch 1\\Batch 13500\\ Train Loss:2.234\\ Train Accuracy:0.174\n",
      "Epoch 1\\Batch 14000\\ Train Loss:2.231\\ Train Accuracy:0.175\n",
      "Epoch 1\\Batch 14500\\ Train Loss:2.230\\ Train Accuracy:0.175\n",
      "Epoch 1\\Batch 15000\\ Train Loss:2.227\\ Train Accuracy:0.176\n",
      "Epoch 1\\Batch 15500\\ Train Loss:2.226\\ Train Accuracy:0.176\n",
      "Epoch 1\\Batch 16000\\ Train Loss:2.224\\ Train Accuracy:0.176\n",
      "Epoch 1\\Batch 16500\\ Train Loss:2.223\\ Train Accuracy:0.177\n",
      "Epoch 1\\Batch 17000\\ Train Loss:2.221\\ Train Accuracy:0.177\n",
      "Epoch 1\\Batch 17500\\ Train Loss:2.218\\ Train Accuracy:0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\\Batch 18000\\ Train Loss:2.216\\ Train Accuracy:0.179\n",
      "Epoch 1\\Batch 18500\\ Train Loss:2.212\\ Train Accuracy:0.179\n",
      "Epoch 1\\Batch 19000\\ Train Loss:2.209\\ Train Accuracy:0.180\n",
      "Epoch 1\\Batch 19500\\ Train Loss:2.207\\ Train Accuracy:0.180\n",
      "Epoch 1\\Batch 20000\\ Train Loss:2.206\\ Train Accuracy:0.181\n",
      "Epoch 1\\Batch 20500\\ Train Loss:2.204\\ Train Accuracy:0.181\n",
      "Epoch 1\\Batch 21000\\ Train Loss:2.201\\ Train Accuracy:0.181\n",
      "Epoch 1\\Batch 21500\\ Train Loss:2.199\\ Train Accuracy:0.182\n",
      "Epoch 1\\Batch 22000\\ Train Loss:2.198\\ Train Accuracy:0.182\n",
      "Epoch 1\\Batch 22500\\ Train Loss:2.196\\ Train Accuracy:0.183\n",
      "Epoch 1\\Batch 23000\\ Train Loss:2.194\\ Train Accuracy:0.183\n",
      "1 1 True\n",
      "run model on validation data...\n",
      "Epoch 1\\ Validation Loss:2.161/ Validation Accuracy:0.199\n",
      "Epoch 2\\Batch 500\\ Train Loss:2.109\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 1000\\ Train Loss:2.113\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 1500\\ Train Loss:2.108\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 2000\\ Train Loss:2.104\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 2500\\ Train Loss:2.106\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 3000\\ Train Loss:2.105\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 3500\\ Train Loss:2.106\\ Train Accuracy:0.204\n",
      "Epoch 2\\Batch 4000\\ Train Loss:2.105\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 4500\\ Train Loss:2.103\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 5000\\ Train Loss:2.100\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 5500\\ Train Loss:2.100\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 6000\\ Train Loss:2.098\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 6500\\ Train Loss:2.099\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 7000\\ Train Loss:2.099\\ Train Accuracy:0.205\n",
      "Epoch 2\\Batch 7500\\ Train Loss:2.098\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 8000\\ Train Loss:2.097\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 8500\\ Train Loss:2.096\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 9000\\ Train Loss:2.094\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 9500\\ Train Loss:2.092\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 10000\\ Train Loss:2.091\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 10500\\ Train Loss:2.090\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 11000\\ Train Loss:2.090\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 11500\\ Train Loss:2.090\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 12000\\ Train Loss:2.089\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 12500\\ Train Loss:2.089\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 13000\\ Train Loss:2.090\\ Train Accuracy:0.206\n",
      "Epoch 2\\Batch 13500\\ Train Loss:2.089\\ Train Accuracy:0.207\n",
      "Epoch 2\\Batch 14000\\ Train Loss:2.087\\ Train Accuracy:0.207\n",
      "Epoch 2\\Batch 14500\\ Train Loss:2.085\\ Train Accuracy:0.207\n",
      "Epoch 2\\Batch 15000\\ Train Loss:2.082\\ Train Accuracy:0.208\n",
      "Epoch 2\\Batch 15500\\ Train Loss:2.080\\ Train Accuracy:0.208\n",
      "Epoch 2\\Batch 16000\\ Train Loss:2.079\\ Train Accuracy:0.209\n",
      "Epoch 2\\Batch 16500\\ Train Loss:2.077\\ Train Accuracy:0.209\n",
      "Epoch 2\\Batch 17000\\ Train Loss:2.076\\ Train Accuracy:0.210\n",
      "Epoch 2\\Batch 17500\\ Train Loss:2.075\\ Train Accuracy:0.210\n",
      "Epoch 2\\Batch 18000\\ Train Loss:2.074\\ Train Accuracy:0.210\n",
      "Epoch 2\\Batch 18500\\ Train Loss:2.073\\ Train Accuracy:0.210\n",
      "Epoch 2\\Batch 19000\\ Train Loss:2.071\\ Train Accuracy:0.211\n",
      "Epoch 2\\Batch 19500\\ Train Loss:2.070\\ Train Accuracy:0.211\n",
      "Epoch 2\\Batch 20000\\ Train Loss:2.069\\ Train Accuracy:0.211\n",
      "Epoch 2\\Batch 20500\\ Train Loss:2.069\\ Train Accuracy:0.211\n"
     ]
    }
   ],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  8  4  1  1\n",
       "1  0  2  2  4\n",
       "2  4  4  1  3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0    8\n",
      "1    4\n",
      "2    1\n",
      "3    1\n",
      "Name: 0, dtype: int32\n",
      "1 0    0\n",
      "1    2\n",
      "2    2\n",
      "3    4\n",
      "Name: 1, dtype: int32\n",
      "2 0    4\n",
      "1    4\n",
      "2    1\n",
      "3    3\n",
      "Name: 2, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "for idx, row in b.iterrows():\n",
    "    print(idx, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
