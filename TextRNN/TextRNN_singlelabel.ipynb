{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义TextRNN结构，使用双向的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN:\n",
    "    def __init__(self, batch_size, num_classes, vocab_size, sentence_len, embed_size, \n",
    "                 learning_rate, decay_steps, decay_rate, is_training):\n",
    "        #1.定义超参数\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = embed_size #lstm层的维度\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "        \n",
    "        #2.输入\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sentence_len], 'input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], 'input_y') #单个标签\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        #3.初始化全连接层参数\n",
    "        self.init_weight()\n",
    "        \n",
    "        #4.网络结构\n",
    "        self.logits = self.inference() #[batch_size, num_classes]\n",
    "        \n",
    "        #5.损失函数\n",
    "        self.loss_val = self.loss()\n",
    "        \n",
    "        #6.优化器\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "        #7.计算acc\n",
    "        self.prediction = tf.argmax(self.logits, axis=1) #[batch_size]\n",
    "        is_right = tf.equal(tf.cast(self.prediction, tf.int32), self.input_y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(is_right, tf.float32))\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], dtype=tf.float32)\n",
    "        self.W = tf.get_variable('W', [self.hidden_size * 2, self.num_classes], dtype=tf.float32) #双向LSTM，输出concat，所以此处为2倍\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        # a.embedding\n",
    "        self.sentence_embed = tf.nn.embedding_lookup(self.Embedding, self.input_x) #[batch_size, sentence_len, embed_size]\n",
    "        \n",
    "        # b.bidiretional lstm\n",
    "        self.fw_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size) #前向单元\n",
    "        self.bw_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size) #后向单元\n",
    "#         if self.dropout_keep_prob is not None:\n",
    "#             self.fw_cell = tf.contrib.rnn.DropoutWrapper(self.fw_cell, output_keep_prob=) \n",
    "                #input_keep_prob是对输入而言，output_keep_prb是对lstm各层而言\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(self.fw_cell, self.bw_cell, self.sentence_embed, dtype=tf.float32)\n",
    "        #输入为 [batch_size, sentence_len, embed_size]，输出为大小为2的元组，每个元素为[batch_size, sentence_len, hidden_size]\n",
    "        \n",
    "        # c.concat\n",
    "        fw_output = outputs[0][:,-1,:]\n",
    "        bw_output = outputs[1][:,-1,:] #[batch_size, 1, hidden_size]\n",
    "        final_output = tf.concat([fw_output, bw_output], axis=1) #[batch_size, 1, hidden_size*2]\n",
    "        final_output = tf.reshape(final_output, [-1, self.hidden_size*2]) #[batch_size, hidden_size * 2]\n",
    "        \n",
    "        # d.full_connection\n",
    "        logits = tf.matmul(final_output, self.W) + self.b #[batch_size, num_classes]\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, l2_lambda=0.0001):\n",
    "        loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "        #先将label转化为one-hot形式，再对logits计算softmax，最后计算交叉熵\n",
    "        loss1 = tf.reduce_mean(loss1)\n",
    "        loss2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) * l2_lambda\n",
    "        return loss1 + loss2\n",
    "    \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_step, learning_rate, optimizer='Adam')\n",
    "        return train_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=19\n",
    "    learning_rate=0.01\n",
    "    batch_size=15\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextRNN(batch_size, num_classes, vocab_size, sequence_length, embed_size, \n",
    "                     learning_rate, decay_step, decay_rate, True)\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 19,size=(batch_size), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, acc, predict, _ = sess.run([model.loss_val, model.accuracy, model.prediction, model.train_op],\n",
    "                                            feed_dict={model.input_x: input_x, model.input_y: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob})\n",
    "            print('loss:',loss, 'acc:', acc, 'label:', input_y, 'predict:', predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'W:0' shape=(200, 19) dtype=float32_ref>, <tf.Variable 'b:0' shape=(19,) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/fw/basic_lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/fw/basic_lstm_cell/bias:0' shape=(400,) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/bw/basic_lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/bw/basic_lstm_cell/bias:0' shape=(400,) dtype=float32_ref>]\n",
      "loss: 3.0606935 acc: 0.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "loss: 2.9713626 acc: 0.26666668 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 0  8  8  8  0  2  8  8 11  3  8  8  8  8  2]\n",
      "loss: 2.8465364 acc: 0.4 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 0  0 12  0  0  0  0  0 11  3  0 14 14  0  2]\n",
      "loss: 2.6395717 acc: 0.6 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 12  0  2 14  0 11  3 12  6 14  0  2]\n",
      "loss: 2.3024232 acc: 0.73333335 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 12  0  6  6 15 11  3 10  6  6 13  2]\n",
      "loss: 1.8482826 acc: 0.6 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4  4  4  0  5  4 15  6  3  4  6  6 13  2]\n",
      "loss: 1.4488752 acc: 0.6 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4  4  4  0  5  4 15  6  3  4  6  6 13  2]\n",
      "loss: 0.9307046 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.53827864 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.29934132 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.15794447 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.09056812 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.06324652 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.05577219 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.055659775 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.057538383 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.059856422 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.062177885 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.06436753 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n",
      "loss: 0.06636828 acc: 1.0 label: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2] predict: [ 4  4 12 16  0  5 17 15 11  3 10  6 14 13  2]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#定义超参数\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 512, 'batch_size')\n",
    "tf.app.flags.DEFINE_integer('num_classes', 1999, 'num_classes')\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 100, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 3000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 10, 'number of epoch')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"testrnn_singlelabel_checkpoint_noembed/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"textrnn_singlelabel_checkpoint_noembed/data_cache.pik\",\"data chche for the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = '../zhihu_data/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y,embedding_final = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    vocab_size = len(word2index)\n",
    "    \n",
    "    print(\"train_X.shape:\", np.array(train_X).shape)\n",
    "    print(\"train_y.shape:\", np.array(train_y).shape)\n",
    "    print(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "    print(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "    print(\"test_X[0]:\", test_X[0])  \n",
    "    print(\"test_y[0]:\", test_y[0]) \n",
    "    \n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextRNN(FLAGS.batch_size, FLAGS.num_classes, vocab_size, FLAGS.sentence_len,FLAGS.embed_size, \n",
    "                        FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.is_training)\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            log(\"restore from checkpoint\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "        else:\n",
    "            log('init variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "#             #是否使用embedding\n",
    "#             print('assign pre-trained embedding')\n",
    "#             embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "#             sess.run(embedding_assign)\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = sess.run(model.epoch_step)\n",
    "                loss, acc, counter = 0., 0., 0.\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    loss_tmp, acc_tmp, _ = sess.run([model.loss_val, model.accuracy, model.train_op], \n",
    "                                                    feed_dict={model.input_x: train_X[start:end,:100], model.input_y: train_y[start:end],\n",
    "                                                               model.dropout_keep_prob: 1})\n",
    "                    loss, acc, counter = loss + loss_tmp, acc + acc_tmp, counter + 1\n",
    "                    if counter % 200 == 0:\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\\ Train Accuracy:%.3f\"%(epoch, counter, loss/float(counter), acc/float(counter)))\n",
    "\n",
    "                print('run model on validation data...')\n",
    "                loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "                log(\"Epoch %d\\ Validation Loss:%.3f/ Validation Accuracy:%.3f\"%(epoch, loss_valid, acc_valid))\n",
    "                #save the checkpoint\n",
    "                save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "            loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "            log(\"Validation Loss:%.3f\\ Validation Accuracy:%.3f\"%(loss_valid, acc_valid))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y, embedding_final\n",
    "\n",
    "def do_eval(sess, model, test_X, test_y, batch_size):\n",
    "    test_X, test_y = test_X[:5000], test_y[:5000]\n",
    "    num_of_data = len(test_y)\n",
    "    loss, acc, counter = 0.0, 0.0, 0\n",
    "    for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "        l,a = sess.run([model.loss_val, model.accuracy], \n",
    "                        feed_dict={model.sentence: test_X[start:end,:100], model.label: test_y[start:end], model.dropout_keep_prob:1.0})\n",
    "        loss, acc, counter = loss+l, acc+a, counter+1\n",
    "    return loss/float(counter), acc/float(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X.shape: (2959966, 200)\n",
      "train_y.shape: (2959966,)\n",
      "test_X.shape: (20000, 200)\n",
      "test_y.shape: (20000,)\n",
      "test_X[0]: [ 579  343 1173 1843    5  583  292 1173 1843    5 1180 1299  989   10\n",
      "    2   68  153  168  531  109  260  217  277   81   59   81  116  514\n",
      "    6  221  253  224  154  718  553    4  806  538  732  264   74    6\n",
      "  221  224  154  326   11  167  136    4  257  145   37   74  175  214\n",
      "   11   57  110  221    6  364   89   20 4050 2344    4  257   78    9\n",
      "  991  326  221   89  699  133   11  597  679 1957  824  884  871 1957\n",
      "  824    4  178   87   87   78  196   52  552   69   47   20   12   37\n",
      " 1371   89    6  755  779   81  667  597    4  586  878    6   35   93\n",
      "    7  719  285  937   35  162   13   11    7 1371   89   35    4  201\n",
      "   68   81   97 1533   81  667  597    9  991  326   35  343  704   16\n",
      "    5   99   13    9  991  654  583  292    4   13  221    6  795  230\n",
      "   11   11  350   12  495  235    7  990  625  718  553  297  215  954\n",
      "  549    4   12  165  198   67   93    9  166  110  146    4   81   86\n",
      "   93  141   87 1146  118  224  154   93  147    9   20    4   81  407\n",
      "   92  116  514   12]\n",
      "test_y[0]: 808\n",
      "[2019/03/21 20:13:48] init variables\n",
      "[2019/03/21 20:15:58] Epoch 0\\Batch 200\\ Train Loss:7.516\\ Train Accuracy:0.006\n",
      "[2019/03/21 20:18:11] Epoch 0\\Batch 400\\ Train Loss:7.489\\ Train Accuracy:0.008\n",
      "[2019/03/21 20:20:22] Epoch 0\\Batch 600\\ Train Loss:7.425\\ Train Accuracy:0.009\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
