{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单标签模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, num_classes, batch_size, vocab_size, embed_size, sentence_len, \n",
    "                 learning_rate, decay_step, decay_rate, filter_num, filter_sizes):\n",
    "        #1.定义超参数\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.learning_rate = learning_rate\n",
    "        self.filter_num = filter_num\n",
    "        self.filter_sizes = filter_sizes #list，如[2,3,4],表示3个卷积核的长度（height）\n",
    "        self.filter_num_total = filter_num * len(filter_sizes)\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_epoch = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_epoch') #在指数衰减函数中会加一\n",
    "        self.epoch_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, self.epoch_step+tf.constant(1))\n",
    "        self.decay_step = decay_step\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        #2.设置输入\n",
    "        self.sentence = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len], name='sentence')\n",
    "        self.label = tf.placeholder(dtype=tf.int32, shape=[None], name='label')\n",
    "        self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "        #self.dropout_keep_prob = 0.5\n",
    "        \n",
    "        #3.参数初始化\n",
    "        self.instantiate_weight()\n",
    "        #4.定义图\n",
    "        self.logits = self.inference()\n",
    "        \n",
    "        #5.定义loss和train_op\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "        #6.预测，计算准确率\n",
    "        self.prediction = tf.argmax(self.logits, axis=1, name='prediction')\n",
    "        correct_pre = tf.equal(tf.cast(self.prediction, tf.int32), self.label)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "    \n",
    "    def instantiate_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], tf.float32, initializer=self.initializer)\n",
    "        self.W = tf.get_variable('weight', [self.filter_num_total, self.num_classes], tf.float32, initializer=self.initializer)\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        #embedding -- 卷积 -- 线性分类器\n",
    "        self.sentece_embedding = tf.nn.embedding_lookup(self.Embedding, self.sentence)\n",
    "        h = self.cnn_single_layer()\n",
    "        logits = tf.matmul(h, self.W) + self.b\n",
    "        return logits\n",
    "    \n",
    "    def cnn_single_layer(self):\n",
    "        #conv2d -- BN -- ReLU -- max_pooling -- dropout -- dense\n",
    "        #conv2d的输入与卷积核都要求是4维的，具体查看文档\n",
    "        sentece_embedding_4d = tf.expand_dims(self.sentece_embedding, -1) #增加一维，[batch_size, sentence_len, embed_size, 1]\n",
    "        pool_output = []\n",
    "        for filter_size in self.filter_sizes:\n",
    "            with tf.variable_scope('convolution-pooling-%d'%filter_size):\n",
    "                ft = tf.get_variable('filter%d'%filter_size, [filter_size, self.embed_size, 1, self.filter_num], \n",
    "                                     tf.float32, initializer=self.initializer)\n",
    "                conv = tf.nn.conv2d(sentece_embedding_4d, ft, strides=[1,1,1,1], padding='VALID')\n",
    "                conv = tf.contrib.layers.batch_norm(conv) #[batch_size, sentence_len-filter_size+1, 1, filter_num]\n",
    "                activation = tf.nn.relu(conv)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(activation, ksize=[1,self.sentence_len-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID')\n",
    "                pool_output.append(pooled) #若干个shape=[batch_size, 1, 1, filter_num]\n",
    "        pool_concat = tf.concat(pool_output, axis=3) #在第三维拼接\n",
    "        flatten_pool = tf.reshape(pool_concat, [-1, self.filter_num_total])\n",
    "        \n",
    "        dropouted = tf.nn.dropout(flatten_pool, keep_prob=self.dropout_keep_prob)\n",
    "        h = tf.layers.dense(dropouted, self.filter_num_total, activation=tf.nn.tanh)\n",
    "        return h\n",
    "        \n",
    "    def loss(self, l2_lambda=0.001):\n",
    "        loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
    "        #先将labels转化为one-hot，再计算softmax交叉熵\n",
    "        loss1 = tf.reduce_mean(loss1)\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name], name='l2_loss') * l2_lambda\n",
    "        loss = loss1 + l2_loss\n",
    "        return loss\n",
    "                \n",
    "    def train(self):\n",
    "        \"\"\"based on the loss, use SGD to update parameter\"\"\"\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_epoch, self.decay_step, self.decay_rate, staircase=True)\n",
    "        self.learning_rate_=learning_rate\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(self.loss_val))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) #ADD 2018.06.01\n",
    "        with tf.control_dependencies(update_ops):  #ADD 2018.06.01\n",
    "            train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return train_op\n",
    "    \n",
    "#         learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_epoch, \n",
    "#                                                    self.decay_step, self.decay_rate, staircase=True)\n",
    "#         train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_epoch, learning_rate, optimizer='Adam')\n",
    "#         return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=19\n",
    "    learning_rate=0.01\n",
    "    batch_size=15\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextCNN(num_classes, batch_size, vocab_size, embed_size, sequence_length,\n",
    "                     learning_rate, decay_step, decay_rate, 50, [2,3,4])\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 19,size=(batch_size), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, acc, predict, _ = sess.run([model.loss_val, model.accuracy, model.prediction, model.train_op],\n",
    "                                            feed_dict={model.sentence: input_x, model.label: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob})\n",
    "            print('loss:',loss, 'acc:', acc, 'label:', input_y, 'predict:', predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'weight:0' shape=(150, 19) dtype=float32_ref>, <tf.Variable 'b:0' shape=(19,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/filter2:0' shape=(2, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/filter3:0' shape=(3, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/filter4:0' shape=(4, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(150, 150) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(150,) dtype=float32_ref>]\n",
      "loss: 8.662104 acc: 0.2 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 14 13 14  0  0 13 15 13  3  0 13  4 16 16]\n",
      "loss: 5.7021413 acc: 0.8666667 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13 10  0  6 10 11]\n",
      "loss: 4.7735806 acc: 0.93333334 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8  6 13  6  0  6  5 11]\n",
      "loss: 3.7739697 acc: 0.93333334 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8  8 13  6  0  6  5 11]\n",
      "loss: 3.0548704 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 2.6413023 acc: 0.93333334 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  0 18 13  6  0  6  5 11]\n",
      "loss: 2.2667234 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 1.916216 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 1.6580796 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 1.4276555 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 1.2643507 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 1.1496302 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 1.0029205 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.9196123 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.85562944 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.7837931 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.7331218 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.693634 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.6655943 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n",
      "loss: 0.6430157 acc: 1.0 label: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11] predict: [15 17  8 14  0 10 10  8 18 13  6  0  6  5 11]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define hyperparameter\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('label_size', 1999, 'number of label')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size for training')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 200, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0003, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 1000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 10, '')\n",
    "tf.app.flags.DEFINE_integer('validation_every', 1, 'Validate every validate_every epochs.')\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"../zhihu_data/textcnn_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"../zhihu_data/textcnn_checkpoint/data_cache.pik\",\"data chche for the model\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"num_filters\", 64, \"number of filters\") #256--->512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main\n",
    "\n",
    "#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data & training (4.validation) \n",
    "\n",
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = '../zhihu_data/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y,embedding_final = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    \n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "\n",
    "    print(\"train_X.shape:\", np.array(train_X).shape)\n",
    "    print(\"train_y.shape:\", np.array(train_y).shape)\n",
    "    print(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "    print(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "    print(\"test_X[0]:\", test_X[0])  \n",
    "    print(\"test_X[1]:\", test_X[1])\n",
    "    print(\"test_y[0]:\", test_y[0])  \n",
    "\n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextCNN(FLAGS.label_size, FLAGS.batch_size, vocab_size, \n",
    "                        FLAGS.embed_size, FLAGS.sentence_len, FLAGS.learning_rate, \n",
    "                        FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.num_filters, [3,4,5])\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        CONTINUE_TRAIN = False\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            print('restore model from checkpoint')\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            print('CONTINUE_TRAIN=', CONTINUE_TRAIN)\n",
    "            sess.run(model.epoch_increment)\n",
    "            print('Continue at Epoch:', sess.run(model.epoch_step))\n",
    "        if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint') or CONTINUE_TRAIN:\n",
    "            if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "                print('initialize variables')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                print('assign pre-trained embedding')\n",
    "                embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "                sess.run(embedding_assign)\n",
    "\n",
    "            #3.训练\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = sess.run(model.epoch_step)\n",
    "                loss, acc, counter = 0.0, 0.0, 0\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    if (epoch == 0 and counter == 0):\n",
    "                        print('train_X[start, end]:', train_X[start:end])\n",
    "                        print('train_y[start, end]:', train_y[start:end])\n",
    "                    l,a,_ = sess.run([model.loss_val, model.accuracy, model.train_op], \n",
    "                                feed_dict={model.sentence: train_X[start:end], model.label: train_y[start:end],\n",
    "                                           model.dropout_keep_prob: 0.8})\n",
    "                    loss, acc, counter = loss+l, acc+a, counter+1\n",
    "\n",
    "                    if (counter % 100 == 0):\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\\ Train Accuracy:%.3f\"%(epoch, counter, loss/float(counter), acc/float(counter)))\n",
    "\n",
    "                #4.验证，每迭代完FLAGS.validation_every轮，在验证集上跑一次\n",
    "                print(epoch,FLAGS.validation_every,(epoch % FLAGS.validation_every==0))\n",
    "                if epoch % FLAGS.validation_every == 0:\n",
    "                    print('run model on validation data...')\n",
    "                    loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "                    log(\"Epoch %d\\ Validation Loss:%.3f/ Validation Accuracy:%.3f\"%(epoch, loss_valid, acc_valid))\n",
    "                    #save the checkpoint\n",
    "                    save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                    saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "        loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "        log(\"Validation Loss:%.3f\\ Validation Accuracy:%.3f\"%(loss_valid, acc_valid))\n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y, embedding_final\n",
    "\n",
    "def do_eval(sess, model, test_X, test_y, batch_size):\n",
    "    test_X, test_y = test_X[:256], test_y[:256]\n",
    "    num_of_data = len(test_y)\n",
    "    loss, acc, counter = 0.0, 0.0, 0\n",
    "    for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "        l,a = sess.run([model.loss_val, model.accuracy], \n",
    "                        feed_dict={model.sentence: test_X[start:end], model.label: test_y[start:end], model.dropout_keep_prob:1.0})\n",
    "        loss, acc, counter = loss+l, acc+a, counter+1\n",
    "    return loss/float(counter), acc/float(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X.shape: (2959966, 200)\n",
      "train_y.shape: (2959966,)\n",
      "test_X.shape: (20000, 200)\n",
      "test_y.shape: (20000,)\n",
      "test_X[0]: [ 579  343 1173 1843    5  583  292 1173 1843    5 1180 1299  989   10\n",
      "    2   68  153  168  531  109  260  217  277   81   59   81  116  514\n",
      "    6  221  253  224  154  718  553    4  806  538  732  264   74    6\n",
      "  221  224  154  326   11  167  136    4  257  145   37   74  175  214\n",
      "   11   57  110  221    6  364   89   20 4050 2344    4  257   78    9\n",
      "  991  326  221   89  699  133   11  597  679 1957  824  884  871 1957\n",
      "  824    4  178   87   87   78  196   52  552   69   47   20   12   37\n",
      " 1371   89    6  755  779   81  667  597    4  586  878    6   35   93\n",
      "    7  719  285  937   35  162   13   11    7 1371   89   35    4  201\n",
      "   68   81   97 1533   81  667  597    9  991  326   35  343  704   16\n",
      "    5   99   13    9  991  654  583  292    4   13  221    6  795  230\n",
      "   11   11  350   12  495  235    7  990  625  718  553  297  215  954\n",
      "  549    4   12  165  198   67   93    9  166  110  146    4   81   86\n",
      "   93  141   87 1146  118  224  154   93  147    9   20    4   81  407\n",
      "   92  116  514   12]\n",
      "test_X[1]: [  52   61   27  505  319  131  491 1514 1514    9  110   24  325    8\n",
      "   28  424  601  664  152  128  838 1292 1047  549   10    2    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "test_y[0]: 808\n",
      "initialize variables\n",
      "assign pre-trained embedding\n",
      "train_X[start, end]: [[  90  132  541 ...    0    0    0]\n",
      " [ 217  190   48 ...   50  336  167]\n",
      " [  11   54  162 ...    0    0    0]\n",
      " ...\n",
      " [  92  828 1534 ...  131    4   69]\n",
      " [1062  863  537 ...    0    0    0]\n",
      " [  48  177 1385 ...    8   33   30]]\n",
      "train_y[start, end]: [1345  510  135 1699  991  108  921 1695 1004  703  311  350 1418  573\n",
      "  156 1792  106  607 1606  497 1574 1089 1250  359 1630   30  735 1831\n",
      " 1682 1259 1428 1547  183 1695 1057  363 1168  349 1679   33  731  380\n",
      "   81 1076 1196 1275 1957 1969 1491 1435 1911  140 1673  967 1212 1877\n",
      "  485  280  508 1302  797  334 1423 1863]\n",
      "[2019/02/28 17:35:09] Epoch 0\\Batch 100\\ Train Loss:28.103\\ Train Accuracy:0.002\n",
      "[2019/02/28 17:35:24] Epoch 0\\Batch 200\\ Train Loss:26.531\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:35:40] Epoch 0\\Batch 300\\ Train Loss:25.209\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:35:56] Epoch 0\\Batch 400\\ Train Loss:24.074\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:36:12] Epoch 0\\Batch 500\\ Train Loss:23.070\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:36:28] Epoch 0\\Batch 600\\ Train Loss:22.182\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:36:45] Epoch 0\\Batch 700\\ Train Loss:21.379\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:37:01] Epoch 0\\Batch 800\\ Train Loss:20.654\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:37:18] Epoch 0\\Batch 900\\ Train Loss:19.994\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:37:34] Epoch 0\\Batch 1000\\ Train Loss:19.391\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:37:50] Epoch 0\\Batch 1100\\ Train Loss:18.837\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:38:06] Epoch 0\\Batch 1200\\ Train Loss:18.329\\ Train Accuracy:0.004\n",
      "[2019/02/28 17:38:22] Epoch 0\\Batch 1300\\ Train Loss:17.860\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:38:38] Epoch 0\\Batch 1400\\ Train Loss:17.428\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:38:54] Epoch 0\\Batch 1500\\ Train Loss:17.026\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:39:10] Epoch 0\\Batch 1600\\ Train Loss:16.653\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:39:26] Epoch 0\\Batch 1700\\ Train Loss:16.306\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:39:42] Epoch 0\\Batch 1800\\ Train Loss:15.983\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:39:57] Epoch 0\\Batch 1900\\ Train Loss:15.681\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:40:14] Epoch 0\\Batch 2000\\ Train Loss:15.399\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:40:30] Epoch 0\\Batch 2100\\ Train Loss:15.135\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:40:45] Epoch 0\\Batch 2200\\ Train Loss:14.887\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:41:01] Epoch 0\\Batch 2300\\ Train Loss:14.654\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:41:17] Epoch 0\\Batch 2400\\ Train Loss:14.436\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:41:32] Epoch 0\\Batch 2500\\ Train Loss:14.230\\ Train Accuracy:0.005\n",
      "[2019/02/28 17:41:48] Epoch 0\\Batch 2600\\ Train Loss:14.036\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:42:04] Epoch 0\\Batch 2700\\ Train Loss:13.853\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:42:20] Epoch 0\\Batch 2800\\ Train Loss:13.680\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:42:36] Epoch 0\\Batch 2900\\ Train Loss:13.516\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:42:52] Epoch 0\\Batch 3000\\ Train Loss:13.361\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:43:07] Epoch 0\\Batch 3100\\ Train Loss:13.213\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:43:23] Epoch 0\\Batch 3200\\ Train Loss:13.072\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:43:39] Epoch 0\\Batch 3300\\ Train Loss:12.939\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:43:55] Epoch 0\\Batch 3400\\ Train Loss:12.812\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:44:11] Epoch 0\\Batch 3500\\ Train Loss:12.690\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:44:26] Epoch 0\\Batch 3600\\ Train Loss:12.575\\ Train Accuracy:0.006\n",
      "[2019/02/28 17:44:42] Epoch 0\\Batch 3700\\ Train Loss:12.465\\ Train Accuracy:0.007\n",
      "[2019/02/28 17:44:58] Epoch 0\\Batch 3800\\ Train Loss:12.359\\ Train Accuracy:0.007\n",
      "[2019/02/28 17:45:14] Epoch 0\\Batch 3900\\ Train Loss:12.257\\ Train Accuracy:0.007\n",
      "[2019/02/28 17:45:30] Epoch 0\\Batch 4000\\ Train Loss:12.160\\ Train Accuracy:0.007\n",
      "[2019/02/28 17:45:46] Epoch 0\\Batch 4100\\ Train Loss:12.066\\ Train Accuracy:0.007\n",
      "[2019/02/28 17:46:02] Epoch 0\\Batch 4200\\ Train Loss:11.976\\ Train Accuracy:0.007\n",
      "[2019/02/28 17:46:18] Epoch 0\\Batch 4300\\ Train Loss:11.889\\ Train Accuracy:0.008\n",
      "[2019/02/28 17:46:34] Epoch 0\\Batch 4400\\ Train Loss:11.805\\ Train Accuracy:0.008\n",
      "[2019/02/28 17:46:50] Epoch 0\\Batch 4500\\ Train Loss:11.724\\ Train Accuracy:0.008\n",
      "[2019/02/28 17:47:06] Epoch 0\\Batch 4600\\ Train Loss:11.645\\ Train Accuracy:0.008\n",
      "[2019/02/28 17:47:22] Epoch 0\\Batch 4700\\ Train Loss:11.568\\ Train Accuracy:0.008\n",
      "[2019/02/28 17:47:38] Epoch 0\\Batch 4800\\ Train Loss:11.494\\ Train Accuracy:0.009\n",
      "[2019/02/28 17:47:54] Epoch 0\\Batch 4900\\ Train Loss:11.421\\ Train Accuracy:0.009\n",
      "[2019/02/28 17:48:09] Epoch 0\\Batch 5000\\ Train Loss:11.351\\ Train Accuracy:0.009\n",
      "[2019/02/28 17:48:22] Epoch 0\\Batch 5100\\ Train Loss:11.282\\ Train Accuracy:0.009\n",
      "[2019/02/28 17:48:35] Epoch 0\\Batch 5200\\ Train Loss:11.215\\ Train Accuracy:0.010\n",
      "[2019/02/28 17:48:48] Epoch 0\\Batch 5300\\ Train Loss:11.149\\ Train Accuracy:0.010\n",
      "[2019/02/28 17:49:00] Epoch 0\\Batch 5400\\ Train Loss:11.084\\ Train Accuracy:0.010\n",
      "[2019/02/28 17:49:13] Epoch 0\\Batch 5500\\ Train Loss:11.021\\ Train Accuracy:0.011\n",
      "[2019/02/28 17:49:26] Epoch 0\\Batch 5600\\ Train Loss:10.958\\ Train Accuracy:0.011\n",
      "[2019/02/28 17:49:38] Epoch 0\\Batch 5700\\ Train Loss:10.897\\ Train Accuracy:0.011\n",
      "[2019/02/28 17:49:50] Epoch 0\\Batch 5800\\ Train Loss:10.837\\ Train Accuracy:0.012\n",
      "[2019/02/28 17:50:03] Epoch 0\\Batch 5900\\ Train Loss:10.778\\ Train Accuracy:0.012\n",
      "[2019/02/28 17:50:14] Epoch 0\\Batch 6000\\ Train Loss:10.720\\ Train Accuracy:0.012\n",
      "[2019/02/28 17:50:27] Epoch 0\\Batch 6100\\ Train Loss:10.663\\ Train Accuracy:0.013\n",
      "[2019/02/28 17:50:39] Epoch 0\\Batch 6200\\ Train Loss:10.607\\ Train Accuracy:0.013\n",
      "[2019/02/28 17:50:51] Epoch 0\\Batch 6300\\ Train Loss:10.552\\ Train Accuracy:0.013\n",
      "[2019/02/28 17:51:02] Epoch 0\\Batch 6400\\ Train Loss:10.498\\ Train Accuracy:0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 17:51:15] Epoch 0\\Batch 6500\\ Train Loss:10.445\\ Train Accuracy:0.014\n",
      "[2019/02/28 17:51:27] Epoch 0\\Batch 6600\\ Train Loss:10.391\\ Train Accuracy:0.015\n",
      "[2019/02/28 17:51:39] Epoch 0\\Batch 6700\\ Train Loss:10.339\\ Train Accuracy:0.015\n",
      "[2019/02/28 17:51:50] Epoch 0\\Batch 6800\\ Train Loss:10.288\\ Train Accuracy:0.016\n",
      "[2019/02/28 17:52:03] Epoch 0\\Batch 6900\\ Train Loss:10.238\\ Train Accuracy:0.016\n",
      "[2019/02/28 17:52:15] Epoch 0\\Batch 7000\\ Train Loss:10.188\\ Train Accuracy:0.016\n",
      "[2019/02/28 17:52:27] Epoch 0\\Batch 7100\\ Train Loss:10.140\\ Train Accuracy:0.017\n",
      "[2019/02/28 17:52:39] Epoch 0\\Batch 7200\\ Train Loss:10.092\\ Train Accuracy:0.017\n",
      "[2019/02/28 17:52:50] Epoch 0\\Batch 7300\\ Train Loss:10.045\\ Train Accuracy:0.018\n",
      "[2019/02/28 17:53:02] Epoch 0\\Batch 7400\\ Train Loss:10.000\\ Train Accuracy:0.018\n",
      "[2019/02/28 17:53:14] Epoch 0\\Batch 7500\\ Train Loss:9.954\\ Train Accuracy:0.019\n",
      "[2019/02/28 17:53:26] Epoch 0\\Batch 7600\\ Train Loss:9.909\\ Train Accuracy:0.019\n",
      "[2019/02/28 17:53:37] Epoch 0\\Batch 7700\\ Train Loss:9.865\\ Train Accuracy:0.020\n",
      "[2019/02/28 17:53:49] Epoch 0\\Batch 7800\\ Train Loss:9.821\\ Train Accuracy:0.020\n",
      "[2019/02/28 17:54:01] Epoch 0\\Batch 7900\\ Train Loss:9.779\\ Train Accuracy:0.021\n",
      "[2019/02/28 17:54:13] Epoch 0\\Batch 8000\\ Train Loss:9.737\\ Train Accuracy:0.021\n",
      "[2019/02/28 17:54:25] Epoch 0\\Batch 8100\\ Train Loss:9.695\\ Train Accuracy:0.022\n",
      "[2019/02/28 17:54:36] Epoch 0\\Batch 8200\\ Train Loss:9.655\\ Train Accuracy:0.023\n",
      "[2019/02/28 17:54:47] Epoch 0\\Batch 8300\\ Train Loss:9.615\\ Train Accuracy:0.023\n",
      "[2019/02/28 17:54:59] Epoch 0\\Batch 8400\\ Train Loss:9.576\\ Train Accuracy:0.024\n",
      "[2019/02/28 17:55:11] Epoch 0\\Batch 8500\\ Train Loss:9.537\\ Train Accuracy:0.024\n",
      "[2019/02/28 17:55:22] Epoch 0\\Batch 8600\\ Train Loss:9.499\\ Train Accuracy:0.025\n",
      "[2019/02/28 17:55:33] Epoch 0\\Batch 8700\\ Train Loss:9.462\\ Train Accuracy:0.025\n",
      "[2019/02/28 17:55:44] Epoch 0\\Batch 8800\\ Train Loss:9.425\\ Train Accuracy:0.026\n",
      "[2019/02/28 17:55:55] Epoch 0\\Batch 8900\\ Train Loss:9.389\\ Train Accuracy:0.027\n",
      "[2019/02/28 17:56:07] Epoch 0\\Batch 9000\\ Train Loss:9.354\\ Train Accuracy:0.027\n",
      "[2019/02/28 17:56:18] Epoch 0\\Batch 9100\\ Train Loss:9.319\\ Train Accuracy:0.028\n",
      "[2019/02/28 17:56:29] Epoch 0\\Batch 9200\\ Train Loss:9.284\\ Train Accuracy:0.028\n",
      "[2019/02/28 17:56:41] Epoch 0\\Batch 9300\\ Train Loss:9.250\\ Train Accuracy:0.029\n",
      "[2019/02/28 17:56:53] Epoch 0\\Batch 9400\\ Train Loss:9.217\\ Train Accuracy:0.029\n",
      "[2019/02/28 17:57:04] Epoch 0\\Batch 9500\\ Train Loss:9.184\\ Train Accuracy:0.030\n",
      "[2019/02/28 17:57:15] Epoch 0\\Batch 9600\\ Train Loss:9.152\\ Train Accuracy:0.031\n",
      "[2019/02/28 17:57:26] Epoch 0\\Batch 9700\\ Train Loss:9.120\\ Train Accuracy:0.031\n",
      "[2019/02/28 17:57:38] Epoch 0\\Batch 9800\\ Train Loss:9.089\\ Train Accuracy:0.032\n",
      "[2019/02/28 17:57:50] Epoch 0\\Batch 9900\\ Train Loss:9.058\\ Train Accuracy:0.032\n",
      "[2019/02/28 17:58:01] Epoch 0\\Batch 10000\\ Train Loss:9.027\\ Train Accuracy:0.033\n",
      "[2019/02/28 17:58:13] Epoch 0\\Batch 10100\\ Train Loss:8.998\\ Train Accuracy:0.034\n",
      "[2019/02/28 17:58:24] Epoch 0\\Batch 10200\\ Train Loss:8.968\\ Train Accuracy:0.034\n",
      "[2019/02/28 17:58:35] Epoch 0\\Batch 10300\\ Train Loss:8.939\\ Train Accuracy:0.035\n",
      "[2019/02/28 17:58:47] Epoch 0\\Batch 10400\\ Train Loss:8.910\\ Train Accuracy:0.035\n",
      "[2019/02/28 17:58:58] Epoch 0\\Batch 10500\\ Train Loss:8.882\\ Train Accuracy:0.036\n",
      "[2019/02/28 17:59:09] Epoch 0\\Batch 10600\\ Train Loss:8.854\\ Train Accuracy:0.036\n",
      "[2019/02/28 17:59:21] Epoch 0\\Batch 10700\\ Train Loss:8.827\\ Train Accuracy:0.037\n",
      "[2019/02/28 17:59:33] Epoch 0\\Batch 10800\\ Train Loss:8.800\\ Train Accuracy:0.038\n",
      "[2019/02/28 17:59:44] Epoch 0\\Batch 10900\\ Train Loss:8.774\\ Train Accuracy:0.038\n",
      "[2019/02/28 17:59:55] Epoch 0\\Batch 11000\\ Train Loss:8.747\\ Train Accuracy:0.039\n",
      "[2019/02/28 18:00:06] Epoch 0\\Batch 11100\\ Train Loss:8.721\\ Train Accuracy:0.039\n",
      "[2019/02/28 18:00:18] Epoch 0\\Batch 11200\\ Train Loss:8.696\\ Train Accuracy:0.040\n",
      "[2019/02/28 18:00:29] Epoch 0\\Batch 11300\\ Train Loss:8.671\\ Train Accuracy:0.041\n",
      "[2019/02/28 18:00:41] Epoch 0\\Batch 11400\\ Train Loss:8.646\\ Train Accuracy:0.041\n",
      "[2019/02/28 18:00:52] Epoch 0\\Batch 11500\\ Train Loss:8.622\\ Train Accuracy:0.042\n",
      "[2019/02/28 18:01:03] Epoch 0\\Batch 11600\\ Train Loss:8.598\\ Train Accuracy:0.042\n",
      "[2019/02/28 18:01:15] Epoch 0\\Batch 11700\\ Train Loss:8.574\\ Train Accuracy:0.043\n",
      "[2019/02/28 18:01:26] Epoch 0\\Batch 11800\\ Train Loss:8.551\\ Train Accuracy:0.043\n",
      "[2019/02/28 18:01:38] Epoch 0\\Batch 11900\\ Train Loss:8.528\\ Train Accuracy:0.044\n",
      "[2019/02/28 18:01:50] Epoch 0\\Batch 12000\\ Train Loss:8.505\\ Train Accuracy:0.045\n",
      "[2019/02/28 18:02:01] Epoch 0\\Batch 12100\\ Train Loss:8.483\\ Train Accuracy:0.045\n",
      "[2019/02/28 18:02:13] Epoch 0\\Batch 12200\\ Train Loss:8.460\\ Train Accuracy:0.046\n",
      "[2019/02/28 18:02:25] Epoch 0\\Batch 12300\\ Train Loss:8.439\\ Train Accuracy:0.046\n",
      "[2019/02/28 18:02:36] Epoch 0\\Batch 12400\\ Train Loss:8.417\\ Train Accuracy:0.047\n",
      "[2019/02/28 18:02:48] Epoch 0\\Batch 12500\\ Train Loss:8.396\\ Train Accuracy:0.048\n",
      "[2019/02/28 18:02:58] Epoch 0\\Batch 12600\\ Train Loss:8.375\\ Train Accuracy:0.048\n",
      "[2019/02/28 18:03:09] Epoch 0\\Batch 12700\\ Train Loss:8.354\\ Train Accuracy:0.049\n",
      "[2019/02/28 18:03:20] Epoch 0\\Batch 12800\\ Train Loss:8.333\\ Train Accuracy:0.049\n",
      "[2019/02/28 18:03:32] Epoch 0\\Batch 12900\\ Train Loss:8.313\\ Train Accuracy:0.050\n",
      "[2019/02/28 18:03:43] Epoch 0\\Batch 13000\\ Train Loss:8.293\\ Train Accuracy:0.050\n",
      "[2019/02/28 18:03:54] Epoch 0\\Batch 13100\\ Train Loss:8.273\\ Train Accuracy:0.051\n",
      "[2019/02/28 18:04:06] Epoch 0\\Batch 13200\\ Train Loss:8.254\\ Train Accuracy:0.052\n",
      "[2019/02/28 18:04:17] Epoch 0\\Batch 13300\\ Train Loss:8.235\\ Train Accuracy:0.052\n",
      "[2019/02/28 18:04:27] Epoch 0\\Batch 13400\\ Train Loss:8.216\\ Train Accuracy:0.053\n",
      "[2019/02/28 18:04:39] Epoch 0\\Batch 13500\\ Train Loss:8.197\\ Train Accuracy:0.053\n",
      "[2019/02/28 18:04:50] Epoch 0\\Batch 13600\\ Train Loss:8.178\\ Train Accuracy:0.054\n",
      "[2019/02/28 18:05:02] Epoch 0\\Batch 13700\\ Train Loss:8.160\\ Train Accuracy:0.054\n",
      "[2019/02/28 18:05:13] Epoch 0\\Batch 13800\\ Train Loss:8.142\\ Train Accuracy:0.055\n",
      "[2019/02/28 18:05:24] Epoch 0\\Batch 13900\\ Train Loss:8.125\\ Train Accuracy:0.055\n",
      "[2019/02/28 18:05:35] Epoch 0\\Batch 14000\\ Train Loss:8.107\\ Train Accuracy:0.056\n",
      "[2019/02/28 18:05:46] Epoch 0\\Batch 14100\\ Train Loss:8.090\\ Train Accuracy:0.056\n",
      "[2019/02/28 18:05:58] Epoch 0\\Batch 14200\\ Train Loss:8.073\\ Train Accuracy:0.057\n",
      "[2019/02/28 18:06:09] Epoch 0\\Batch 14300\\ Train Loss:8.056\\ Train Accuracy:0.057\n",
      "[2019/02/28 18:06:20] Epoch 0\\Batch 14400\\ Train Loss:8.040\\ Train Accuracy:0.058\n",
      "[2019/02/28 18:06:32] Epoch 0\\Batch 14500\\ Train Loss:8.023\\ Train Accuracy:0.058\n",
      "[2019/02/28 18:06:44] Epoch 0\\Batch 14600\\ Train Loss:8.007\\ Train Accuracy:0.059\n",
      "[2019/02/28 18:06:55] Epoch 0\\Batch 14700\\ Train Loss:7.991\\ Train Accuracy:0.060\n",
      "[2019/02/28 18:07:07] Epoch 0\\Batch 14800\\ Train Loss:7.975\\ Train Accuracy:0.060\n",
      "[2019/02/28 18:07:18] Epoch 0\\Batch 14900\\ Train Loss:7.959\\ Train Accuracy:0.061\n",
      "[2019/02/28 18:07:30] Epoch 0\\Batch 15000\\ Train Loss:7.944\\ Train Accuracy:0.061\n",
      "[2019/02/28 18:07:42] Epoch 0\\Batch 15100\\ Train Loss:7.928\\ Train Accuracy:0.062\n",
      "[2019/02/28 18:07:53] Epoch 0\\Batch 15200\\ Train Loss:7.913\\ Train Accuracy:0.062\n",
      "[2019/02/28 18:08:05] Epoch 0\\Batch 15300\\ Train Loss:7.898\\ Train Accuracy:0.063\n",
      "[2019/02/28 18:08:15] Epoch 0\\Batch 15400\\ Train Loss:7.883\\ Train Accuracy:0.063\n",
      "[2019/02/28 18:08:27] Epoch 0\\Batch 15500\\ Train Loss:7.868\\ Train Accuracy:0.064\n",
      "[2019/02/28 18:08:38] Epoch 0\\Batch 15600\\ Train Loss:7.854\\ Train Accuracy:0.064\n",
      "[2019/02/28 18:08:48] Epoch 0\\Batch 15700\\ Train Loss:7.839\\ Train Accuracy:0.065\n",
      "[2019/02/28 18:08:58] Epoch 0\\Batch 15800\\ Train Loss:7.825\\ Train Accuracy:0.065\n",
      "[2019/02/28 18:09:09] Epoch 0\\Batch 15900\\ Train Loss:7.811\\ Train Accuracy:0.065\n",
      "[2019/02/28 18:09:20] Epoch 0\\Batch 16000\\ Train Loss:7.797\\ Train Accuracy:0.066\n",
      "[2019/02/28 18:09:32] Epoch 0\\Batch 16100\\ Train Loss:7.783\\ Train Accuracy:0.067\n",
      "[2019/02/28 18:09:43] Epoch 0\\Batch 16200\\ Train Loss:7.769\\ Train Accuracy:0.067\n",
      "[2019/02/28 18:09:54] Epoch 0\\Batch 16300\\ Train Loss:7.756\\ Train Accuracy:0.067\n",
      "[2019/02/28 18:10:05] Epoch 0\\Batch 16400\\ Train Loss:7.743\\ Train Accuracy:0.068\n",
      "[2019/02/28 18:10:15] Epoch 0\\Batch 16500\\ Train Loss:7.729\\ Train Accuracy:0.068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 18:10:27] Epoch 0\\Batch 16600\\ Train Loss:7.716\\ Train Accuracy:0.069\n",
      "[2019/02/28 18:10:38] Epoch 0\\Batch 16700\\ Train Loss:7.703\\ Train Accuracy:0.069\n",
      "[2019/02/28 18:10:48] Epoch 0\\Batch 16800\\ Train Loss:7.690\\ Train Accuracy:0.070\n",
      "[2019/02/28 18:10:58] Epoch 0\\Batch 16900\\ Train Loss:7.678\\ Train Accuracy:0.070\n",
      "[2019/02/28 18:11:09] Epoch 0\\Batch 17000\\ Train Loss:7.665\\ Train Accuracy:0.071\n",
      "[2019/02/28 18:11:20] Epoch 0\\Batch 17100\\ Train Loss:7.653\\ Train Accuracy:0.071\n",
      "[2019/02/28 18:11:31] Epoch 0\\Batch 17200\\ Train Loss:7.641\\ Train Accuracy:0.072\n",
      "[2019/02/28 18:11:41] Epoch 0\\Batch 17300\\ Train Loss:7.629\\ Train Accuracy:0.072\n",
      "[2019/02/28 18:11:52] Epoch 0\\Batch 17400\\ Train Loss:7.617\\ Train Accuracy:0.073\n",
      "[2019/02/28 18:12:03] Epoch 0\\Batch 17500\\ Train Loss:7.605\\ Train Accuracy:0.073\n",
      "[2019/02/28 18:12:14] Epoch 0\\Batch 17600\\ Train Loss:7.593\\ Train Accuracy:0.074\n",
      "[2019/02/28 18:12:24] Epoch 0\\Batch 17700\\ Train Loss:7.581\\ Train Accuracy:0.074\n",
      "[2019/02/28 18:12:36] Epoch 0\\Batch 17800\\ Train Loss:7.570\\ Train Accuracy:0.074\n",
      "[2019/02/28 18:12:48] Epoch 0\\Batch 17900\\ Train Loss:7.558\\ Train Accuracy:0.075\n",
      "[2019/02/28 18:12:59] Epoch 0\\Batch 18000\\ Train Loss:7.547\\ Train Accuracy:0.075\n",
      "[2019/02/28 18:13:10] Epoch 0\\Batch 18100\\ Train Loss:7.536\\ Train Accuracy:0.076\n",
      "[2019/02/28 18:13:21] Epoch 0\\Batch 18200\\ Train Loss:7.524\\ Train Accuracy:0.076\n",
      "[2019/02/28 18:13:32] Epoch 0\\Batch 18300\\ Train Loss:7.513\\ Train Accuracy:0.077\n",
      "[2019/02/28 18:13:42] Epoch 0\\Batch 18400\\ Train Loss:7.503\\ Train Accuracy:0.077\n",
      "[2019/02/28 18:13:53] Epoch 0\\Batch 18500\\ Train Loss:7.492\\ Train Accuracy:0.077\n",
      "[2019/02/28 18:14:03] Epoch 0\\Batch 18600\\ Train Loss:7.481\\ Train Accuracy:0.078\n",
      "[2019/02/28 18:14:14] Epoch 0\\Batch 18700\\ Train Loss:7.471\\ Train Accuracy:0.078\n",
      "[2019/02/28 18:14:25] Epoch 0\\Batch 18800\\ Train Loss:7.460\\ Train Accuracy:0.079\n",
      "[2019/02/28 18:14:36] Epoch 0\\Batch 18900\\ Train Loss:7.450\\ Train Accuracy:0.079\n",
      "[2019/02/28 18:14:46] Epoch 0\\Batch 19000\\ Train Loss:7.440\\ Train Accuracy:0.079\n",
      "[2019/02/28 18:14:57] Epoch 0\\Batch 19100\\ Train Loss:7.430\\ Train Accuracy:0.080\n",
      "[2019/02/28 18:15:09] Epoch 0\\Batch 19200\\ Train Loss:7.420\\ Train Accuracy:0.080\n",
      "[2019/02/28 18:15:20] Epoch 0\\Batch 19300\\ Train Loss:7.410\\ Train Accuracy:0.081\n",
      "[2019/02/28 18:15:31] Epoch 0\\Batch 19400\\ Train Loss:7.400\\ Train Accuracy:0.081\n",
      "[2019/02/28 18:15:42] Epoch 0\\Batch 19500\\ Train Loss:7.390\\ Train Accuracy:0.081\n",
      "[2019/02/28 18:15:53] Epoch 0\\Batch 19600\\ Train Loss:7.380\\ Train Accuracy:0.082\n",
      "[2019/02/28 18:16:04] Epoch 0\\Batch 19700\\ Train Loss:7.371\\ Train Accuracy:0.082\n",
      "[2019/02/28 18:16:15] Epoch 0\\Batch 19800\\ Train Loss:7.361\\ Train Accuracy:0.083\n",
      "[2019/02/28 18:16:25] Epoch 0\\Batch 19900\\ Train Loss:7.352\\ Train Accuracy:0.083\n",
      "[2019/02/28 18:16:37] Epoch 0\\Batch 20000\\ Train Loss:7.343\\ Train Accuracy:0.083\n",
      "[2019/02/28 18:16:48] Epoch 0\\Batch 20100\\ Train Loss:7.333\\ Train Accuracy:0.084\n",
      "[2019/02/28 18:16:59] Epoch 0\\Batch 20200\\ Train Loss:7.324\\ Train Accuracy:0.084\n",
      "[2019/02/28 18:17:10] Epoch 0\\Batch 20300\\ Train Loss:7.315\\ Train Accuracy:0.084\n",
      "[2019/02/28 18:17:21] Epoch 0\\Batch 20400\\ Train Loss:7.306\\ Train Accuracy:0.085\n",
      "[2019/02/28 18:17:32] Epoch 0\\Batch 20500\\ Train Loss:7.297\\ Train Accuracy:0.085\n",
      "[2019/02/28 18:17:43] Epoch 0\\Batch 20600\\ Train Loss:7.288\\ Train Accuracy:0.086\n",
      "[2019/02/28 18:17:53] Epoch 0\\Batch 20700\\ Train Loss:7.279\\ Train Accuracy:0.086\n",
      "[2019/02/28 18:18:04] Epoch 0\\Batch 20800\\ Train Loss:7.271\\ Train Accuracy:0.086\n",
      "[2019/02/28 18:18:15] Epoch 0\\Batch 20900\\ Train Loss:7.262\\ Train Accuracy:0.087\n",
      "[2019/02/28 18:18:25] Epoch 0\\Batch 21000\\ Train Loss:7.253\\ Train Accuracy:0.087\n",
      "[2019/02/28 18:18:36] Epoch 0\\Batch 21100\\ Train Loss:7.245\\ Train Accuracy:0.087\n",
      "[2019/02/28 18:18:47] Epoch 0\\Batch 21200\\ Train Loss:7.236\\ Train Accuracy:0.088\n",
      "[2019/02/28 18:18:58] Epoch 0\\Batch 21300\\ Train Loss:7.228\\ Train Accuracy:0.088\n",
      "[2019/02/28 18:19:09] Epoch 0\\Batch 21400\\ Train Loss:7.220\\ Train Accuracy:0.088\n",
      "[2019/02/28 18:19:20] Epoch 0\\Batch 21500\\ Train Loss:7.211\\ Train Accuracy:0.089\n",
      "[2019/02/28 18:19:32] Epoch 0\\Batch 21600\\ Train Loss:7.203\\ Train Accuracy:0.089\n",
      "[2019/02/28 18:19:43] Epoch 0\\Batch 21700\\ Train Loss:7.195\\ Train Accuracy:0.090\n",
      "[2019/02/28 18:19:55] Epoch 0\\Batch 21800\\ Train Loss:7.188\\ Train Accuracy:0.090\n",
      "[2019/02/28 18:20:05] Epoch 0\\Batch 21900\\ Train Loss:7.180\\ Train Accuracy:0.090\n",
      "[2019/02/28 18:20:15] Epoch 0\\Batch 22000\\ Train Loss:7.172\\ Train Accuracy:0.091\n",
      "[2019/02/28 18:20:25] Epoch 0\\Batch 22100\\ Train Loss:7.164\\ Train Accuracy:0.091\n",
      "[2019/02/28 18:20:36] Epoch 0\\Batch 22200\\ Train Loss:7.156\\ Train Accuracy:0.091\n",
      "[2019/02/28 18:20:47] Epoch 0\\Batch 22300\\ Train Loss:7.149\\ Train Accuracy:0.092\n",
      "[2019/02/28 18:20:58] Epoch 0\\Batch 22400\\ Train Loss:7.141\\ Train Accuracy:0.092\n",
      "[2019/02/28 18:21:10] Epoch 0\\Batch 22500\\ Train Loss:7.134\\ Train Accuracy:0.092\n",
      "[2019/02/28 18:21:21] Epoch 0\\Batch 22600\\ Train Loss:7.126\\ Train Accuracy:0.093\n",
      "[2019/02/28 18:21:32] Epoch 0\\Batch 22700\\ Train Loss:7.119\\ Train Accuracy:0.093\n",
      "[2019/02/28 18:21:43] Epoch 0\\Batch 22800\\ Train Loss:7.112\\ Train Accuracy:0.093\n",
      "[2019/02/28 18:21:55] Epoch 0\\Batch 22900\\ Train Loss:7.104\\ Train Accuracy:0.094\n",
      "[2019/02/28 18:22:06] Epoch 0\\Batch 23000\\ Train Loss:7.097\\ Train Accuracy:0.094\n",
      "[2019/02/28 18:22:18] Epoch 0\\Batch 23100\\ Train Loss:7.090\\ Train Accuracy:0.094\n",
      "[2019/02/28 18:22:28] Epoch 0\\Batch 23200\\ Train Loss:7.083\\ Train Accuracy:0.095\n",
      "[2019/02/28 18:22:40] Epoch 0\\Batch 23300\\ Train Loss:7.076\\ Train Accuracy:0.095\n",
      "[2019/02/28 18:22:51] Epoch 0\\Batch 23400\\ Train Loss:7.069\\ Train Accuracy:0.095\n",
      "[2019/02/28 18:23:02] Epoch 0\\Batch 23500\\ Train Loss:7.062\\ Train Accuracy:0.096\n",
      "[2019/02/28 18:23:13] Epoch 0\\Batch 23600\\ Train Loss:7.055\\ Train Accuracy:0.096\n",
      "[2019/02/28 18:23:24] Epoch 0\\Batch 23700\\ Train Loss:7.048\\ Train Accuracy:0.096\n",
      "[2019/02/28 18:23:36] Epoch 0\\Batch 23800\\ Train Loss:7.041\\ Train Accuracy:0.097\n",
      "[2019/02/28 18:23:47] Epoch 0\\Batch 23900\\ Train Loss:7.035\\ Train Accuracy:0.097\n",
      "[2019/02/28 18:23:57] Epoch 0\\Batch 24000\\ Train Loss:7.028\\ Train Accuracy:0.097\n",
      "[2019/02/28 18:24:08] Epoch 0\\Batch 24100\\ Train Loss:7.021\\ Train Accuracy:0.097\n",
      "[2019/02/28 18:24:19] Epoch 0\\Batch 24200\\ Train Loss:7.015\\ Train Accuracy:0.098\n",
      "[2019/02/28 18:24:30] Epoch 0\\Batch 24300\\ Train Loss:7.008\\ Train Accuracy:0.098\n",
      "[2019/02/28 18:24:40] Epoch 0\\Batch 24400\\ Train Loss:7.002\\ Train Accuracy:0.098\n",
      "[2019/02/28 18:24:51] Epoch 0\\Batch 24500\\ Train Loss:6.995\\ Train Accuracy:0.099\n",
      "[2019/02/28 18:25:02] Epoch 0\\Batch 24600\\ Train Loss:6.989\\ Train Accuracy:0.099\n",
      "[2019/02/28 18:25:13] Epoch 0\\Batch 24700\\ Train Loss:6.983\\ Train Accuracy:0.099\n",
      "[2019/02/28 18:25:23] Epoch 0\\Batch 24800\\ Train Loss:6.976\\ Train Accuracy:0.100\n",
      "[2019/02/28 18:25:34] Epoch 0\\Batch 24900\\ Train Loss:6.970\\ Train Accuracy:0.100\n",
      "[2019/02/28 18:25:44] Epoch 0\\Batch 25000\\ Train Loss:6.964\\ Train Accuracy:0.100\n",
      "[2019/02/28 18:25:56] Epoch 0\\Batch 25100\\ Train Loss:6.958\\ Train Accuracy:0.101\n",
      "[2019/02/28 18:26:06] Epoch 0\\Batch 25200\\ Train Loss:6.952\\ Train Accuracy:0.101\n",
      "[2019/02/28 18:26:18] Epoch 0\\Batch 25300\\ Train Loss:6.946\\ Train Accuracy:0.101\n",
      "[2019/02/28 18:26:29] Epoch 0\\Batch 25400\\ Train Loss:6.940\\ Train Accuracy:0.101\n",
      "[2019/02/28 18:26:39] Epoch 0\\Batch 25500\\ Train Loss:6.934\\ Train Accuracy:0.102\n",
      "[2019/02/28 18:26:51] Epoch 0\\Batch 25600\\ Train Loss:6.928\\ Train Accuracy:0.102\n",
      "[2019/02/28 18:27:01] Epoch 0\\Batch 25700\\ Train Loss:6.922\\ Train Accuracy:0.102\n",
      "[2019/02/28 18:27:12] Epoch 0\\Batch 25800\\ Train Loss:6.916\\ Train Accuracy:0.102\n",
      "[2019/02/28 18:27:23] Epoch 0\\Batch 25900\\ Train Loss:6.910\\ Train Accuracy:0.103\n",
      "[2019/02/28 18:27:35] Epoch 0\\Batch 26000\\ Train Loss:6.905\\ Train Accuracy:0.103\n",
      "[2019/02/28 18:27:46] Epoch 0\\Batch 26100\\ Train Loss:6.899\\ Train Accuracy:0.103\n",
      "[2019/02/28 18:27:57] Epoch 0\\Batch 26200\\ Train Loss:6.893\\ Train Accuracy:0.104\n",
      "[2019/02/28 18:28:08] Epoch 0\\Batch 26300\\ Train Loss:6.888\\ Train Accuracy:0.104\n",
      "[2019/02/28 18:28:20] Epoch 0\\Batch 26400\\ Train Loss:6.882\\ Train Accuracy:0.104\n",
      "[2019/02/28 18:28:31] Epoch 0\\Batch 26500\\ Train Loss:6.877\\ Train Accuracy:0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 18:28:42] Epoch 0\\Batch 26600\\ Train Loss:6.871\\ Train Accuracy:0.105\n",
      "[2019/02/28 18:28:54] Epoch 0\\Batch 26700\\ Train Loss:6.865\\ Train Accuracy:0.105\n",
      "[2019/02/28 18:29:04] Epoch 0\\Batch 26800\\ Train Loss:6.860\\ Train Accuracy:0.105\n",
      "[2019/02/28 18:29:15] Epoch 0\\Batch 26900\\ Train Loss:6.855\\ Train Accuracy:0.105\n",
      "[2019/02/28 18:29:25] Epoch 0\\Batch 27000\\ Train Loss:6.849\\ Train Accuracy:0.106\n",
      "[2019/02/28 18:29:36] Epoch 0\\Batch 27100\\ Train Loss:6.844\\ Train Accuracy:0.106\n",
      "[2019/02/28 18:29:46] Epoch 0\\Batch 27200\\ Train Loss:6.839\\ Train Accuracy:0.106\n",
      "[2019/02/28 18:29:57] Epoch 0\\Batch 27300\\ Train Loss:6.833\\ Train Accuracy:0.107\n",
      "[2019/02/28 18:30:08] Epoch 0\\Batch 27400\\ Train Loss:6.828\\ Train Accuracy:0.107\n",
      "[2019/02/28 18:30:20] Epoch 0\\Batch 27500\\ Train Loss:6.823\\ Train Accuracy:0.107\n",
      "[2019/02/28 18:30:31] Epoch 0\\Batch 27600\\ Train Loss:6.818\\ Train Accuracy:0.107\n",
      "[2019/02/28 18:30:43] Epoch 0\\Batch 27700\\ Train Loss:6.812\\ Train Accuracy:0.108\n",
      "[2019/02/28 18:30:54] Epoch 0\\Batch 27800\\ Train Loss:6.807\\ Train Accuracy:0.108\n",
      "[2019/02/28 18:31:05] Epoch 0\\Batch 27900\\ Train Loss:6.802\\ Train Accuracy:0.108\n",
      "[2019/02/28 18:31:17] Epoch 0\\Batch 28000\\ Train Loss:6.797\\ Train Accuracy:0.108\n",
      "[2019/02/28 18:31:28] Epoch 0\\Batch 28100\\ Train Loss:6.792\\ Train Accuracy:0.109\n",
      "[2019/02/28 18:31:39] Epoch 0\\Batch 28200\\ Train Loss:6.787\\ Train Accuracy:0.109\n",
      "[2019/02/28 18:31:50] Epoch 0\\Batch 28300\\ Train Loss:6.782\\ Train Accuracy:0.109\n",
      "[2019/02/28 18:32:01] Epoch 0\\Batch 28400\\ Train Loss:6.777\\ Train Accuracy:0.109\n",
      "[2019/02/28 18:32:12] Epoch 0\\Batch 28500\\ Train Loss:6.772\\ Train Accuracy:0.110\n",
      "[2019/02/28 18:32:23] Epoch 0\\Batch 28600\\ Train Loss:6.768\\ Train Accuracy:0.110\n",
      "[2019/02/28 18:32:34] Epoch 0\\Batch 28700\\ Train Loss:6.763\\ Train Accuracy:0.110\n",
      "[2019/02/28 18:32:45] Epoch 0\\Batch 28800\\ Train Loss:6.758\\ Train Accuracy:0.110\n",
      "[2019/02/28 18:32:55] Epoch 0\\Batch 28900\\ Train Loss:6.753\\ Train Accuracy:0.111\n",
      "[2019/02/28 18:33:05] Epoch 0\\Batch 29000\\ Train Loss:6.749\\ Train Accuracy:0.111\n",
      "[2019/02/28 18:33:16] Epoch 0\\Batch 29100\\ Train Loss:6.744\\ Train Accuracy:0.111\n",
      "[2019/02/28 18:33:26] Epoch 0\\Batch 29200\\ Train Loss:6.739\\ Train Accuracy:0.111\n",
      "[2019/02/28 18:33:36] Epoch 0\\Batch 29300\\ Train Loss:6.735\\ Train Accuracy:0.112\n",
      "[2019/02/28 18:33:48] Epoch 0\\Batch 29400\\ Train Loss:6.730\\ Train Accuracy:0.112\n",
      "[2019/02/28 18:33:58] Epoch 0\\Batch 29500\\ Train Loss:6.725\\ Train Accuracy:0.112\n",
      "[2019/02/28 18:34:09] Epoch 0\\Batch 29600\\ Train Loss:6.721\\ Train Accuracy:0.112\n",
      "[2019/02/28 18:34:20] Epoch 0\\Batch 29700\\ Train Loss:6.716\\ Train Accuracy:0.113\n",
      "[2019/02/28 18:34:31] Epoch 0\\Batch 29800\\ Train Loss:6.712\\ Train Accuracy:0.113\n",
      "[2019/02/28 18:34:41] Epoch 0\\Batch 29900\\ Train Loss:6.707\\ Train Accuracy:0.113\n",
      "[2019/02/28 18:34:52] Epoch 0\\Batch 30000\\ Train Loss:6.703\\ Train Accuracy:0.113\n",
      "[2019/02/28 18:35:02] Epoch 0\\Batch 30100\\ Train Loss:6.699\\ Train Accuracy:0.113\n",
      "[2019/02/28 18:35:13] Epoch 0\\Batch 30200\\ Train Loss:6.694\\ Train Accuracy:0.114\n",
      "[2019/02/28 18:35:24] Epoch 0\\Batch 30300\\ Train Loss:6.690\\ Train Accuracy:0.114\n",
      "[2019/02/28 18:35:35] Epoch 0\\Batch 30400\\ Train Loss:6.686\\ Train Accuracy:0.114\n",
      "[2019/02/28 18:35:46] Epoch 0\\Batch 30500\\ Train Loss:6.681\\ Train Accuracy:0.114\n",
      "[2019/02/28 18:35:57] Epoch 0\\Batch 30600\\ Train Loss:6.677\\ Train Accuracy:0.115\n",
      "[2019/02/28 18:36:08] Epoch 0\\Batch 30700\\ Train Loss:6.673\\ Train Accuracy:0.115\n",
      "[2019/02/28 18:36:19] Epoch 0\\Batch 30800\\ Train Loss:6.669\\ Train Accuracy:0.115\n",
      "[2019/02/28 18:36:30] Epoch 0\\Batch 30900\\ Train Loss:6.664\\ Train Accuracy:0.115\n",
      "[2019/02/28 18:36:41] Epoch 0\\Batch 31000\\ Train Loss:6.660\\ Train Accuracy:0.115\n",
      "[2019/02/28 18:36:52] Epoch 0\\Batch 31100\\ Train Loss:6.656\\ Train Accuracy:0.116\n",
      "[2019/02/28 18:37:02] Epoch 0\\Batch 31200\\ Train Loss:6.652\\ Train Accuracy:0.116\n",
      "[2019/02/28 18:37:14] Epoch 0\\Batch 31300\\ Train Loss:6.648\\ Train Accuracy:0.116\n",
      "[2019/02/28 18:37:24] Epoch 0\\Batch 31400\\ Train Loss:6.644\\ Train Accuracy:0.116\n",
      "[2019/02/28 18:37:35] Epoch 0\\Batch 31500\\ Train Loss:6.640\\ Train Accuracy:0.117\n",
      "[2019/02/28 18:37:46] Epoch 0\\Batch 31600\\ Train Loss:6.636\\ Train Accuracy:0.117\n",
      "[2019/02/28 18:37:57] Epoch 0\\Batch 31700\\ Train Loss:6.632\\ Train Accuracy:0.117\n",
      "[2019/02/28 18:38:07] Epoch 0\\Batch 31800\\ Train Loss:6.628\\ Train Accuracy:0.117\n",
      "[2019/02/28 18:38:17] Epoch 0\\Batch 31900\\ Train Loss:6.624\\ Train Accuracy:0.117\n",
      "[2019/02/28 18:38:28] Epoch 0\\Batch 32000\\ Train Loss:6.620\\ Train Accuracy:0.118\n",
      "[2019/02/28 18:38:39] Epoch 0\\Batch 32100\\ Train Loss:6.616\\ Train Accuracy:0.118\n",
      "[2019/02/28 18:38:50] Epoch 0\\Batch 32200\\ Train Loss:6.612\\ Train Accuracy:0.118\n",
      "[2019/02/28 18:39:02] Epoch 0\\Batch 32300\\ Train Loss:6.608\\ Train Accuracy:0.118\n",
      "[2019/02/28 18:39:13] Epoch 0\\Batch 32400\\ Train Loss:6.605\\ Train Accuracy:0.118\n",
      "[2019/02/28 18:39:24] Epoch 0\\Batch 32500\\ Train Loss:6.601\\ Train Accuracy:0.119\n",
      "[2019/02/28 18:39:34] Epoch 0\\Batch 32600\\ Train Loss:6.597\\ Train Accuracy:0.119\n",
      "[2019/02/28 18:39:46] Epoch 0\\Batch 32700\\ Train Loss:6.593\\ Train Accuracy:0.119\n",
      "[2019/02/28 18:39:57] Epoch 0\\Batch 32800\\ Train Loss:6.589\\ Train Accuracy:0.119\n",
      "[2019/02/28 18:40:08] Epoch 0\\Batch 32900\\ Train Loss:6.586\\ Train Accuracy:0.119\n",
      "[2019/02/28 18:40:20] Epoch 0\\Batch 33000\\ Train Loss:6.582\\ Train Accuracy:0.120\n",
      "[2019/02/28 18:40:31] Epoch 0\\Batch 33100\\ Train Loss:6.579\\ Train Accuracy:0.120\n",
      "[2019/02/28 18:40:43] Epoch 0\\Batch 33200\\ Train Loss:6.575\\ Train Accuracy:0.120\n",
      "[2019/02/28 18:40:53] Epoch 0\\Batch 33300\\ Train Loss:6.572\\ Train Accuracy:0.120\n",
      "[2019/02/28 18:41:04] Epoch 0\\Batch 33400\\ Train Loss:6.568\\ Train Accuracy:0.120\n",
      "[2019/02/28 18:41:16] Epoch 0\\Batch 33500\\ Train Loss:6.564\\ Train Accuracy:0.121\n",
      "[2019/02/28 18:41:27] Epoch 0\\Batch 33600\\ Train Loss:6.561\\ Train Accuracy:0.121\n",
      "[2019/02/28 18:41:39] Epoch 0\\Batch 33700\\ Train Loss:6.557\\ Train Accuracy:0.121\n",
      "[2019/02/28 18:41:50] Epoch 0\\Batch 33800\\ Train Loss:6.554\\ Train Accuracy:0.121\n",
      "[2019/02/28 18:42:00] Epoch 0\\Batch 33900\\ Train Loss:6.550\\ Train Accuracy:0.121\n",
      "[2019/02/28 18:42:11] Epoch 0\\Batch 34000\\ Train Loss:6.547\\ Train Accuracy:0.122\n",
      "[2019/02/28 18:42:22] Epoch 0\\Batch 34100\\ Train Loss:6.543\\ Train Accuracy:0.122\n",
      "[2019/02/28 18:42:33] Epoch 0\\Batch 34200\\ Train Loss:6.540\\ Train Accuracy:0.122\n",
      "[2019/02/28 18:42:44] Epoch 0\\Batch 34300\\ Train Loss:6.536\\ Train Accuracy:0.122\n",
      "[2019/02/28 18:42:54] Epoch 0\\Batch 34400\\ Train Loss:6.533\\ Train Accuracy:0.122\n",
      "[2019/02/28 18:43:05] Epoch 0\\Batch 34500\\ Train Loss:6.529\\ Train Accuracy:0.122\n",
      "[2019/02/28 18:43:16] Epoch 0\\Batch 34600\\ Train Loss:6.526\\ Train Accuracy:0.123\n",
      "[2019/02/28 18:43:27] Epoch 0\\Batch 34700\\ Train Loss:6.523\\ Train Accuracy:0.123\n",
      "[2019/02/28 18:43:39] Epoch 0\\Batch 34800\\ Train Loss:6.519\\ Train Accuracy:0.123\n",
      "[2019/02/28 18:43:49] Epoch 0\\Batch 34900\\ Train Loss:6.516\\ Train Accuracy:0.123\n",
      "[2019/02/28 18:44:00] Epoch 0\\Batch 35000\\ Train Loss:6.512\\ Train Accuracy:0.123\n",
      "[2019/02/28 18:44:11] Epoch 0\\Batch 35100\\ Train Loss:6.509\\ Train Accuracy:0.124\n",
      "[2019/02/28 18:44:22] Epoch 0\\Batch 35200\\ Train Loss:6.506\\ Train Accuracy:0.124\n",
      "[2019/02/28 18:44:33] Epoch 0\\Batch 35300\\ Train Loss:6.503\\ Train Accuracy:0.124\n",
      "[2019/02/28 18:44:44] Epoch 0\\Batch 35400\\ Train Loss:6.499\\ Train Accuracy:0.124\n",
      "[2019/02/28 18:44:55] Epoch 0\\Batch 35500\\ Train Loss:6.496\\ Train Accuracy:0.124\n",
      "[2019/02/28 18:45:06] Epoch 0\\Batch 35600\\ Train Loss:6.493\\ Train Accuracy:0.124\n",
      "[2019/02/28 18:45:17] Epoch 0\\Batch 35700\\ Train Loss:6.490\\ Train Accuracy:0.125\n",
      "[2019/02/28 18:45:29] Epoch 0\\Batch 35800\\ Train Loss:6.486\\ Train Accuracy:0.125\n",
      "[2019/02/28 18:45:40] Epoch 0\\Batch 35900\\ Train Loss:6.483\\ Train Accuracy:0.125\n",
      "[2019/02/28 18:45:51] Epoch 0\\Batch 36000\\ Train Loss:6.480\\ Train Accuracy:0.125\n",
      "[2019/02/28 18:46:02] Epoch 0\\Batch 36100\\ Train Loss:6.477\\ Train Accuracy:0.125\n",
      "[2019/02/28 18:46:14] Epoch 0\\Batch 36200\\ Train Loss:6.473\\ Train Accuracy:0.126\n",
      "[2019/02/28 18:46:25] Epoch 0\\Batch 36300\\ Train Loss:6.470\\ Train Accuracy:0.126\n",
      "[2019/02/28 18:46:36] Epoch 0\\Batch 36400\\ Train Loss:6.467\\ Train Accuracy:0.126\n",
      "[2019/02/28 18:46:47] Epoch 0\\Batch 36500\\ Train Loss:6.464\\ Train Accuracy:0.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 18:46:58] Epoch 0\\Batch 36600\\ Train Loss:6.461\\ Train Accuracy:0.126\n",
      "[2019/02/28 18:47:08] Epoch 0\\Batch 36700\\ Train Loss:6.458\\ Train Accuracy:0.126\n",
      "[2019/02/28 18:47:19] Epoch 0\\Batch 36800\\ Train Loss:6.455\\ Train Accuracy:0.127\n",
      "[2019/02/28 18:47:31] Epoch 0\\Batch 36900\\ Train Loss:6.452\\ Train Accuracy:0.127\n",
      "[2019/02/28 18:47:43] Epoch 0\\Batch 37000\\ Train Loss:6.449\\ Train Accuracy:0.127\n",
      "[2019/02/28 18:47:53] Epoch 0\\Batch 37100\\ Train Loss:6.446\\ Train Accuracy:0.127\n",
      "[2019/02/28 18:48:04] Epoch 0\\Batch 37200\\ Train Loss:6.443\\ Train Accuracy:0.127\n",
      "[2019/02/28 18:48:15] Epoch 0\\Batch 37300\\ Train Loss:6.440\\ Train Accuracy:0.128\n",
      "[2019/02/28 18:48:27] Epoch 0\\Batch 37400\\ Train Loss:6.437\\ Train Accuracy:0.128\n",
      "[2019/02/28 18:48:38] Epoch 0\\Batch 37500\\ Train Loss:6.434\\ Train Accuracy:0.128\n",
      "[2019/02/28 18:48:50] Epoch 0\\Batch 37600\\ Train Loss:6.431\\ Train Accuracy:0.128\n",
      "[2019/02/28 18:49:01] Epoch 0\\Batch 37700\\ Train Loss:6.428\\ Train Accuracy:0.128\n",
      "[2019/02/28 18:49:12] Epoch 0\\Batch 37800\\ Train Loss:6.426\\ Train Accuracy:0.128\n",
      "[2019/02/28 18:49:22] Epoch 0\\Batch 37900\\ Train Loss:6.423\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:49:33] Epoch 0\\Batch 38000\\ Train Loss:6.420\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:49:44] Epoch 0\\Batch 38100\\ Train Loss:6.417\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:49:55] Epoch 0\\Batch 38200\\ Train Loss:6.414\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:50:05] Epoch 0\\Batch 38300\\ Train Loss:6.411\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:50:16] Epoch 0\\Batch 38400\\ Train Loss:6.409\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:50:26] Epoch 0\\Batch 38500\\ Train Loss:6.406\\ Train Accuracy:0.129\n",
      "[2019/02/28 18:50:37] Epoch 0\\Batch 38600\\ Train Loss:6.403\\ Train Accuracy:0.130\n",
      "[2019/02/28 18:50:47] Epoch 0\\Batch 38700\\ Train Loss:6.400\\ Train Accuracy:0.130\n",
      "[2019/02/28 18:50:57] Epoch 0\\Batch 38800\\ Train Loss:6.398\\ Train Accuracy:0.130\n",
      "[2019/02/28 18:51:08] Epoch 0\\Batch 38900\\ Train Loss:6.395\\ Train Accuracy:0.130\n",
      "[2019/02/28 18:51:20] Epoch 0\\Batch 39000\\ Train Loss:6.392\\ Train Accuracy:0.130\n",
      "[2019/02/28 18:51:31] Epoch 0\\Batch 39100\\ Train Loss:6.390\\ Train Accuracy:0.130\n",
      "[2019/02/28 18:51:41] Epoch 0\\Batch 39200\\ Train Loss:6.387\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:51:53] Epoch 0\\Batch 39300\\ Train Loss:6.384\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:52:04] Epoch 0\\Batch 39400\\ Train Loss:6.382\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:52:15] Epoch 0\\Batch 39500\\ Train Loss:6.379\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:52:25] Epoch 0\\Batch 39600\\ Train Loss:6.377\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:52:36] Epoch 0\\Batch 39700\\ Train Loss:6.374\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:52:47] Epoch 0\\Batch 39800\\ Train Loss:6.371\\ Train Accuracy:0.131\n",
      "[2019/02/28 18:52:58] Epoch 0\\Batch 39900\\ Train Loss:6.369\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:53:09] Epoch 0\\Batch 40000\\ Train Loss:6.366\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:53:29] Epoch 0\\Batch 40100\\ Train Loss:6.364\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:53:49] Epoch 0\\Batch 40200\\ Train Loss:6.361\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:54:09] Epoch 0\\Batch 40300\\ Train Loss:6.359\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:54:28] Epoch 0\\Batch 40400\\ Train Loss:6.356\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:54:46] Epoch 0\\Batch 40500\\ Train Loss:6.354\\ Train Accuracy:0.132\n",
      "[2019/02/28 18:55:05] Epoch 0\\Batch 40600\\ Train Loss:6.351\\ Train Accuracy:0.133\n",
      "[2019/02/28 18:55:23] Epoch 0\\Batch 40700\\ Train Loss:6.349\\ Train Accuracy:0.133\n",
      "[2019/02/28 18:55:42] Epoch 0\\Batch 40800\\ Train Loss:6.346\\ Train Accuracy:0.133\n",
      "[2019/02/28 18:56:01] Epoch 0\\Batch 40900\\ Train Loss:6.344\\ Train Accuracy:0.133\n",
      "[2019/02/28 18:56:21] Epoch 0\\Batch 41000\\ Train Loss:6.341\\ Train Accuracy:0.133\n",
      "[2019/02/28 18:56:40] Epoch 0\\Batch 41100\\ Train Loss:6.339\\ Train Accuracy:0.133\n",
      "[2019/02/28 18:56:59] Epoch 0\\Batch 41200\\ Train Loss:6.336\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:57:19] Epoch 0\\Batch 41300\\ Train Loss:6.334\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:57:35] Epoch 0\\Batch 41400\\ Train Loss:6.331\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:57:48] Epoch 0\\Batch 41500\\ Train Loss:6.329\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:58:02] Epoch 0\\Batch 41600\\ Train Loss:6.327\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:58:16] Epoch 0\\Batch 41700\\ Train Loss:6.324\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:58:29] Epoch 0\\Batch 41800\\ Train Loss:6.322\\ Train Accuracy:0.134\n",
      "[2019/02/28 18:58:41] Epoch 0\\Batch 41900\\ Train Loss:6.319\\ Train Accuracy:0.135\n",
      "[2019/02/28 18:58:54] Epoch 0\\Batch 42000\\ Train Loss:6.317\\ Train Accuracy:0.135\n",
      "[2019/02/28 18:59:07] Epoch 0\\Batch 42100\\ Train Loss:6.315\\ Train Accuracy:0.135\n",
      "[2019/02/28 18:59:20] Epoch 0\\Batch 42200\\ Train Loss:6.313\\ Train Accuracy:0.135\n",
      "[2019/02/28 18:59:32] Epoch 0\\Batch 42300\\ Train Loss:6.310\\ Train Accuracy:0.135\n",
      "[2019/02/28 18:59:45] Epoch 0\\Batch 42400\\ Train Loss:6.308\\ Train Accuracy:0.135\n",
      "[2019/02/28 18:59:58] Epoch 0\\Batch 42500\\ Train Loss:6.306\\ Train Accuracy:0.135\n",
      "[2019/02/28 19:00:11] Epoch 0\\Batch 42600\\ Train Loss:6.303\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:00:24] Epoch 0\\Batch 42700\\ Train Loss:6.301\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:00:36] Epoch 0\\Batch 42800\\ Train Loss:6.299\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:00:48] Epoch 0\\Batch 42900\\ Train Loss:6.297\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:01:01] Epoch 0\\Batch 43000\\ Train Loss:6.294\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:01:13] Epoch 0\\Batch 43100\\ Train Loss:6.292\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:01:25] Epoch 0\\Batch 43200\\ Train Loss:6.290\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:01:37] Epoch 0\\Batch 43300\\ Train Loss:6.288\\ Train Accuracy:0.136\n",
      "[2019/02/28 19:01:49] Epoch 0\\Batch 43400\\ Train Loss:6.285\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:02:01] Epoch 0\\Batch 43500\\ Train Loss:6.283\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:02:13] Epoch 0\\Batch 43600\\ Train Loss:6.281\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:02:26] Epoch 0\\Batch 43700\\ Train Loss:6.279\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:02:38] Epoch 0\\Batch 43800\\ Train Loss:6.277\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:02:49] Epoch 0\\Batch 43900\\ Train Loss:6.275\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:03:01] Epoch 0\\Batch 44000\\ Train Loss:6.272\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:03:13] Epoch 0\\Batch 44100\\ Train Loss:6.270\\ Train Accuracy:0.137\n",
      "[2019/02/28 19:03:25] Epoch 0\\Batch 44200\\ Train Loss:6.268\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:03:37] Epoch 0\\Batch 44300\\ Train Loss:6.266\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:03:49] Epoch 0\\Batch 44400\\ Train Loss:6.264\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:04:00] Epoch 0\\Batch 44500\\ Train Loss:6.262\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:04:12] Epoch 0\\Batch 44600\\ Train Loss:6.260\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:04:23] Epoch 0\\Batch 44700\\ Train Loss:6.258\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:04:35] Epoch 0\\Batch 44800\\ Train Loss:6.256\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:04:47] Epoch 0\\Batch 44900\\ Train Loss:6.253\\ Train Accuracy:0.138\n",
      "[2019/02/28 19:04:58] Epoch 0\\Batch 45000\\ Train Loss:6.251\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:05:10] Epoch 0\\Batch 45100\\ Train Loss:6.249\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:05:21] Epoch 0\\Batch 45200\\ Train Loss:6.247\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:05:32] Epoch 0\\Batch 45300\\ Train Loss:6.245\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:05:44] Epoch 0\\Batch 45400\\ Train Loss:6.243\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:05:55] Epoch 0\\Batch 45500\\ Train Loss:6.241\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:06:07] Epoch 0\\Batch 45600\\ Train Loss:6.239\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:06:18] Epoch 0\\Batch 45700\\ Train Loss:6.237\\ Train Accuracy:0.139\n",
      "[2019/02/28 19:06:30] Epoch 0\\Batch 45800\\ Train Loss:6.235\\ Train Accuracy:0.140\n",
      "[2019/02/28 19:06:42] Epoch 0\\Batch 45900\\ Train Loss:6.233\\ Train Accuracy:0.140\n",
      "[2019/02/28 19:06:53] Epoch 0\\Batch 46000\\ Train Loss:6.231\\ Train Accuracy:0.140\n",
      "[2019/02/28 19:07:05] Epoch 0\\Batch 46100\\ Train Loss:6.229\\ Train Accuracy:0.140\n",
      "[2019/02/28 19:07:16] Epoch 0\\Batch 46200\\ Train Loss:6.227\\ Train Accuracy:0.140\n",
      "0 1 True\n",
      "run model on validation data...\n",
      "[2019/02/28 19:07:22] Epoch 0\\ Validation Loss:5.351/ Validation Accuracy:0.151\n",
      "[2019/02/28 19:07:34] Epoch 1\\Batch 100\\ Train Loss:5.363\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:07:45] Epoch 1\\Batch 200\\ Train Loss:5.345\\ Train Accuracy:0.195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 19:07:57] Epoch 1\\Batch 300\\ Train Loss:5.336\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:08:08] Epoch 1\\Batch 400\\ Train Loss:5.331\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:08:20] Epoch 1\\Batch 500\\ Train Loss:5.324\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:08:31] Epoch 1\\Batch 600\\ Train Loss:5.330\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:08:43] Epoch 1\\Batch 700\\ Train Loss:5.334\\ Train Accuracy:0.196\n",
      "[2019/02/28 19:08:54] Epoch 1\\Batch 800\\ Train Loss:5.329\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:09:06] Epoch 1\\Batch 900\\ Train Loss:5.325\\ Train Accuracy:0.196\n",
      "[2019/02/28 19:09:17] Epoch 1\\Batch 1000\\ Train Loss:5.324\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:09:29] Epoch 1\\Batch 1100\\ Train Loss:5.323\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:09:40] Epoch 1\\Batch 1200\\ Train Loss:5.324\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:09:52] Epoch 1\\Batch 1300\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:10:03] Epoch 1\\Batch 1400\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:10:15] Epoch 1\\Batch 1500\\ Train Loss:5.322\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:10:26] Epoch 1\\Batch 1600\\ Train Loss:5.323\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:10:38] Epoch 1\\Batch 1700\\ Train Loss:5.319\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:10:50] Epoch 1\\Batch 1800\\ Train Loss:5.318\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:11:01] Epoch 1\\Batch 1900\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:11:12] Epoch 1\\Batch 2000\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:11:24] Epoch 1\\Batch 2100\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:11:35] Epoch 1\\Batch 2200\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:11:47] Epoch 1\\Batch 2300\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:11:58] Epoch 1\\Batch 2400\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:12:09] Epoch 1\\Batch 2500\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:12:20] Epoch 1\\Batch 2600\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:12:32] Epoch 1\\Batch 2700\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:12:44] Epoch 1\\Batch 2800\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:12:56] Epoch 1\\Batch 2900\\ Train Loss:5.321\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:13:08] Epoch 1\\Batch 3000\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:13:19] Epoch 1\\Batch 3100\\ Train Loss:5.320\\ Train Accuracy:0.197\n",
      "[2019/02/28 19:13:31] Epoch 1\\Batch 3200\\ Train Loss:5.317\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:13:43] Epoch 1\\Batch 3300\\ Train Loss:5.317\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:13:56] Epoch 1\\Batch 3400\\ Train Loss:5.317\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:14:08] Epoch 1\\Batch 3500\\ Train Loss:5.316\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:14:20] Epoch 1\\Batch 3600\\ Train Loss:5.316\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:14:31] Epoch 1\\Batch 3700\\ Train Loss:5.316\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:14:42] Epoch 1\\Batch 3800\\ Train Loss:5.315\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:14:55] Epoch 1\\Batch 3900\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:15:06] Epoch 1\\Batch 4000\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:15:17] Epoch 1\\Batch 4100\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:15:29] Epoch 1\\Batch 4200\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:15:40] Epoch 1\\Batch 4300\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:15:52] Epoch 1\\Batch 4400\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:16:03] Epoch 1\\Batch 4500\\ Train Loss:5.315\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:16:15] Epoch 1\\Batch 4600\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:16:27] Epoch 1\\Batch 4700\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:16:38] Epoch 1\\Batch 4800\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:16:50] Epoch 1\\Batch 4900\\ Train Loss:5.314\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:17:01] Epoch 1\\Batch 5000\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:17:13] Epoch 1\\Batch 5100\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:17:24] Epoch 1\\Batch 5200\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:17:36] Epoch 1\\Batch 5300\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:17:48] Epoch 1\\Batch 5400\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:17:59] Epoch 1\\Batch 5500\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:18:11] Epoch 1\\Batch 5600\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:18:23] Epoch 1\\Batch 5700\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:18:34] Epoch 1\\Batch 5800\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:18:46] Epoch 1\\Batch 5900\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:18:58] Epoch 1\\Batch 6000\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:19:10] Epoch 1\\Batch 6100\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:19:22] Epoch 1\\Batch 6200\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:19:33] Epoch 1\\Batch 6300\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:19:45] Epoch 1\\Batch 6400\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:19:57] Epoch 1\\Batch 6500\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:20:09] Epoch 1\\Batch 6600\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:20:20] Epoch 1\\Batch 6700\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:20:31] Epoch 1\\Batch 6800\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:20:43] Epoch 1\\Batch 6900\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:20:54] Epoch 1\\Batch 7000\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:21:05] Epoch 1\\Batch 7100\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:21:17] Epoch 1\\Batch 7200\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:21:29] Epoch 1\\Batch 7300\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:21:40] Epoch 1\\Batch 7400\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:21:52] Epoch 1\\Batch 7500\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:22:04] Epoch 1\\Batch 7600\\ Train Loss:5.313\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:22:17] Epoch 1\\Batch 7700\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:22:29] Epoch 1\\Batch 7800\\ Train Loss:5.312\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:22:41] Epoch 1\\Batch 7900\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:22:53] Epoch 1\\Batch 8000\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:23:05] Epoch 1\\Batch 8100\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:23:17] Epoch 1\\Batch 8200\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:23:28] Epoch 1\\Batch 8300\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:23:40] Epoch 1\\Batch 8400\\ Train Loss:5.311\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:23:52] Epoch 1\\Batch 8500\\ Train Loss:5.310\\ Train Accuracy:0.198\n",
      "[2019/02/28 19:24:04] Epoch 1\\Batch 8600\\ Train Loss:5.310\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:24:16] Epoch 1\\Batch 8700\\ Train Loss:5.310\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:24:28] Epoch 1\\Batch 8800\\ Train Loss:5.309\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:24:40] Epoch 1\\Batch 8900\\ Train Loss:5.309\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:24:51] Epoch 1\\Batch 9000\\ Train Loss:5.309\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:25:03] Epoch 1\\Batch 9100\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:25:15] Epoch 1\\Batch 9200\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:25:26] Epoch 1\\Batch 9300\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:25:38] Epoch 1\\Batch 9400\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:25:50] Epoch 1\\Batch 9500\\ Train Loss:5.309\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:26:02] Epoch 1\\Batch 9600\\ Train Loss:5.309\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:26:15] Epoch 1\\Batch 9700\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:26:26] Epoch 1\\Batch 9800\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:26:38] Epoch 1\\Batch 9900\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:26:50] Epoch 1\\Batch 10000\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:27:01] Epoch 1\\Batch 10100\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:27:13] Epoch 1\\Batch 10200\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:27:25] Epoch 1\\Batch 10300\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:27:36] Epoch 1\\Batch 10400\\ Train Loss:5.307\\ Train Accuracy:0.199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 19:27:47] Epoch 1\\Batch 10500\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:27:59] Epoch 1\\Batch 10600\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:28:11] Epoch 1\\Batch 10700\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:28:23] Epoch 1\\Batch 10800\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:28:35] Epoch 1\\Batch 10900\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:28:46] Epoch 1\\Batch 11000\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:28:57] Epoch 1\\Batch 11100\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:29:09] Epoch 1\\Batch 11200\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:29:21] Epoch 1\\Batch 11300\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:29:33] Epoch 1\\Batch 11400\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:29:44] Epoch 1\\Batch 11500\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:29:55] Epoch 1\\Batch 11600\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:30:07] Epoch 1\\Batch 11700\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:30:18] Epoch 1\\Batch 11800\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:30:29] Epoch 1\\Batch 11900\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:30:40] Epoch 1\\Batch 12000\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:30:52] Epoch 1\\Batch 12100\\ Train Loss:5.308\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:31:04] Epoch 1\\Batch 12200\\ Train Loss:5.307\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:31:15] Epoch 1\\Batch 12300\\ Train Loss:5.307\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:31:27] Epoch 1\\Batch 12400\\ Train Loss:5.307\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:31:38] Epoch 1\\Batch 12500\\ Train Loss:5.307\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:31:49] Epoch 1\\Batch 12600\\ Train Loss:5.307\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:32:00] Epoch 1\\Batch 12700\\ Train Loss:5.306\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:32:11] Epoch 1\\Batch 12800\\ Train Loss:5.306\\ Train Accuracy:0.199\n",
      "[2019/02/28 19:32:22] Epoch 1\\Batch 12900\\ Train Loss:5.306\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:32:33] Epoch 1\\Batch 13000\\ Train Loss:5.306\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:32:44] Epoch 1\\Batch 13100\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:32:55] Epoch 1\\Batch 13200\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:33:07] Epoch 1\\Batch 13300\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:33:18] Epoch 1\\Batch 13400\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:33:30] Epoch 1\\Batch 13500\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:33:41] Epoch 1\\Batch 13600\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:33:53] Epoch 1\\Batch 13700\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:34:05] Epoch 1\\Batch 13800\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:34:16] Epoch 1\\Batch 13900\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:34:27] Epoch 1\\Batch 14000\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:34:39] Epoch 1\\Batch 14100\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:34:51] Epoch 1\\Batch 14200\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:35:02] Epoch 1\\Batch 14300\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:35:13] Epoch 1\\Batch 14400\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:35:24] Epoch 1\\Batch 14500\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:35:36] Epoch 1\\Batch 14600\\ Train Loss:5.305\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:35:47] Epoch 1\\Batch 14700\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:35:58] Epoch 1\\Batch 14800\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:36:09] Epoch 1\\Batch 14900\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:36:20] Epoch 1\\Batch 15000\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:36:32] Epoch 1\\Batch 15100\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:36:43] Epoch 1\\Batch 15200\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:36:55] Epoch 1\\Batch 15300\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:37:06] Epoch 1\\Batch 15400\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:37:17] Epoch 1\\Batch 15500\\ Train Loss:5.304\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:37:29] Epoch 1\\Batch 15600\\ Train Loss:5.303\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:37:40] Epoch 1\\Batch 15700\\ Train Loss:5.303\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:37:52] Epoch 1\\Batch 15800\\ Train Loss:5.303\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:38:03] Epoch 1\\Batch 15900\\ Train Loss:5.303\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:38:14] Epoch 1\\Batch 16000\\ Train Loss:5.303\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:38:25] Epoch 1\\Batch 16100\\ Train Loss:5.302\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:38:37] Epoch 1\\Batch 16200\\ Train Loss:5.302\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:38:48] Epoch 1\\Batch 16300\\ Train Loss:5.302\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:39:00] Epoch 1\\Batch 16400\\ Train Loss:5.302\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:39:11] Epoch 1\\Batch 16500\\ Train Loss:5.302\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:39:23] Epoch 1\\Batch 16600\\ Train Loss:5.302\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:39:34] Epoch 1\\Batch 16700\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:39:46] Epoch 1\\Batch 16800\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:39:57] Epoch 1\\Batch 16900\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:40:09] Epoch 1\\Batch 17000\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:40:21] Epoch 1\\Batch 17100\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:40:33] Epoch 1\\Batch 17200\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:40:44] Epoch 1\\Batch 17300\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:40:55] Epoch 1\\Batch 17400\\ Train Loss:5.301\\ Train Accuracy:0.200\n",
      "[2019/02/28 19:41:05] Epoch 1\\Batch 17500\\ Train Loss:5.301\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:41:14] Epoch 1\\Batch 17600\\ Train Loss:5.301\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:41:23] Epoch 1\\Batch 17700\\ Train Loss:5.301\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:41:33] Epoch 1\\Batch 17800\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:41:43] Epoch 1\\Batch 17900\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:41:53] Epoch 1\\Batch 18000\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:42:02] Epoch 1\\Batch 18100\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:42:12] Epoch 1\\Batch 18200\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:42:22] Epoch 1\\Batch 18300\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:42:32] Epoch 1\\Batch 18400\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:42:41] Epoch 1\\Batch 18500\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:42:51] Epoch 1\\Batch 18600\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:00] Epoch 1\\Batch 18700\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:10] Epoch 1\\Batch 18800\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:19] Epoch 1\\Batch 18900\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:29] Epoch 1\\Batch 19000\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:38] Epoch 1\\Batch 19100\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:48] Epoch 1\\Batch 19200\\ Train Loss:5.300\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:43:57] Epoch 1\\Batch 19300\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:44:07] Epoch 1\\Batch 19400\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:44:17] Epoch 1\\Batch 19500\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:44:27] Epoch 1\\Batch 19600\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:44:36] Epoch 1\\Batch 19700\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:44:45] Epoch 1\\Batch 19800\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:44:55] Epoch 1\\Batch 19900\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:45:05] Epoch 1\\Batch 20000\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:45:14] Epoch 1\\Batch 20100\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:45:24] Epoch 1\\Batch 20200\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:45:34] Epoch 1\\Batch 20300\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:45:43] Epoch 1\\Batch 20400\\ Train Loss:5.299\\ Train Accuracy:0.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 19:45:52] Epoch 1\\Batch 20500\\ Train Loss:5.299\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:46:02] Epoch 1\\Batch 20600\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:46:12] Epoch 1\\Batch 20700\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:46:21] Epoch 1\\Batch 20800\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:46:31] Epoch 1\\Batch 20900\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:46:41] Epoch 1\\Batch 21000\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:46:51] Epoch 1\\Batch 21100\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:00] Epoch 1\\Batch 21200\\ Train Loss:5.298\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:10] Epoch 1\\Batch 21300\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:20] Epoch 1\\Batch 21400\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:30] Epoch 1\\Batch 21500\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:39] Epoch 1\\Batch 21600\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:49] Epoch 1\\Batch 21700\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:47:58] Epoch 1\\Batch 21800\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:48:08] Epoch 1\\Batch 21900\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:48:18] Epoch 1\\Batch 22000\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:48:27] Epoch 1\\Batch 22100\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:48:37] Epoch 1\\Batch 22200\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:48:46] Epoch 1\\Batch 22300\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:48:56] Epoch 1\\Batch 22400\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:49:06] Epoch 1\\Batch 22500\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:49:16] Epoch 1\\Batch 22600\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:49:26] Epoch 1\\Batch 22700\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:49:35] Epoch 1\\Batch 22800\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:49:45] Epoch 1\\Batch 22900\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:49:55] Epoch 1\\Batch 23000\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:50:05] Epoch 1\\Batch 23100\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:50:15] Epoch 1\\Batch 23200\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:50:25] Epoch 1\\Batch 23300\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:50:34] Epoch 1\\Batch 23400\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:50:44] Epoch 1\\Batch 23500\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:50:55] Epoch 1\\Batch 23600\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:51:05] Epoch 1\\Batch 23700\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:51:15] Epoch 1\\Batch 23800\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:51:24] Epoch 1\\Batch 23900\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:51:35] Epoch 1\\Batch 24000\\ Train Loss:5.297\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:51:44] Epoch 1\\Batch 24100\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:51:54] Epoch 1\\Batch 24200\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:52:04] Epoch 1\\Batch 24300\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:52:14] Epoch 1\\Batch 24400\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:52:24] Epoch 1\\Batch 24500\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:52:34] Epoch 1\\Batch 24600\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:52:44] Epoch 1\\Batch 24700\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:52:53] Epoch 1\\Batch 24800\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:53:03] Epoch 1\\Batch 24900\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:53:13] Epoch 1\\Batch 25000\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:53:22] Epoch 1\\Batch 25100\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:53:32] Epoch 1\\Batch 25200\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:53:42] Epoch 1\\Batch 25300\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:53:52] Epoch 1\\Batch 25400\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:54:02] Epoch 1\\Batch 25500\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:54:12] Epoch 1\\Batch 25600\\ Train Loss:5.296\\ Train Accuracy:0.201\n",
      "[2019/02/28 19:54:22] Epoch 1\\Batch 25700\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:54:32] Epoch 1\\Batch 25800\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:54:42] Epoch 1\\Batch 25900\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:54:52] Epoch 1\\Batch 26000\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:55:02] Epoch 1\\Batch 26100\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:55:13] Epoch 1\\Batch 26200\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:55:24] Epoch 1\\Batch 26300\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:55:37] Epoch 1\\Batch 26400\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:55:50] Epoch 1\\Batch 26500\\ Train Loss:5.296\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:56:04] Epoch 1\\Batch 26600\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:56:17] Epoch 1\\Batch 26700\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:56:31] Epoch 1\\Batch 26800\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:56:44] Epoch 1\\Batch 26900\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:56:57] Epoch 1\\Batch 27000\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:57:10] Epoch 1\\Batch 27100\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:57:23] Epoch 1\\Batch 27200\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:57:37] Epoch 1\\Batch 27300\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:57:50] Epoch 1\\Batch 27400\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:58:03] Epoch 1\\Batch 27500\\ Train Loss:5.295\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:58:17] Epoch 1\\Batch 27600\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:58:30] Epoch 1\\Batch 27700\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:58:44] Epoch 1\\Batch 27800\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:58:57] Epoch 1\\Batch 27900\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:59:10] Epoch 1\\Batch 28000\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:59:24] Epoch 1\\Batch 28100\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:59:38] Epoch 1\\Batch 28200\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 19:59:50] Epoch 1\\Batch 28300\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:00:02] Epoch 1\\Batch 28400\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:00:16] Epoch 1\\Batch 28500\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:00:29] Epoch 1\\Batch 28600\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:00:56] Epoch 1\\Batch 28800\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:01:09] Epoch 1\\Batch 28900\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:01:22] Epoch 1\\Batch 29000\\ Train Loss:5.294\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:01:36] Epoch 1\\Batch 29100\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:01:49] Epoch 1\\Batch 29200\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:02:03] Epoch 1\\Batch 29300\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:02:16] Epoch 1\\Batch 29400\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:02:29] Epoch 1\\Batch 29500\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:02:42] Epoch 1\\Batch 29600\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:02:55] Epoch 1\\Batch 29700\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:03:09] Epoch 1\\Batch 29800\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:03:22] Epoch 1\\Batch 29900\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:03:35] Epoch 1\\Batch 30000\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:03:48] Epoch 1\\Batch 30100\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:04:01] Epoch 1\\Batch 30200\\ Train Loss:5.293\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:04:15] Epoch 1\\Batch 30300\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:04:29] Epoch 1\\Batch 30400\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:04:42] Epoch 1\\Batch 30500\\ Train Loss:5.292\\ Train Accuracy:0.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/02/28 20:04:55] Epoch 1\\Batch 30600\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:05:09] Epoch 1\\Batch 30700\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:05:22] Epoch 1\\Batch 30800\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:05:35] Epoch 1\\Batch 30900\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:05:49] Epoch 1\\Batch 31000\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:06:03] Epoch 1\\Batch 31100\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:06:16] Epoch 1\\Batch 31200\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:06:30] Epoch 1\\Batch 31300\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:06:43] Epoch 1\\Batch 31400\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:06:57] Epoch 1\\Batch 31500\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:07:11] Epoch 1\\Batch 31600\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:07:24] Epoch 1\\Batch 31700\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:07:37] Epoch 1\\Batch 31800\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:07:50] Epoch 1\\Batch 31900\\ Train Loss:5.292\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:08:30] Epoch 1\\Batch 32200\\ Train Loss:5.291\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:08:44] Epoch 1\\Batch 32300\\ Train Loss:5.291\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:08:57] Epoch 1\\Batch 32400\\ Train Loss:5.291\\ Train Accuracy:0.203\n",
      "[2019/02/28 20:09:10] Epoch 1\\Batch 32500\\ Train Loss:5.291\\ Train Accuracy:0.203\n",
      "[2019/02/28 20:09:23] Epoch 1\\Batch 32600\\ Train Loss:5.291\\ Train Accuracy:0.203\n",
      "[2019/02/28 20:09:36] Epoch 1\\Batch 32700\\ Train Loss:5.291\\ Train Accuracy:0.203\n",
      "[2019/02/28 20:09:48] Epoch 1\\Batch 32800\\ Train Loss:5.291\\ Train Accuracy:0.202\n",
      "[2019/02/28 20:10:01] Epoch 1\\Batch 32900\\ Train Loss:5.291\\ Train Accuracy:0.203\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
