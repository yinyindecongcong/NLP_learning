{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义TextRNN结构，使用双向的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN:\n",
    "    def __init__(self, batch_size, num_classes, vocab_size, sentence_len, embed_size, \n",
    "                 learning_rate, decay_steps, decay_rate, is_training):\n",
    "        #1.定义超参数\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = embed_size #lstm层的维度\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_training = is_training\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "        \n",
    "        #2.输入\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sentence_len], 'input_x')\n",
    "#         self.input_y = tf.placeholder(tf.int32, [None], 'input_y') #单个标签\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], 'input_y') #多个标签\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        #3.初始化全连接层参数\n",
    "        self.init_weight()\n",
    "        \n",
    "        #4.网络结构\n",
    "        self.logits = self.inference() #[batch_size, num_classes]\n",
    "        \n",
    "        #5.损失函数\n",
    "        self.loss_val = self.loss()\n",
    "        \n",
    "        #6.优化器\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], dtype=tf.float32)\n",
    "        self.W = tf.get_variable('W', [self.hidden_size * 2, self.num_classes], dtype=tf.float32) #双向LSTM，输出concat，所以此处为2倍\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        # a.embedding\n",
    "        self.sentence_embed = tf.nn.embedding_lookup(self.Embedding, self.input_x) #[batch_size, sentence_len, embed_size]\n",
    "        \n",
    "        # b.bidiretional lstm\n",
    "        self.fw_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size) #前向单元\n",
    "        self.bw_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size) #后向单元\n",
    "#         if self.dropout_keep_prob is not None:\n",
    "#             self.fw_cell = tf.contrib.rnn.DropoutWrapper(self.fw_cell, output_keep_prob=) \n",
    "                #input_keep_prob是对输入而言，output_keep_prb是对lstm各层而言\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(self.fw_cell, self.bw_cell, self.sentence_embed, dtype=tf.float32)\n",
    "        #输入为 [batch_size, sentence_len, embed_size]，输出为大小为2的元组，每个元素为[batch_size, sentence_len, hidden_size]\n",
    "        \n",
    "        # c.concat\n",
    "        fw_output = outputs[0][:,-1,:]\n",
    "        bw_output = outputs[1][:,-1,:] #[batch_size, 1, hidden_size]\n",
    "        final_output = tf.concat([fw_output, bw_output], axis=1) #[batch_size, 1, hidden_size*2]\n",
    "        final_output = tf.reshape(final_output, [-1, self.hidden_size*2]) #[batch_size, hidden_size * 2]\n",
    "        \n",
    "        # d.full_connection\n",
    "        logits = tf.matmul(final_output, self.W) + self.b #[batch_size, num_classes]\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, l2_lambda=0.0001):\n",
    "#         loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "        #先将label转化为one-hot形式，再对logits计算softmax，最后计算交叉熵\n",
    "        loss1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "        loss1 = tf.reduce_mean(tf.reduce_sum(loss1, axis=1))\n",
    "        loss2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) * l2_lambda\n",
    "        return loss1 + loss2\n",
    "    \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_step, learning_rate, optimizer='Adam')\n",
    "        return train_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=5\n",
    "    learning_rate=0.01\n",
    "    batch_size=5\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextRNN(batch_size, num_classes, vocab_size, sequence_length, embed_size, \n",
    "                     learning_rate, decay_step, decay_rate, True)\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 2,size=(batch_size, num_classes), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, logits, _ = sess.run([model.loss_val, model.logits, model.train_op],\n",
    "                                            feed_dict={model.input_x: input_x, model.input_y: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob})\n",
    "            logits = np.argsort(logits)\n",
    "            print('****label****\\n', input_y)\n",
    "            print('****pre_y****\\n', logits, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'W:0' shape=(200, 5) dtype=float32_ref>, <tf.Variable 'b:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/fw/basic_lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/fw/basic_lstm_cell/bias:0' shape=(400,) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/bw/basic_lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'bidirectional_rnn/bw/basic_lstm_cell/bias:0' shape=(400,) dtype=float32_ref>]\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[4 0 1 2 3]\n",
      " [4 0 1 2 3]\n",
      " [4 0 1 2 3]\n",
      " [4 0 1 2 3]\n",
      " [4 0 1 2 3]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[4 1 0 2 3]\n",
      " [4 0 1 3 2]\n",
      " [4 0 1 3 2]\n",
      " [4 0 1 3 2]\n",
      " [4 1 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[4 1 0 2 3]\n",
      " [4 1 0 3 2]\n",
      " [4 0 1 3 2]\n",
      " [4 1 0 3 2]\n",
      " [4 1 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 4 2 0 3]\n",
      " [4 1 0 3 2]\n",
      " [4 0 1 3 2]\n",
      " [4 1 0 3 2]\n",
      " [4 1 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [4 0 3 1 2]\n",
      " [1 4 2 0 3]\n",
      " [1 4 3 0 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 3 0 2]\n",
      " [4 0 3 1 2]\n",
      " [1 2 4 0 3]\n",
      " [1 4 3 2 0]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [4 0 3 1 2]\n",
      " [2 1 4 3 0]\n",
      " [1 4 2 3 0]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [3 4 0 1 2]\n",
      " [2 1 4 3 0]\n",
      " [1 4 3 0 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [3 0 4 1 2]\n",
      " [2 1 0 3 4]\n",
      " [1 4 3 0 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [0 3 4 1 2]\n",
      " [2 0 3 1 4]\n",
      " [1 4 2 3 0]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [0 3 4 1 2]\n",
      " [2 3 0 4 1]\n",
      " [1 4 2 3 0]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [0 3 4 1 2]\n",
      " [3 0 2 4 1]\n",
      " [1 4 2 0 3]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 3 0 1 2]\n",
      " [0 3 4 1 2]\n",
      " [3 0 2 1 4]\n",
      " [1 4 2 0 3]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [3 0 2 1 4]\n",
      " [1 4 0 2 3]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [3 1 2 0 4]\n",
      " [1 4 0 2 3]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [1 3 2 0 4]\n",
      " [1 4 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [1 3 2 0 4]\n",
      " [1 4 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [1 2 3 0 4]\n",
      " [1 4 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [1 2 3 0 4]\n",
      " [1 4 0 3 2]] \n",
      "\n",
      "****label****\n",
      " [[1 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 1 1]\n",
      " [1 0 1 1 0]]\n",
      "****pre_y****\n",
      " [[1 2 4 3 0]\n",
      " [4 1 0 3 2]\n",
      " [0 3 4 2 1]\n",
      " [1 2 3 0 4]\n",
      " [1 4 0 3 2]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#定义超参数\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 256, 'batch_size')\n",
    "tf.app.flags.DEFINE_integer('num_classes', 1999, 'num_classes')\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 100, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 1000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 10, 'number of epoch')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"testrnn_multilabel_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"textrnn_multilabel_checkpoint/data_cache.pik\",\"data chche for the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = '/data/chenhy/data/ieee_zhihu_cup/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    vocab_size = len(word2index)\n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "\n",
    "    print(\"train_X[0:5]:\", train_X[0:5])\n",
    "    print(\"train_Y[0:5]:\", train_y[0:5])\n",
    "    train_y_short = get_target_label_short(train_y[0])\n",
    "    print(\"train_y_short:\", train_y_short)\n",
    "    \n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextRNN(FLAGS.batch_size, FLAGS.num_classes, vocab_size, FLAGS.sentence_len,FLAGS.embed_size, \n",
    "                        FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.is_training)\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        CONTINUE_TRAIN = True\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            log(\"restore from checkpoint\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            if CONTINUE_TRAIN: log(\"continue training...\")\n",
    "        else:\n",
    "            log('init variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "#             #是否使用embedding\n",
    "#             print('assign pre-trained embedding')\n",
    "#             embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "#             sess.run(embedding_assign)\n",
    "        if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint') or CONTINUE_TRAIN:\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = sess.run(model.epoch_step)\n",
    "                loss, counter = 0., 0.\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    loss_tmp,  _ = sess.run([model.loss_val, model.train_op], \n",
    "                                                    feed_dict={model.input_x: train_X[start:end, :FLAGS.sentence_len], model.input_y: train_y[start:end],\n",
    "                                                               model.dropout_keep_prob: 1})\n",
    "                    loss, counter = loss + loss_tmp, counter + 1\n",
    "                    if counter % 200 == 0:\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\"%(epoch, counter, loss/float(counter)))\n",
    "                    if counter % 1400 == 0:\n",
    "                        print('run model on validation data...')\n",
    "                        loss_valid, f1_score, precision, recall = do_eval(sess, model, vaild_X, valid_y)\n",
    "                        log(\"Epoch %d/ Validation Loss:%.3f/ F1_score:%.3f/ Precision:%.3f/ Recall:%.3f\"%(epoch, loss_valid, f1_score, precision, recall))\n",
    "                        #save the checkpoint\n",
    "                        save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                        saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些辅助函数\n",
    "def get_target_label_short(eval_y):\n",
    "    res = [idx for idx in range(len(eval_y)) if eval_y[idx] > 0] #结果如：[45,100,1555]\n",
    "    return res\n",
    "\n",
    "def get_label_using_logits(logits, top_number=5):\n",
    "    predict_y = [idx for idx in range(len(logits)) if logits[idx] >= 0.5]\n",
    "    if len(predict_y) == 0: predict_y = [np.argmax(logits)]\n",
    "    return predict_y\n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    #embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y\n",
    "\n",
    "def do_eval(sess, model, eval_X, eval_y):\n",
    "    test_X, test_y = eval_X[:3000], eval_y[:3000]\n",
    "    num_of_data = len(test_y)\n",
    "    batch_size = 1\n",
    "    loss, F1, p, r = 0., 0., 0., 0.\n",
    "    label_dict_confuse = {'TP':0.000001, 'FN':0.000001, 'FP':0.000001}\n",
    "    for start in range(num_of_data):\n",
    "        end = start + 1\n",
    "        lo,logits = sess.run([model.loss_val, model.logits], \n",
    "                        feed_dict={model.input_x: test_X[start:end,:FLAGS.sentence_len], model.input_y: test_y[start:end],\n",
    "                                   model.dropout_keep_prob:1.0})\n",
    "        loss += lo\n",
    "        pre = get_label_using_logits(logits[0])\n",
    "        label = get_target_label_short(test_y[start])\n",
    "#         pre = np.argsort(logits[0])[-5:]\n",
    "#         label = [i for i in range(len(test_y[start])) if test_y[start][i] > 0]\n",
    "        if start == 0: print('label:',label, 'predict:', pre)\n",
    "        inter = len([x for x in pre if x in label])\n",
    "        label_dict_confuse['TP'] += inter\n",
    "        label_dict_confuse['FN'] += len(label) - inter\n",
    "        label_dict_confuse['FP'] += len(pre) - inter\n",
    "    print(label_dict_confuse)\n",
    "    p = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FP'])\n",
    "    r = float(label_dict_confuse['TP'])/(label_dict_confuse['TP']+label_dict_confuse['FN'])\n",
    "    if p + r == 0: return loss/num_of_data, 0, 0, 0\n",
    "    F1 = (2 * p * r)/(p + r)\n",
    "    return loss/num_of_data, F1, p, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X[0:5]: [[ 832   60  256 1172 3407  516   96  138  103 1108   16    3   96  177\n",
      "    22   11  672   53   18 1560 1560   15   65   12  180   10  342  173\n",
      "    13  103  141  707  191   12  342  173   15   13   22   11  229  264\n",
      "   163 1362  135 1249  156  156  731  115   84   10  808 1713  103  141\n",
      "   229  264  788  421  103  141   12   95  316   10  808 1713  103  141\n",
      "    12 2413 1227   15 1397  997   22  116  301  489   12   18  858   99\n",
      "   596   98   26  646  813   10  386 1093  197  767   22   11 1179 1849\n",
      "   593   84   22   11   94  102  322   60  190  220  583  355   10  153\n",
      "   103  141  192  153   12   10 1244  466  116  103  141  189   58  130\n",
      "    95  316   12  788  421  699   16    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 270  154  166  504   53  154   89   60   13  210  259  589   12  151\n",
      "    18 1682   10  154  150  113   97   26  145   67   33   16    3   61\n",
      "    18  118  151   18  948   10  151   18  550   80  123  159   93  154\n",
      "    12  150  186  104  174  537   10  116   66   41   12  117  195   10\n",
      "   104   66   41  116  154   12  117  195   16   95  316  104  590  605\n",
      "   610   19   59   19  202  502  214  516  398   16    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 186  163  284  109  108   32  437   41  232  555  121  177   47   19\n",
      "    67   33  188  309  576  948   12  670  775   16    3    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  32  143  194  453  537  903  450  608   95   10  332   15   47   97\n",
      "   892   75  231  143  238  450   10   97  444  444  314  278   12  166\n",
      "   410   16    3  177  257  338  159   32  143  194   12   18  207   56\n",
      "    75  453  537  903   75  450  608   95   10   75  389  113   12  629\n",
      "    56  342  173   99   13  157  255  629  361   10 1238  268   13  537\n",
      "   903 1950   75   10 2004 1790   53   87   92  188  309  281   75  381\n",
      "  1950   75   12  701  841   10 1917  602  703  763 2205 2574  815  909\n",
      "   909  133   84   26   22   11  177  257  103  301 2739 2162   10   32\n",
      "   143  285  397  593  576  608   54   18  208 1078   10  880 1828  454\n",
      "  1083   10  202  502   32  143  194   75  124  451  173 1426  817   22\n",
      "    11  746  194  399  609   71   13  121  177   10  444  264  424  399\n",
      "  2436  553  241  632  629 1302  847  255   10  125  542  158  724  724\n",
      "   593  650   78  236   10  502  159  168   13  650   53   26   75  389\n",
      "    12  399  609   78  236   10  200   93  612  593  972  807  628  305\n",
      "   461   10 2060 2372]\n",
      " [ 224  461 3115  912  560  833  224  333  224 1499  359  106   83   52\n",
      "    36   30   35   21  253  372  414   96  138   16    3  434  135   13\n",
      "   300  352 1078  351  614  345  189  129   18   62  295   10  230  160\n",
      "   108  437  774 1292  372   18  118  347   22  224  461  130 3115  912\n",
      "   130  560  833  130  224  333  130  224 1499  359  106   83   52   36\n",
      "    30   35   21  253  372  414   96  138   16    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "train_Y[0:5]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_y_short: [1473]\n",
      "[2019/03/23 08:28:30] restore from checkpoint\n",
      "INFO:tensorflow:Restoring parameters from testrnn_multilabel_checkpoint/model.ckpt-2\n",
      "[2019/03/23 08:28:30] continue training...\n",
      "[2019/03/23 08:30:53] Epoch 2\\Batch 200\\ Train Loss:16.031\n",
      "[2019/03/23 08:34:33] Epoch 2\\Batch 400\\ Train Loss:15.970\n",
      "[2019/03/23 08:38:11] Epoch 2\\Batch 600\\ Train Loss:15.912\n",
      "[2019/03/23 08:41:50] Epoch 2\\Batch 800\\ Train Loss:15.851\n",
      "[2019/03/23 08:45:29] Epoch 2\\Batch 1000\\ Train Loss:15.790\n",
      "[2019/03/23 08:49:09] Epoch 2\\Batch 1200\\ Train Loss:15.727\n",
      "[2019/03/23 08:52:47] Epoch 2\\Batch 1400\\ Train Loss:15.666\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6724.000001, 'FP': 2725.000001, 'TP': 275.000001}\n",
      "[2019/03/23 08:53:41] Epoch 2/ Validation Loss:15.244/ F1_score:0.055/ Precision:0.092/ Recall:0.039\n",
      "[2019/03/23 08:57:21] Epoch 2\\Batch 1600\\ Train Loss:15.868\n",
      "[2019/03/23 09:01:01] Epoch 2\\Batch 1800\\ Train Loss:15.853\n",
      "[2019/03/23 09:04:41] Epoch 2\\Batch 2000\\ Train Loss:15.819\n",
      "[2019/03/23 09:08:21] Epoch 2\\Batch 2200\\ Train Loss:15.778\n",
      "[2019/03/23 09:12:01] Epoch 2\\Batch 2400\\ Train Loss:15.734\n",
      "[2019/03/23 09:15:44] Epoch 2\\Batch 2600\\ Train Loss:15.691\n",
      "[2019/03/23 09:19:33] Epoch 2\\Batch 2800\\ Train Loss:15.647\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6716.000001, 'FP': 2717.000001, 'TP': 283.000001}\n",
      "[2019/03/23 09:20:29] Epoch 2/ Validation Loss:15.033/ F1_score:0.057/ Precision:0.094/ Recall:0.040\n",
      "[2019/03/23 09:26:00] Epoch 3\\Batch 200\\ Train Loss:14.997\n",
      "[2019/03/23 09:29:46] Epoch 3\\Batch 400\\ Train Loss:14.967\n",
      "[2019/03/23 09:33:25] Epoch 3\\Batch 600\\ Train Loss:14.941\n",
      "[2019/03/23 09:37:02] Epoch 3\\Batch 800\\ Train Loss:14.911\n",
      "[2019/03/23 09:40:42] Epoch 3\\Batch 1000\\ Train Loss:14.886\n",
      "[2019/03/23 09:44:21] Epoch 3\\Batch 1200\\ Train Loss:14.861\n",
      "[2019/03/23 09:48:00] Epoch 3\\Batch 1400\\ Train Loss:14.838\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6656.000001, 'FP': 2657.000001, 'TP': 343.000001}\n",
      "[2019/03/23 09:48:51] Epoch 3/ Validation Loss:14.673/ F1_score:0.069/ Precision:0.114/ Recall:0.049\n",
      "[2019/03/23 09:52:30] Epoch 3\\Batch 1600\\ Train Loss:14.809\n",
      "[2019/03/23 09:56:10] Epoch 3\\Batch 1800\\ Train Loss:14.784\n",
      "[2019/03/23 09:59:49] Epoch 3\\Batch 2000\\ Train Loss:14.763\n",
      "[2019/03/23 10:03:28] Epoch 3\\Batch 2200\\ Train Loss:14.742\n",
      "[2019/03/23 10:07:08] Epoch 3\\Batch 2400\\ Train Loss:14.721\n",
      "[2019/03/23 10:10:48] Epoch 3\\Batch 2600\\ Train Loss:14.699\n",
      "[2019/03/23 10:14:27] Epoch 3\\Batch 2800\\ Train Loss:14.677\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6608.000001, 'FP': 2609.000001, 'TP': 391.000001}\n",
      "[2019/03/23 10:15:18] Epoch 3/ Validation Loss:14.379/ F1_score:0.078/ Precision:0.130/ Recall:0.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/23 10:20:37] Epoch 4\\Batch 200\\ Train Loss:14.351\n",
      "[2019/03/23 10:24:15] Epoch 4\\Batch 400\\ Train Loss:14.331\n",
      "[2019/03/23 10:27:55] Epoch 4\\Batch 600\\ Train Loss:14.315\n",
      "[2019/03/23 10:31:34] Epoch 4\\Batch 800\\ Train Loss:14.296\n",
      "[2019/03/23 10:35:15] Epoch 4\\Batch 1000\\ Train Loss:14.280\n",
      "[2019/03/23 10:38:55] Epoch 4\\Batch 1200\\ Train Loss:14.264\n",
      "[2019/03/23 10:42:35] Epoch 4\\Batch 1400\\ Train Loss:14.247\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6570.000001, 'FP': 2571.000001, 'TP': 429.000001}\n",
      "[2019/03/23 10:43:27] Epoch 4/ Validation Loss:14.128/ F1_score:0.086/ Precision:0.143/ Recall:0.061\n",
      "[2019/03/23 10:47:07] Epoch 4\\Batch 1600\\ Train Loss:14.225\n",
      "[2019/03/23 10:50:48] Epoch 4\\Batch 1800\\ Train Loss:14.206\n",
      "[2019/03/23 10:54:28] Epoch 4\\Batch 2000\\ Train Loss:14.191\n",
      "[2019/03/23 10:58:09] Epoch 4\\Batch 2200\\ Train Loss:14.177\n",
      "[2019/03/23 11:01:49] Epoch 4\\Batch 2400\\ Train Loss:14.160\n",
      "[2019/03/23 11:05:28] Epoch 4\\Batch 2600\\ Train Loss:14.144\n",
      "[2019/03/23 11:09:08] Epoch 4\\Batch 2800\\ Train Loss:14.127\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6540.000001, 'FP': 2541.000001, 'TP': 459.000001}\n",
      "[2019/03/23 11:10:00] Epoch 4/ Validation Loss:13.903/ F1_score:0.092/ Precision:0.153/ Recall:0.066\n",
      "[2019/03/23 11:15:18] Epoch 5\\Batch 200\\ Train Loss:13.887\n",
      "[2019/03/23 11:18:58] Epoch 5\\Batch 400\\ Train Loss:13.867\n",
      "[2019/03/23 11:22:37] Epoch 5\\Batch 600\\ Train Loss:13.853\n",
      "[2019/03/23 11:26:17] Epoch 5\\Batch 800\\ Train Loss:13.838\n",
      "[2019/03/23 11:29:57] Epoch 5\\Batch 1000\\ Train Loss:13.827\n",
      "[2019/03/23 11:33:38] Epoch 5\\Batch 1200\\ Train Loss:13.814\n",
      "[2019/03/23 11:37:18] Epoch 5\\Batch 1400\\ Train Loss:13.801\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6493.000001, 'FP': 2495.000001, 'TP': 506.000001}\n",
      "[2019/03/23 11:38:10] Epoch 5/ Validation Loss:13.697/ F1_score:0.101/ Precision:0.169/ Recall:0.072\n",
      "[2019/03/23 11:41:49] Epoch 5\\Batch 1600\\ Train Loss:13.784\n",
      "[2019/03/23 11:45:30] Epoch 5\\Batch 1800\\ Train Loss:13.770\n",
      "[2019/03/23 11:49:09] Epoch 5\\Batch 2000\\ Train Loss:13.759\n",
      "[2019/03/23 11:52:48] Epoch 5\\Batch 2200\\ Train Loss:13.750\n",
      "[2019/03/23 11:56:28] Epoch 5\\Batch 2400\\ Train Loss:13.738\n",
      "[2019/03/23 12:00:07] Epoch 5\\Batch 2600\\ Train Loss:13.727\n",
      "[2019/03/23 12:03:47] Epoch 5\\Batch 2800\\ Train Loss:13.716\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6472.000001, 'FP': 2474.000001, 'TP': 527.000001}\n",
      "[2019/03/23 12:04:38] Epoch 5/ Validation Loss:13.557/ F1_score:0.105/ Precision:0.176/ Recall:0.075\n",
      "[2019/03/23 12:09:56] Epoch 6\\Batch 200\\ Train Loss:13.566\n",
      "[2019/03/23 12:13:35] Epoch 6\\Batch 400\\ Train Loss:13.552\n",
      "[2019/03/23 12:17:17] Epoch 6\\Batch 600\\ Train Loss:13.544\n",
      "[2019/03/23 12:21:06] Epoch 6\\Batch 800\\ Train Loss:13.535\n",
      "[2019/03/23 12:24:54] Epoch 6\\Batch 1000\\ Train Loss:13.529\n",
      "[2019/03/23 12:28:42] Epoch 6\\Batch 1200\\ Train Loss:13.522\n",
      "[2019/03/23 12:32:25] Epoch 6\\Batch 1400\\ Train Loss:13.516\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6449.000001, 'FP': 2451.000001, 'TP': 550.000001}\n",
      "[2019/03/23 12:33:17] Epoch 6/ Validation Loss:13.449/ F1_score:0.110/ Precision:0.183/ Recall:0.079\n",
      "[2019/03/23 12:36:58] Epoch 6\\Batch 1600\\ Train Loss:13.505\n",
      "[2019/03/23 12:40:38] Epoch 6\\Batch 1800\\ Train Loss:13.497\n",
      "[2019/03/23 12:44:16] Epoch 6\\Batch 2000\\ Train Loss:13.492\n",
      "[2019/03/23 12:47:57] Epoch 6\\Batch 2200\\ Train Loss:13.488\n",
      "[2019/03/23 12:51:36] Epoch 6\\Batch 2400\\ Train Loss:13.481\n",
      "[2019/03/23 12:55:15] Epoch 6\\Batch 2600\\ Train Loss:13.476\n",
      "[2019/03/23 12:58:55] Epoch 6\\Batch 2800\\ Train Loss:13.469\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6434.000001, 'FP': 2437.000001, 'TP': 565.000001}\n",
      "[2019/03/23 12:59:47] Epoch 6/ Validation Loss:13.369/ F1_score:0.113/ Precision:0.188/ Recall:0.081\n",
      "[2019/03/23 13:05:16] Epoch 7\\Batch 200\\ Train Loss:13.395\n",
      "[2019/03/23 13:09:05] Epoch 7\\Batch 400\\ Train Loss:13.386\n",
      "[2019/03/23 13:12:53] Epoch 7\\Batch 600\\ Train Loss:13.381\n",
      "[2019/03/23 13:16:35] Epoch 7\\Batch 800\\ Train Loss:13.375\n",
      "[2019/03/23 13:20:13] Epoch 7\\Batch 1000\\ Train Loss:13.373\n",
      "[2019/03/23 13:23:53] Epoch 7\\Batch 1200\\ Train Loss:13.369\n",
      "[2019/03/23 13:27:33] Epoch 7\\Batch 1400\\ Train Loss:13.366\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6418.000001, 'FP': 2421.000001, 'TP': 581.000001}\n",
      "[2019/03/23 13:28:24] Epoch 7/ Validation Loss:13.316/ F1_score:0.116/ Precision:0.194/ Recall:0.083\n",
      "[2019/03/23 13:32:05] Epoch 7\\Batch 1600\\ Train Loss:13.358\n",
      "[2019/03/23 13:35:47] Epoch 7\\Batch 1800\\ Train Loss:13.353\n",
      "[2019/03/23 13:39:28] Epoch 7\\Batch 2000\\ Train Loss:13.351\n",
      "[2019/03/23 13:43:17] Epoch 7\\Batch 2200\\ Train Loss:13.349\n",
      "[2019/03/23 13:47:07] Epoch 7\\Batch 2400\\ Train Loss:13.346\n",
      "[2019/03/23 13:50:57] Epoch 7\\Batch 2600\\ Train Loss:13.343\n",
      "[2019/03/23 13:54:45] Epoch 7\\Batch 2800\\ Train Loss:13.338\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6418.000001, 'FP': 2421.000001, 'TP': 581.000001}\n",
      "[2019/03/23 13:55:36] Epoch 7/ Validation Loss:13.277/ F1_score:0.116/ Precision:0.194/ Recall:0.083\n",
      "[2019/03/23 14:01:07] Epoch 8\\Batch 200\\ Train Loss:13.302\n",
      "[2019/03/23 14:04:56] Epoch 8\\Batch 400\\ Train Loss:13.296\n",
      "[2019/03/23 14:08:47] Epoch 8\\Batch 600\\ Train Loss:13.293\n",
      "[2019/03/23 14:12:30] Epoch 8\\Batch 800\\ Train Loss:13.289\n",
      "[2019/03/23 14:16:16] Epoch 8\\Batch 1000\\ Train Loss:13.288\n",
      "[2019/03/23 14:20:06] Epoch 8\\Batch 1200\\ Train Loss:13.286\n",
      "[2019/03/23 14:23:54] Epoch 8\\Batch 1400\\ Train Loss:13.284\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6407.000001, 'FP': 2410.000001, 'TP': 592.000001}\n",
      "[2019/03/23 14:24:49] Epoch 8/ Validation Loss:13.247/ F1_score:0.118/ Precision:0.197/ Recall:0.085\n",
      "[2019/03/23 14:28:36] Epoch 8\\Batch 1600\\ Train Loss:13.279\n",
      "[2019/03/23 14:32:16] Epoch 8\\Batch 1800\\ Train Loss:13.274\n",
      "[2019/03/23 14:35:57] Epoch 8\\Batch 2000\\ Train Loss:13.274\n",
      "[2019/03/23 14:39:46] Epoch 8\\Batch 2200\\ Train Loss:13.274\n",
      "[2019/03/23 14:43:35] Epoch 8\\Batch 2400\\ Train Loss:13.272\n",
      "[2019/03/23 14:47:24] Epoch 8\\Batch 2600\\ Train Loss:13.270\n",
      "[2019/03/23 14:51:10] Epoch 8\\Batch 2800\\ Train Loss:13.267\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6410.000001, 'FP': 2413.000001, 'TP': 589.000001}\n",
      "[2019/03/23 14:52:02] Epoch 8/ Validation Loss:13.221/ F1_score:0.118/ Precision:0.196/ Recall:0.084\n",
      "[2019/03/23 14:57:22] Epoch 9\\Batch 200\\ Train Loss:13.251\n",
      "[2019/03/23 15:01:11] Epoch 9\\Batch 400\\ Train Loss:13.247\n",
      "[2019/03/23 15:05:03] Epoch 9\\Batch 600\\ Train Loss:13.245\n",
      "[2019/03/23 15:08:58] Epoch 9\\Batch 800\\ Train Loss:13.243\n",
      "[2019/03/23 15:13:12] Epoch 9\\Batch 1000\\ Train Loss:13.243\n",
      "[2019/03/23 15:17:40] Epoch 9\\Batch 1200\\ Train Loss:13.242\n",
      "[2019/03/23 15:22:10] Epoch 9\\Batch 1400\\ Train Loss:13.241\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6403.000001, 'FP': 2406.000001, 'TP': 596.000001}\n",
      "[2019/03/23 15:23:11] Epoch 9/ Validation Loss:13.207/ F1_score:0.119/ Precision:0.199/ Recall:0.085\n",
      "[2019/03/23 15:27:27] Epoch 9\\Batch 1600\\ Train Loss:13.236\n",
      "[2019/03/23 15:31:50] Epoch 9\\Batch 1800\\ Train Loss:13.233\n",
      "[2019/03/23 15:36:15] Epoch 9\\Batch 2000\\ Train Loss:13.234\n",
      "[2019/03/23 15:40:45] Epoch 9\\Batch 2200\\ Train Loss:13.234\n",
      "[2019/03/23 15:45:14] Epoch 9\\Batch 2400\\ Train Loss:13.233\n",
      "[2019/03/23 15:49:41] Epoch 9\\Batch 2600\\ Train Loss:13.232\n",
      "[2019/03/23 15:54:06] Epoch 9\\Batch 2800\\ Train Loss:13.230\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 15:55:06] Epoch 9/ Validation Loss:13.196/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 16:01:15] Epoch 10\\Batch 200\\ Train Loss:13.226\n",
      "[2019/03/23 16:05:28] Epoch 10\\Batch 400\\ Train Loss:13.222\n",
      "[2019/03/23 16:09:52] Epoch 10\\Batch 600\\ Train Loss:13.221\n",
      "[2019/03/23 16:14:17] Epoch 10\\Batch 800\\ Train Loss:13.219\n",
      "[2019/03/23 16:18:41] Epoch 10\\Batch 1000\\ Train Loss:13.220\n",
      "[2019/03/23 16:23:01] Epoch 10\\Batch 1200\\ Train Loss:13.219\n",
      "[2019/03/23 16:27:14] Epoch 10\\Batch 1400\\ Train Loss:13.219\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6402.000001, 'FP': 2405.000001, 'TP': 597.000001}\n",
      "[2019/03/23 16:28:16] Epoch 10/ Validation Loss:13.189/ F1_score:0.119/ Precision:0.199/ Recall:0.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/23 16:32:43] Epoch 10\\Batch 1600\\ Train Loss:13.214\n",
      "[2019/03/23 16:37:06] Epoch 10\\Batch 1800\\ Train Loss:13.211\n",
      "[2019/03/23 16:41:28] Epoch 10\\Batch 2000\\ Train Loss:13.213\n",
      "[2019/03/23 16:45:19] Epoch 10\\Batch 2200\\ Train Loss:13.214\n",
      "[2019/03/23 16:49:12] Epoch 10\\Batch 2400\\ Train Loss:13.213\n",
      "[2019/03/23 16:53:07] Epoch 10\\Batch 2600\\ Train Loss:13.212\n",
      "[2019/03/23 16:57:00] Epoch 10\\Batch 2800\\ Train Loss:13.211\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6411.000001, 'FP': 2414.000001, 'TP': 588.000001}\n",
      "[2019/03/23 16:57:55] Epoch 10/ Validation Loss:13.181/ F1_score:0.118/ Precision:0.196/ Recall:0.084\n",
      "[2019/03/23 17:03:24] Epoch 11\\Batch 200\\ Train Loss:13.212\n",
      "[2019/03/23 17:07:11] Epoch 11\\Batch 400\\ Train Loss:13.209\n",
      "[2019/03/23 17:14:41] Epoch 11\\Batch 800\\ Train Loss:13.206\n",
      "[2019/03/23 17:18:25] Epoch 11\\Batch 1000\\ Train Loss:13.207\n",
      "[2019/03/23 17:22:07] Epoch 11\\Batch 1200\\ Train Loss:13.207\n",
      "[2019/03/23 17:25:46] Epoch 11\\Batch 1400\\ Train Loss:13.207\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6405.000001, 'FP': 2408.000001, 'TP': 594.000001}\n",
      "[2019/03/23 17:26:38] Epoch 11/ Validation Loss:13.179/ F1_score:0.119/ Precision:0.198/ Recall:0.085\n",
      "[2019/03/23 17:30:17] Epoch 11\\Batch 1600\\ Train Loss:13.203\n",
      "[2019/03/23 17:33:58] Epoch 11\\Batch 1800\\ Train Loss:13.200\n",
      "[2019/03/23 17:37:37] Epoch 11\\Batch 2000\\ Train Loss:13.202\n",
      "[2019/03/23 17:41:17] Epoch 11\\Batch 2200\\ Train Loss:13.203\n",
      "[2019/03/23 17:44:55] Epoch 11\\Batch 2400\\ Train Loss:13.202\n",
      "[2019/03/23 17:48:35] Epoch 11\\Batch 2600\\ Train Loss:13.202\n",
      "[2019/03/23 17:52:15] Epoch 11\\Batch 2800\\ Train Loss:13.201\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6412.000001, 'FP': 2415.000001, 'TP': 587.000001}\n",
      "[2019/03/23 17:53:07] Epoch 11/ Validation Loss:13.175/ F1_score:0.117/ Precision:0.196/ Recall:0.084\n",
      "[2019/03/23 17:58:26] Epoch 12\\Batch 200\\ Train Loss:13.205\n",
      "[2019/03/23 18:02:18] Epoch 12\\Batch 400\\ Train Loss:13.202\n",
      "[2019/03/23 18:06:10] Epoch 12\\Batch 600\\ Train Loss:13.201\n",
      "[2019/03/23 18:10:05] Epoch 12\\Batch 800\\ Train Loss:13.199\n",
      "[2019/03/23 18:14:00] Epoch 12\\Batch 1000\\ Train Loss:13.201\n",
      "[2019/03/23 18:17:51] Epoch 12\\Batch 1200\\ Train Loss:13.200\n",
      "[2019/03/23 18:21:31] Epoch 12\\Batch 1400\\ Train Loss:13.201\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6406.000001, 'FP': 2409.000001, 'TP': 593.000001}\n",
      "[2019/03/23 18:22:23] Epoch 12/ Validation Loss:13.174/ F1_score:0.119/ Precision:0.198/ Recall:0.085\n",
      "[2019/03/23 18:26:12] Epoch 12\\Batch 1600\\ Train Loss:13.197\n",
      "[2019/03/23 18:30:02] Epoch 12\\Batch 1800\\ Train Loss:13.194\n",
      "[2019/03/23 18:33:51] Epoch 12\\Batch 2000\\ Train Loss:13.196\n",
      "[2019/03/23 18:37:39] Epoch 12\\Batch 2200\\ Train Loss:13.197\n",
      "[2019/03/23 18:41:27] Epoch 12\\Batch 2400\\ Train Loss:13.197\n",
      "[2019/03/23 18:45:16] Epoch 12\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/23 18:49:03] Epoch 12\\Batch 2800\\ Train Loss:13.195\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6411.000001, 'FP': 2414.000001, 'TP': 588.000001}\n",
      "[2019/03/23 18:49:57] Epoch 12/ Validation Loss:13.172/ F1_score:0.118/ Precision:0.196/ Recall:0.084\n",
      "[2019/03/23 18:55:16] Epoch 13\\Batch 200\\ Train Loss:13.202\n",
      "[2019/03/23 18:58:56] Epoch 13\\Batch 400\\ Train Loss:13.198\n",
      "[2019/03/23 19:02:36] Epoch 13\\Batch 600\\ Train Loss:13.198\n",
      "[2019/03/23 19:06:16] Epoch 13\\Batch 800\\ Train Loss:13.196\n",
      "[2019/03/23 19:09:56] Epoch 13\\Batch 1000\\ Train Loss:13.197\n",
      "[2019/03/23 19:13:36] Epoch 13\\Batch 1200\\ Train Loss:13.197\n",
      "[2019/03/23 19:17:16] Epoch 13\\Batch 1400\\ Train Loss:13.197\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6407.000001, 'FP': 2410.000001, 'TP': 592.000001}\n",
      "[2019/03/23 19:18:07] Epoch 13/ Validation Loss:13.171/ F1_score:0.118/ Precision:0.197/ Recall:0.085\n",
      "[2019/03/23 19:21:48] Epoch 13\\Batch 1600\\ Train Loss:13.193\n",
      "[2019/03/23 19:25:27] Epoch 13\\Batch 1800\\ Train Loss:13.191\n",
      "[2019/03/23 19:29:15] Epoch 13\\Batch 2000\\ Train Loss:13.192\n",
      "[2019/03/23 19:33:04] Epoch 13\\Batch 2200\\ Train Loss:13.194\n",
      "[2019/03/23 19:36:53] Epoch 13\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/23 19:40:40] Epoch 13\\Batch 2600\\ Train Loss:13.193\n",
      "[2019/03/23 19:44:20] Epoch 13\\Batch 2800\\ Train Loss:13.192\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6409.000001, 'FP': 2412.000001, 'TP': 590.000001}\n",
      "[2019/03/23 19:45:12] Epoch 13/ Validation Loss:13.170/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 19:50:42] Epoch 14\\Batch 200\\ Train Loss:13.200\n",
      "[2019/03/23 19:54:31] Epoch 14\\Batch 400\\ Train Loss:13.196\n",
      "[2019/03/23 19:58:21] Epoch 14\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/23 20:02:09] Epoch 14\\Batch 800\\ Train Loss:13.194\n",
      "[2019/03/23 20:06:01] Epoch 14\\Batch 1000\\ Train Loss:13.195\n",
      "[2019/03/23 20:09:57] Epoch 14\\Batch 1200\\ Train Loss:13.195\n",
      "[2019/03/23 20:13:53] Epoch 14\\Batch 1400\\ Train Loss:13.195\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6405.000001, 'FP': 2408.000001, 'TP': 594.000001}\n",
      "[2019/03/23 20:14:47] Epoch 14/ Validation Loss:13.169/ F1_score:0.119/ Precision:0.198/ Recall:0.085\n",
      "[2019/03/23 20:18:41] Epoch 14\\Batch 1600\\ Train Loss:13.192\n",
      "[2019/03/23 20:22:31] Epoch 14\\Batch 1800\\ Train Loss:13.189\n",
      "[2019/03/23 20:26:20] Epoch 14\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/23 20:30:09] Epoch 14\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/23 20:34:07] Epoch 14\\Batch 2400\\ Train Loss:13.192\n",
      "[2019/03/23 20:38:03] Epoch 14\\Batch 2600\\ Train Loss:13.192\n",
      "[2019/03/23 20:41:58] Epoch 14\\Batch 2800\\ Train Loss:13.191\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6407.000001, 'FP': 2410.000001, 'TP': 592.000001}\n",
      "[2019/03/23 20:42:53] Epoch 14/ Validation Loss:13.169/ F1_score:0.118/ Precision:0.197/ Recall:0.085\n",
      "[2019/03/23 20:48:35] Epoch 15\\Batch 200\\ Train Loss:13.199\n",
      "[2019/03/23 20:52:28] Epoch 15\\Batch 400\\ Train Loss:13.195\n",
      "[2019/03/23 20:56:22] Epoch 15\\Batch 600\\ Train Loss:13.195\n",
      "[2019/03/23 21:00:17] Epoch 15\\Batch 800\\ Train Loss:13.193\n",
      "[2019/03/23 21:04:11] Epoch 15\\Batch 1000\\ Train Loss:13.194\n",
      "[2019/03/23 21:08:04] Epoch 15\\Batch 1200\\ Train Loss:13.194\n",
      "[2019/03/23 21:11:53] Epoch 15\\Batch 1400\\ Train Loss:13.195\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 21:12:46] Epoch 15/ Validation Loss:13.169/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 21:16:38] Epoch 15\\Batch 1600\\ Train Loss:13.191\n",
      "[2019/03/23 21:20:27] Epoch 15\\Batch 1800\\ Train Loss:13.188\n",
      "[2019/03/23 21:24:17] Epoch 15\\Batch 2000\\ Train Loss:13.190\n",
      "[2019/03/23 21:28:03] Epoch 15\\Batch 2200\\ Train Loss:13.192\n",
      "[2019/03/23 21:31:53] Epoch 15\\Batch 2400\\ Train Loss:13.191\n",
      "[2019/03/23 21:35:40] Epoch 15\\Batch 2600\\ Train Loss:13.191\n",
      "[2019/03/23 21:39:30] Epoch 15\\Batch 2800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 21:40:23] Epoch 15/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 21:45:50] Epoch 16\\Batch 200\\ Train Loss:13.198\n",
      "[2019/03/23 21:49:41] Epoch 16\\Batch 400\\ Train Loss:13.195\n",
      "[2019/03/23 21:53:30] Epoch 16\\Batch 600\\ Train Loss:13.194\n",
      "[2019/03/23 21:57:17] Epoch 16\\Batch 800\\ Train Loss:13.193\n",
      "[2019/03/23 22:01:01] Epoch 16\\Batch 1000\\ Train Loss:13.194\n",
      "[2019/03/23 22:04:50] Epoch 16\\Batch 1200\\ Train Loss:13.194\n",
      "[2019/03/23 22:08:39] Epoch 16\\Batch 1400\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 22:09:33] Epoch 16/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 22:13:24] Epoch 16\\Batch 1600\\ Train Loss:13.190\n",
      "[2019/03/23 22:17:15] Epoch 16\\Batch 1800\\ Train Loss:13.188\n",
      "[2019/03/23 22:21:05] Epoch 16\\Batch 2000\\ Train Loss:13.190\n",
      "[2019/03/23 22:24:55] Epoch 16\\Batch 2200\\ Train Loss:13.191\n",
      "[2019/03/23 22:28:45] Epoch 16\\Batch 2400\\ Train Loss:13.191\n",
      "[2019/03/23 22:32:35] Epoch 16\\Batch 2600\\ Train Loss:13.191\n",
      "[2019/03/23 22:36:24] Epoch 16\\Batch 2800\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 22:37:17] Epoch 16/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 22:42:47] Epoch 17\\Batch 200\\ Train Loss:13.198\n",
      "[2019/03/23 22:46:26] Epoch 17\\Batch 400\\ Train Loss:13.194\n",
      "[2019/03/23 22:50:06] Epoch 17\\Batch 600\\ Train Loss:13.194\n",
      "[2019/03/23 22:53:45] Epoch 17\\Batch 800\\ Train Loss:13.192\n",
      "[2019/03/23 22:57:25] Epoch 17\\Batch 1000\\ Train Loss:13.194\n",
      "[2019/03/23 23:01:05] Epoch 17\\Batch 1200\\ Train Loss:13.194\n",
      "[2019/03/23 23:04:45] Epoch 17\\Batch 1400\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 23:05:36] Epoch 17/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 23:09:17] Epoch 17\\Batch 1600\\ Train Loss:13.190\n",
      "[2019/03/23 23:12:57] Epoch 17\\Batch 1800\\ Train Loss:13.188\n",
      "[2019/03/23 23:16:35] Epoch 17\\Batch 2000\\ Train Loss:13.189\n",
      "[2019/03/23 23:20:16] Epoch 17\\Batch 2200\\ Train Loss:13.191\n",
      "[2019/03/23 23:23:57] Epoch 17\\Batch 2400\\ Train Loss:13.191\n",
      "[2019/03/23 23:27:37] Epoch 17\\Batch 2600\\ Train Loss:13.190\n",
      "[2019/03/23 23:31:17] Epoch 17\\Batch 2800\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/23 23:32:09] Epoch 17/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/23 23:37:27] Epoch 18\\Batch 200\\ Train Loss:13.198\n",
      "[2019/03/23 23:41:07] Epoch 18\\Batch 400\\ Train Loss:13.194\n",
      "[2019/03/23 23:44:46] Epoch 18\\Batch 600\\ Train Loss:13.194\n",
      "[2019/03/23 23:48:23] Epoch 18\\Batch 800\\ Train Loss:13.192\n",
      "[2019/03/23 23:52:03] Epoch 18\\Batch 1000\\ Train Loss:13.194\n",
      "[2019/03/23 23:55:43] Epoch 18\\Batch 1200\\ Train Loss:13.193\n",
      "[2019/03/23 23:59:21] Epoch 18\\Batch 1400\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 00:00:13] Epoch 18/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 00:03:52] Epoch 18\\Batch 1600\\ Train Loss:13.190\n",
      "[2019/03/24 00:07:33] Epoch 18\\Batch 1800\\ Train Loss:13.188\n",
      "[2019/03/24 00:11:13] Epoch 18\\Batch 2000\\ Train Loss:13.189\n",
      "[2019/03/24 00:14:54] Epoch 18\\Batch 2200\\ Train Loss:13.191\n",
      "[2019/03/24 00:18:34] Epoch 18\\Batch 2400\\ Train Loss:13.190\n",
      "[2019/03/24 00:22:13] Epoch 18\\Batch 2600\\ Train Loss:13.190\n",
      "[2019/03/24 00:25:52] Epoch 18\\Batch 2800\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 00:26:43] Epoch 18/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 00:32:02] Epoch 19\\Batch 200\\ Train Loss:13.197\n",
      "[2019/03/24 00:35:41] Epoch 19\\Batch 400\\ Train Loss:13.194\n",
      "[2019/03/24 00:39:21] Epoch 19\\Batch 600\\ Train Loss:13.194\n",
      "[2019/03/24 00:43:00] Epoch 19\\Batch 800\\ Train Loss:13.192\n",
      "[2019/03/24 00:46:40] Epoch 19\\Batch 1000\\ Train Loss:13.193\n",
      "[2019/03/24 00:50:20] Epoch 19\\Batch 1200\\ Train Loss:13.193\n",
      "[2019/03/24 00:53:58] Epoch 19\\Batch 1400\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 00:54:50] Epoch 19/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 00:58:31] Epoch 19\\Batch 1600\\ Train Loss:13.190\n",
      "[2019/03/24 01:02:11] Epoch 19\\Batch 1800\\ Train Loss:13.187\n",
      "[2019/03/24 01:05:51] Epoch 19\\Batch 2000\\ Train Loss:13.189\n",
      "[2019/03/24 01:09:32] Epoch 19\\Batch 2200\\ Train Loss:13.191\n",
      "[2019/03/24 01:13:11] Epoch 19\\Batch 2400\\ Train Loss:13.190\n",
      "[2019/03/24 01:16:50] Epoch 19\\Batch 2600\\ Train Loss:13.190\n",
      "[2019/03/24 01:20:31] Epoch 19\\Batch 2800\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 01:21:22] Epoch 19/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 01:26:39] Epoch 20\\Batch 200\\ Train Loss:13.197\n",
      "[2019/03/24 01:30:18] Epoch 20\\Batch 400\\ Train Loss:13.194\n",
      "[2019/03/24 01:33:58] Epoch 20\\Batch 600\\ Train Loss:13.194\n",
      "[2019/03/24 01:37:36] Epoch 20\\Batch 800\\ Train Loss:13.192\n",
      "[2019/03/24 01:41:17] Epoch 20\\Batch 1000\\ Train Loss:13.193\n",
      "[2019/03/24 01:44:56] Epoch 20\\Batch 1200\\ Train Loss:13.193\n",
      "[2019/03/24 01:48:35] Epoch 20\\Batch 1400\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 01:49:26] Epoch 20/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 01:53:05] Epoch 20\\Batch 1600\\ Train Loss:13.190\n",
      "[2019/03/24 01:56:45] Epoch 20\\Batch 1800\\ Train Loss:13.187\n",
      "[2019/03/24 01:59:48] Epoch 20\\Batch 2000\\ Train Loss:13.189\n",
      "[2019/03/24 02:02:35] Epoch 20\\Batch 2200\\ Train Loss:13.191\n",
      "[2019/03/24 02:05:23] Epoch 20\\Batch 2400\\ Train Loss:13.190\n",
      "[2019/03/24 02:08:11] Epoch 20\\Batch 2600\\ Train Loss:13.190\n",
      "[2019/03/24 02:10:59] Epoch 20\\Batch 2800\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 02:11:39] Epoch 20/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 02:15:43] Epoch 21\\Batch 200\\ Train Loss:13.197\n",
      "[2019/03/24 02:18:29] Epoch 21\\Batch 400\\ Train Loss:13.194\n",
      "[2019/03/24 02:21:17] Epoch 21\\Batch 600\\ Train Loss:13.194\n",
      "[2019/03/24 02:24:04] Epoch 21\\Batch 800\\ Train Loss:13.192\n",
      "[2019/03/24 02:26:51] Epoch 21\\Batch 1000\\ Train Loss:13.193\n",
      "[2019/03/24 02:29:22] Epoch 21\\Batch 1200\\ Train Loss:13.193\n",
      "[2019/03/24 02:31:37] Epoch 21\\Batch 1400\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 02:32:24] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 02:34:39] Epoch 21\\Batch 1600\\ Train Loss:13.190\n",
      "[2019/03/24 02:36:53] Epoch 21\\Batch 1800\\ Train Loss:13.187\n",
      "[2019/03/24 02:39:07] Epoch 21\\Batch 2000\\ Train Loss:13.189\n",
      "[2019/03/24 02:41:21] Epoch 21\\Batch 2200\\ Train Loss:13.191\n",
      "[2019/03/24 02:43:35] Epoch 21\\Batch 2400\\ Train Loss:13.190\n",
      "[2019/03/24 02:45:50] Epoch 21\\Batch 2600\\ Train Loss:13.190\n",
      "[2019/03/24 02:48:05] Epoch 21\\Batch 2800\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'FN': 6408.000001, 'FP': 2411.000001, 'TP': 591.000001}\n",
      "[2019/03/24 02:48:52] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X[0:5]: [[ 832   60  256 1172 3407  516   96  138  103 1108   16    3   96  177\n",
      "    22   11  672   53   18 1560 1560   15   65   12  180   10  342  173\n",
      "    13  103  141  707  191   12  342  173   15   13   22   11  229  264\n",
      "   163 1362  135 1249  156  156  731  115   84   10  808 1713  103  141\n",
      "   229  264  788  421  103  141   12   95  316   10  808 1713  103  141\n",
      "    12 2413 1227   15 1397  997   22  116  301  489   12   18  858   99\n",
      "   596   98   26  646  813   10  386 1093  197  767   22   11 1179 1849\n",
      "   593   84   22   11   94  102  322   60  190  220  583  355   10  153\n",
      "   103  141  192  153   12   10 1244  466  116  103  141  189   58  130\n",
      "    95  316   12  788  421  699   16    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 270  154  166  504   53  154   89   60   13  210  259  589   12  151\n",
      "    18 1682   10  154  150  113   97   26  145   67   33   16    3   61\n",
      "    18  118  151   18  948   10  151   18  550   80  123  159   93  154\n",
      "    12  150  186  104  174  537   10  116   66   41   12  117  195   10\n",
      "   104   66   41  116  154   12  117  195   16   95  316  104  590  605\n",
      "   610   19   59   19  202  502  214  516  398   16    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 186  163  284  109  108   32  437   41  232  555  121  177   47   19\n",
      "    67   33  188  309  576  948   12  670  775   16    3    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  32  143  194  453  537  903  450  608   95   10  332   15   47   97\n",
      "   892   75  231  143  238  450   10   97  444  444  314  278   12  166\n",
      "   410   16    3  177  257  338  159   32  143  194   12   18  207   56\n",
      "    75  453  537  903   75  450  608   95   10   75  389  113   12  629\n",
      "    56  342  173   99   13  157  255  629  361   10 1238  268   13  537\n",
      "   903 1950   75   10 2004 1790   53   87   92  188  309  281   75  381\n",
      "  1950   75   12  701  841   10 1917  602  703  763 2205 2574  815  909\n",
      "   909  133   84   26   22   11  177  257  103  301 2739 2162   10   32\n",
      "   143  285  397  593  576  608   54   18  208 1078   10  880 1828  454\n",
      "  1083   10  202  502   32  143  194   75  124  451  173 1426  817   22\n",
      "    11  746  194  399  609   71   13  121  177   10  444  264  424  399\n",
      "  2436  553  241  632  629 1302  847  255   10  125  542  158  724  724\n",
      "   593  650   78  236   10  502  159  168   13  650   53   26   75  389\n",
      "    12  399  609   78  236   10  200   93  612  593  972  807  628  305\n",
      "   461   10 2060 2372]\n",
      " [ 224  461 3115  912  560  833  224  333  224 1499  359  106   83   52\n",
      "    36   30   35   21  253  372  414   96  138   16    3  434  135   13\n",
      "   300  352 1078  351  614  345  189  129   18   62  295   10  230  160\n",
      "   108  437  774 1292  372   18  118  347   22  224  461  130 3115  912\n",
      "   130  560  833  130  224  333  130  224 1499  359  106   83   52   36\n",
      "    30   35   21  253  372  414   96  138   16    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "train_Y[0:5]: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "train_y_short: [1473]\n",
      "[2019/03/24 08:27:17] restore from checkpoint\n",
      "INFO:tensorflow:Restoring parameters from testrnn_multilabel_checkpoint/model.ckpt-21\n",
      "[2019/03/24 08:27:17] continue training...\n",
      "[2019/03/24 08:28:05] Epoch 21\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 08:28:51] Epoch 21\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 08:29:38] Epoch 21\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 08:30:25] Epoch 21\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 08:31:12] Epoch 21\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 08:31:58] Epoch 21\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 08:32:45] Epoch 21\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 08:33:33] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 08:34:20] Epoch 21\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 08:35:06] Epoch 21\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 08:35:52] Epoch 21\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 08:36:39] Epoch 21\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 08:37:25] Epoch 21\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 08:38:11] Epoch 21\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 08:38:58] Epoch 21\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 08:39:44] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 08:40:30] Epoch 21\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 08:41:16] Epoch 21\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 08:42:03] Epoch 21\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 08:42:49] Epoch 21\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 08:43:35] Epoch 21\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 08:44:22] Epoch 21\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 08:45:08] Epoch 21\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 08:45:55] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 08:46:41] Epoch 21\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 08:47:28] Epoch 21\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 08:48:15] Epoch 21\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 08:49:01] Epoch 21\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 08:49:48] Epoch 21\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 08:50:37] Epoch 21\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 08:51:25] Epoch 21\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 08:52:04] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 08:52:51] Epoch 21\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 08:53:39] Epoch 21\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 08:54:27] Epoch 21\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 08:55:14] Epoch 21\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 08:56:01] Epoch 21\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 08:56:46] Epoch 21\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 08:57:34] Epoch 21\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 08:58:12] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 08:59:00] Epoch 21\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 08:59:47] Epoch 21\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 09:00:32] Epoch 21\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 09:01:20] Epoch 21\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 09:02:08] Epoch 21\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 09:02:56] Epoch 21\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 09:03:43] Epoch 21\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:04:23] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:05:11] Epoch 21\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 09:05:58] Epoch 21\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 09:06:46] Epoch 21\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 09:07:34] Epoch 21\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 09:08:19] Epoch 21\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 09:09:06] Epoch 21\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 09:09:53] Epoch 21\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:10:31] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:11:19] Epoch 21\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 09:12:05] Epoch 21\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 09:12:51] Epoch 21\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 09:13:39] Epoch 21\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 09:14:27] Epoch 21\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 09:15:14] Epoch 21\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 09:16:01] Epoch 21\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:16:40] Epoch 21/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:17:28] Epoch 21\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 09:18:55] Epoch 22\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 09:19:42] Epoch 22\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 09:20:27] Epoch 22\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 09:21:14] Epoch 22\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 09:22:01] Epoch 22\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 09:22:49] Epoch 22\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 09:23:36] Epoch 22\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:24:15] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:25:03] Epoch 22\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 09:25:50] Epoch 22\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 09:26:38] Epoch 22\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 09:27:25] Epoch 22\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 09:28:11] Epoch 22\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 09:28:57] Epoch 22\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 09:29:44] Epoch 22\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:30:22] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:31:09] Epoch 22\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 09:31:56] Epoch 22\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 09:32:42] Epoch 22\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 09:33:29] Epoch 22\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 09:34:16] Epoch 22\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 09:35:04] Epoch 22\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 09:35:51] Epoch 22\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:36:31] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:37:19] Epoch 22\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 09:38:07] Epoch 22\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 09:38:54] Epoch 22\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 09:39:42] Epoch 22\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 09:40:27] Epoch 22\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 09:41:14] Epoch 22\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 09:42:01] Epoch 22\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:42:39] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:43:27] Epoch 22\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 09:44:12] Epoch 22\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 09:44:59] Epoch 22\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 09:45:46] Epoch 22\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 09:46:34] Epoch 22\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 09:47:21] Epoch 22\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 09:48:07] Epoch 22\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:48:46] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:49:34] Epoch 22\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 09:50:21] Epoch 22\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 09:51:08] Epoch 22\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 09:51:55] Epoch 22\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 09:52:41] Epoch 22\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 09:53:28] Epoch 22\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 09:54:16] Epoch 22\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 09:54:54] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 09:55:42] Epoch 22\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 09:56:29] Epoch 22\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 09:57:17] Epoch 22\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 09:58:02] Epoch 22\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 09:58:49] Epoch 22\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 09:59:37] Epoch 22\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 10:00:24] Epoch 22\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:01:02] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:01:48] Epoch 22\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 10:02:34] Epoch 22\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 10:03:22] Epoch 22\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 10:04:09] Epoch 22\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 10:04:57] Epoch 22\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 10:05:43] Epoch 22\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 10:06:30] Epoch 22\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:07:07] Epoch 22/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:07:55] Epoch 22\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 10:09:21] Epoch 23\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 10:10:06] Epoch 23\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 10:10:53] Epoch 23\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 10:11:41] Epoch 23\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 10:12:29] Epoch 23\\Batch 1000\\ Train Loss:13.189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 10:13:17] Epoch 23\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 10:14:03] Epoch 23\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:14:40] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:15:28] Epoch 23\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 10:16:15] Epoch 23\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 10:17:02] Epoch 23\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 10:17:48] Epoch 23\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 10:18:34] Epoch 23\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 10:19:22] Epoch 23\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 10:20:10] Epoch 23\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:20:47] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:21:34] Epoch 23\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 10:22:20] Epoch 23\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 10:23:08] Epoch 23\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 10:23:55] Epoch 23\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 10:24:42] Epoch 23\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 10:25:30] Epoch 23\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 10:26:15] Epoch 23\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:26:54] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:27:41] Epoch 23\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 10:28:28] Epoch 23\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 10:29:16] Epoch 23\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 10:30:01] Epoch 23\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 10:30:48] Epoch 23\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 10:31:35] Epoch 23\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 10:32:23] Epoch 23\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:33:01] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:33:47] Epoch 23\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 10:34:33] Epoch 23\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 10:35:21] Epoch 23\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 10:36:08] Epoch 23\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 10:36:55] Epoch 23\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 10:37:41] Epoch 23\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 10:38:27] Epoch 23\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:39:05] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:39:53] Epoch 23\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 10:40:41] Epoch 23\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 10:41:28] Epoch 23\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 10:42:14] Epoch 23\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 10:43:01] Epoch 23\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 10:43:48] Epoch 23\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 10:44:36] Epoch 23\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:45:13] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:45:59] Epoch 23\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 10:46:46] Epoch 23\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 10:47:34] Epoch 23\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 10:48:22] Epoch 23\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 10:49:09] Epoch 23\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 10:49:55] Epoch 23\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 10:50:42] Epoch 23\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:51:20] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:52:07] Epoch 23\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 10:52:55] Epoch 23\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 10:53:40] Epoch 23\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 10:54:27] Epoch 23\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 10:55:14] Epoch 23\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 10:56:02] Epoch 23\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 10:56:50] Epoch 23\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 10:57:28] Epoch 23/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 10:58:14] Epoch 23\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 10:59:40] Epoch 24\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 11:00:27] Epoch 24\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 11:01:15] Epoch 24\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 11:02:02] Epoch 24\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 11:02:49] Epoch 24\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 11:03:34] Epoch 24\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 11:04:21] Epoch 24\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:04:59] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:05:46] Epoch 24\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 11:06:34] Epoch 24\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 11:07:20] Epoch 24\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 11:08:06] Epoch 24\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 11:08:53] Epoch 24\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 11:09:40] Epoch 24\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 11:10:27] Epoch 24\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:11:06] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:11:52] Epoch 24\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 11:12:39] Epoch 24\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 11:13:26] Epoch 24\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 11:14:14] Epoch 24\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 11:15:01] Epoch 24\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 11:15:46] Epoch 24\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 11:16:34] Epoch 24\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:17:11] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:17:59] Epoch 24\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 11:18:47] Epoch 24\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 11:19:32] Epoch 24\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 11:20:19] Epoch 24\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 11:21:05] Epoch 24\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 11:21:53] Epoch 24\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 11:22:40] Epoch 24\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:23:20] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:24:07] Epoch 24\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 11:24:54] Epoch 24\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 11:25:41] Epoch 24\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 11:26:29] Epoch 24\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 11:27:15] Epoch 24\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 11:28:01] Epoch 24\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 11:28:49] Epoch 24\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:29:26] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 11:30:14] Epoch 24\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 11:31:01] Epoch 24\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 11:31:47] Epoch 24\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 11:32:34] Epoch 24\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 11:33:21] Epoch 24\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 11:34:09] Epoch 24\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 11:34:56] Epoch 24\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:35:36] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:36:24] Epoch 24\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 11:37:11] Epoch 24\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 11:37:58] Epoch 24\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 11:38:46] Epoch 24\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 11:39:31] Epoch 24\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 11:40:18] Epoch 24\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 11:41:05] Epoch 24\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:41:43] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:42:31] Epoch 24\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 11:43:16] Epoch 24\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 11:44:03] Epoch 24\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 11:44:51] Epoch 24\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 11:45:38] Epoch 24\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 11:46:26] Epoch 24\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 11:47:12] Epoch 24\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:47:50] Epoch 24/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:48:38] Epoch 24\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 11:50:04] Epoch 25\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 11:50:51] Epoch 25\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 11:51:36] Epoch 25\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 11:52:23] Epoch 25\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 11:53:11] Epoch 25\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 11:53:58] Epoch 25\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 11:54:45] Epoch 25\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 11:55:25] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 11:56:13] Epoch 25\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 11:57:01] Epoch 25\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 11:57:49] Epoch 25\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 11:58:36] Epoch 25\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 11:59:21] Epoch 25\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 12:00:08] Epoch 25\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 12:00:56] Epoch 25\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:01:34] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:02:22] Epoch 25\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 12:03:08] Epoch 25\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 12:03:54] Epoch 25\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 12:04:41] Epoch 25\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 12:05:29] Epoch 25\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 12:06:16] Epoch 25\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 12:07:04] Epoch 25\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:07:41] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:08:29] Epoch 25\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 12:09:14] Epoch 25\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 12:10:01] Epoch 25\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 12:10:48] Epoch 25\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 12:11:36] Epoch 25\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 12:12:23] Epoch 25\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 12:13:08] Epoch 25\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:13:46] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:14:34] Epoch 25\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 12:15:21] Epoch 25\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 12:16:08] Epoch 25\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 12:16:54] Epoch 25\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 12:17:40] Epoch 25\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 12:18:27] Epoch 25\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 12:19:14] Epoch 25\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:19:52] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:20:39] Epoch 25\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 12:21:25] Epoch 25\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 12:22:12] Epoch 25\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 12:23:00] Epoch 25\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 12:23:48] Epoch 25\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 12:24:35] Epoch 25\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 12:25:20] Epoch 25\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:25:58] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:26:45] Epoch 25\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 12:27:33] Epoch 25\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 12:28:19] Epoch 25\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 12:29:05] Epoch 25\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 12:29:52] Epoch 25\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 12:30:40] Epoch 25\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 12:31:27] Epoch 25\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:32:05] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:32:51] Epoch 25\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 12:33:38] Epoch 25\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 12:34:26] Epoch 25\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 12:35:13] Epoch 25\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 12:36:01] Epoch 25\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 12:36:47] Epoch 25\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 12:37:33] Epoch 25\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:38:11] Epoch 25/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:38:59] Epoch 25\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 12:40:24] Epoch 26\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 12:41:09] Epoch 26\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 12:41:57] Epoch 26\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 12:42:44] Epoch 26\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 12:43:32] Epoch 26\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 12:44:18] Epoch 26\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 12:45:03] Epoch 26\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:45:41] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:46:29] Epoch 26\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 12:47:16] Epoch 26\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 12:48:03] Epoch 26\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 12:48:49] Epoch 26\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 12:49:36] Epoch 26\\Batch 2400\\ Train Loss:13.194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 12:50:23] Epoch 26\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 12:51:11] Epoch 26\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:51:48] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:52:35] Epoch 26\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 12:53:21] Epoch 26\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 12:54:08] Epoch 26\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 12:54:56] Epoch 26\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 12:55:43] Epoch 26\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 12:56:30] Epoch 26\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 12:57:16] Epoch 26\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 12:57:54] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 12:58:41] Epoch 26\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 12:59:29] Epoch 26\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 13:00:16] Epoch 26\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 13:01:01] Epoch 26\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 13:01:48] Epoch 26\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 13:02:35] Epoch 26\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 13:03:22] Epoch 26\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:04:00] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:04:46] Epoch 26\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 13:05:32] Epoch 26\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 13:06:20] Epoch 26\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 13:07:07] Epoch 26\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 13:07:55] Epoch 26\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 13:08:41] Epoch 26\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 13:09:27] Epoch 26\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:10:05] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:10:52] Epoch 26\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 13:11:40] Epoch 26\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 13:12:28] Epoch 26\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 13:13:15] Epoch 26\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 13:14:03] Epoch 26\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 13:14:48] Epoch 26\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 13:15:35] Epoch 26\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:16:13] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:17:01] Epoch 26\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 13:17:49] Epoch 26\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 13:18:34] Epoch 26\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 13:19:21] Epoch 26\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 13:20:08] Epoch 26\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 13:20:55] Epoch 26\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 13:21:43] Epoch 26\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:22:22] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:23:08] Epoch 26\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 13:23:55] Epoch 26\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 13:24:43] Epoch 26\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 13:25:30] Epoch 26\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 13:26:17] Epoch 26\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 13:27:02] Epoch 26\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 13:27:50] Epoch 26\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:28:27] Epoch 26/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:29:15] Epoch 26\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 13:30:38] Epoch 27\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 13:31:25] Epoch 27\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 13:32:13] Epoch 27\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 13:33:00] Epoch 27\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 13:33:47] Epoch 27\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 13:34:33] Epoch 27\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 13:35:20] Epoch 27\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:35:58] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:36:45] Epoch 27\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 13:37:32] Epoch 27\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 13:38:18] Epoch 27\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 13:39:04] Epoch 27\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 13:39:51] Epoch 27\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 13:40:38] Epoch 27\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 13:41:26] Epoch 27\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:42:04] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:42:49] Epoch 27\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 13:43:36] Epoch 27\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 13:44:24] Epoch 27\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 13:45:11] Epoch 27\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 13:45:58] Epoch 27\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 13:46:43] Epoch 27\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 13:47:31] Epoch 27\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:48:08] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:48:56] Epoch 27\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 13:49:43] Epoch 27\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 13:50:29] Epoch 27\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 13:51:16] Epoch 27\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 13:52:03] Epoch 27\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 13:52:51] Epoch 27\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 13:53:38] Epoch 27\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 13:54:17] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 13:55:04] Epoch 27\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 13:55:51] Epoch 27\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 13:56:39] Epoch 27\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 13:57:26] Epoch 27\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 13:58:12] Epoch 27\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 13:58:58] Epoch 27\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 13:59:46] Epoch 27\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:00:23] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:01:11] Epoch 27\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 14:01:59] Epoch 27\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 14:02:43] Epoch 27\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 14:03:31] Epoch 27\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 14:04:18] Epoch 27\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 14:05:05] Epoch 27\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 14:05:52] Epoch 27\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:06:33] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 14:07:19] Epoch 27\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 14:08:07] Epoch 27\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 14:08:55] Epoch 27\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 14:09:41] Epoch 27\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 14:10:27] Epoch 27\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 14:11:13] Epoch 27\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 14:12:00] Epoch 27\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:12:38] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:13:26] Epoch 27\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 14:14:13] Epoch 27\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 14:14:58] Epoch 27\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 14:15:45] Epoch 27\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 14:16:33] Epoch 27\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 14:17:21] Epoch 27\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 14:18:08] Epoch 27\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:18:45] Epoch 27/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:19:32] Epoch 27\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 14:20:57] Epoch 28\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 14:21:45] Epoch 28\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 14:22:31] Epoch 28\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 14:23:19] Epoch 28\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 14:24:06] Epoch 28\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 14:24:51] Epoch 28\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 14:25:38] Epoch 28\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:26:15] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:27:03] Epoch 28\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 14:27:50] Epoch 28\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 14:28:36] Epoch 28\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 14:29:22] Epoch 28\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 14:30:09] Epoch 28\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 14:30:56] Epoch 28\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 14:31:44] Epoch 28\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:32:22] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:33:08] Epoch 28\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 14:33:55] Epoch 28\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 14:34:42] Epoch 28\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 14:35:29] Epoch 28\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 14:36:16] Epoch 28\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 14:37:01] Epoch 28\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 14:37:49] Epoch 28\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:38:26] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:39:14] Epoch 28\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 14:40:01] Epoch 28\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 14:40:47] Epoch 28\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 14:41:34] Epoch 28\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 14:42:21] Epoch 28\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 14:43:08] Epoch 28\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 14:43:55] Epoch 28\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:44:33] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:45:20] Epoch 28\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 14:46:07] Epoch 28\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 14:46:54] Epoch 28\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 14:47:42] Epoch 28\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 14:48:29] Epoch 28\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 14:49:15] Epoch 28\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 14:50:02] Epoch 28\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:50:39] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:51:27] Epoch 28\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 14:52:14] Epoch 28\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 14:52:59] Epoch 28\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 14:53:46] Epoch 28\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 14:54:33] Epoch 28\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 14:55:21] Epoch 28\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 14:56:08] Epoch 28\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 14:56:48] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 14:57:35] Epoch 28\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 14:58:23] Epoch 28\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 14:59:10] Epoch 28\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 14:59:58] Epoch 28\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 15:00:43] Epoch 28\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 15:01:30] Epoch 28\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 15:02:17] Epoch 28\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:02:55] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:03:42] Epoch 28\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 15:04:29] Epoch 28\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 15:05:15] Epoch 28\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 15:06:02] Epoch 28\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 15:06:49] Epoch 28\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 15:07:37] Epoch 28\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 15:08:24] Epoch 28\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:09:04] Epoch 28/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:09:51] Epoch 28\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 15:11:17] Epoch 29\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 15:12:04] Epoch 29\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 15:12:49] Epoch 29\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 15:13:35] Epoch 29\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 15:14:23] Epoch 29\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 15:15:11] Epoch 29\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 15:15:58] Epoch 29\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:16:37] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:17:24] Epoch 29\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 15:18:11] Epoch 29\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 15:18:58] Epoch 29\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 15:19:45] Epoch 29\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 15:20:32] Epoch 29\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 15:21:18] Epoch 29\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 15:22:05] Epoch 29\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:22:43] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:23:31] Epoch 29\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 15:24:17] Epoch 29\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 15:25:03] Epoch 29\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 15:25:50] Epoch 29\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 15:26:38] Epoch 29\\Batch 3800\\ Train Loss:13.194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 15:27:25] Epoch 29\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 15:28:12] Epoch 29\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:28:50] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:29:38] Epoch 29\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 15:30:24] Epoch 29\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 15:31:10] Epoch 29\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 15:31:58] Epoch 29\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 15:32:45] Epoch 29\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 15:33:33] Epoch 29\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 15:34:19] Epoch 29\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:34:58] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:35:46] Epoch 29\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 15:36:33] Epoch 29\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 15:37:20] Epoch 29\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 15:38:07] Epoch 29\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 15:38:52] Epoch 29\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 15:39:40] Epoch 29\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 15:40:27] Epoch 29\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:41:04] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:41:52] Epoch 29\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 15:42:37] Epoch 29\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 15:43:24] Epoch 29\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 15:44:11] Epoch 29\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 15:44:58] Epoch 29\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 15:45:52] Epoch 29\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 15:46:45] Epoch 29\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:47:23] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:48:11] Epoch 29\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 15:49:07] Epoch 29\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 15:50:37] Epoch 29\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 15:51:24] Epoch 29\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 15:52:10] Epoch 29\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 15:52:58] Epoch 29\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 15:53:46] Epoch 29\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 15:54:25] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 15:55:13] Epoch 29\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 15:55:59] Epoch 29\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 15:56:45] Epoch 29\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 15:57:33] Epoch 29\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 15:58:20] Epoch 29\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 15:59:07] Epoch 29\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 15:59:52] Epoch 29\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:00:30] Epoch 29/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:01:17] Epoch 29\\Batch 11400\\ Train Loss:13.189\n",
      "[2019/03/24 16:02:43] Epoch 30\\Batch 200\\ Train Loss:13.194\n",
      "[2019/03/24 16:03:29] Epoch 30\\Batch 400\\ Train Loss:13.178\n",
      "[2019/03/24 16:04:16] Epoch 30\\Batch 600\\ Train Loss:13.196\n",
      "[2019/03/24 16:05:03] Epoch 30\\Batch 800\\ Train Loss:13.197\n",
      "[2019/03/24 16:05:51] Epoch 30\\Batch 1000\\ Train Loss:13.189\n",
      "[2019/03/24 16:06:38] Epoch 30\\Batch 1200\\ Train Loss:13.188\n",
      "[2019/03/24 16:07:24] Epoch 30\\Batch 1400\\ Train Loss:13.186\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:08:05] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:08:52] Epoch 30\\Batch 1600\\ Train Loss:13.194\n",
      "[2019/03/24 16:09:39] Epoch 30\\Batch 1800\\ Train Loss:13.193\n",
      "[2019/03/24 16:10:27] Epoch 30\\Batch 2000\\ Train Loss:13.191\n",
      "[2019/03/24 16:11:14] Epoch 30\\Batch 2200\\ Train Loss:13.193\n",
      "[2019/03/24 16:12:00] Epoch 30\\Batch 2400\\ Train Loss:13.194\n",
      "[2019/03/24 16:12:47] Epoch 30\\Batch 2600\\ Train Loss:13.196\n",
      "[2019/03/24 16:13:35] Epoch 30\\Batch 2800\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:14:14] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:15:02] Epoch 30\\Batch 3000\\ Train Loss:13.192\n",
      "[2019/03/24 16:15:47] Epoch 30\\Batch 3200\\ Train Loss:13.192\n",
      "[2019/03/24 16:16:35] Epoch 30\\Batch 3400\\ Train Loss:13.193\n",
      "[2019/03/24 16:17:22] Epoch 30\\Batch 3600\\ Train Loss:13.193\n",
      "[2019/03/24 16:18:09] Epoch 30\\Batch 3800\\ Train Loss:13.194\n",
      "[2019/03/24 16:18:57] Epoch 30\\Batch 4000\\ Train Loss:13.193\n",
      "[2019/03/24 16:19:42] Epoch 30\\Batch 4200\\ Train Loss:13.193\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:20:20] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:21:08] Epoch 30\\Batch 4400\\ Train Loss:13.192\n",
      "[2019/03/24 16:21:55] Epoch 30\\Batch 4600\\ Train Loss:13.191\n",
      "[2019/03/24 16:22:42] Epoch 30\\Batch 4800\\ Train Loss:13.193\n",
      "[2019/03/24 16:23:28] Epoch 30\\Batch 5000\\ Train Loss:13.195\n",
      "[2019/03/24 16:24:14] Epoch 30\\Batch 5200\\ Train Loss:13.195\n",
      "[2019/03/24 16:25:01] Epoch 30\\Batch 5400\\ Train Loss:13.192\n",
      "[2019/03/24 16:25:49] Epoch 30\\Batch 5600\\ Train Loss:13.194\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:26:28] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:27:14] Epoch 30\\Batch 5800\\ Train Loss:13.193\n",
      "[2019/03/24 16:28:01] Epoch 30\\Batch 6000\\ Train Loss:13.194\n",
      "[2019/03/24 16:28:48] Epoch 30\\Batch 6200\\ Train Loss:13.191\n",
      "[2019/03/24 16:29:36] Epoch 30\\Batch 6400\\ Train Loss:13.190\n",
      "[2019/03/24 16:30:23] Epoch 30\\Batch 6600\\ Train Loss:13.188\n",
      "[2019/03/24 16:31:10] Epoch 30\\Batch 6800\\ Train Loss:13.188\n",
      "[2019/03/24 16:31:56] Epoch 30\\Batch 7000\\ Train Loss:13.188\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:32:33] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:33:21] Epoch 30\\Batch 7200\\ Train Loss:13.187\n",
      "[2019/03/24 16:34:08] Epoch 30\\Batch 7400\\ Train Loss:13.188\n",
      "[2019/03/24 16:34:55] Epoch 30\\Batch 7600\\ Train Loss:13.188\n",
      "[2019/03/24 16:35:42] Epoch 30\\Batch 7800\\ Train Loss:13.189\n",
      "[2019/03/24 16:36:30] Epoch 30\\Batch 8000\\ Train Loss:13.189\n",
      "[2019/03/24 16:37:15] Epoch 30\\Batch 8200\\ Train Loss:13.189\n",
      "[2019/03/24 16:38:02] Epoch 30\\Batch 8400\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:38:41] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:39:28] Epoch 30\\Batch 8600\\ Train Loss:13.190\n",
      "[2019/03/24 16:40:16] Epoch 30\\Batch 8800\\ Train Loss:13.191\n",
      "[2019/03/24 16:41:01] Epoch 30\\Batch 9000\\ Train Loss:13.191\n",
      "[2019/03/24 16:41:48] Epoch 30\\Batch 9200\\ Train Loss:13.189\n",
      "[2019/03/24 16:42:35] Epoch 30\\Batch 9400\\ Train Loss:13.190\n",
      "[2019/03/24 16:43:22] Epoch 30\\Batch 9600\\ Train Loss:13.190\n",
      "[2019/03/24 16:44:09] Epoch 30\\Batch 9800\\ Train Loss:13.190\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:44:49] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019/03/24 16:45:36] Epoch 30\\Batch 10000\\ Train Loss:13.190\n",
      "[2019/03/24 16:46:23] Epoch 30\\Batch 10200\\ Train Loss:13.190\n",
      "[2019/03/24 16:47:10] Epoch 30\\Batch 10400\\ Train Loss:13.190\n",
      "[2019/03/24 16:47:57] Epoch 30\\Batch 10600\\ Train Loss:13.191\n",
      "[2019/03/24 16:48:44] Epoch 30\\Batch 10800\\ Train Loss:13.190\n",
      "[2019/03/24 16:49:31] Epoch 30\\Batch 11000\\ Train Loss:13.189\n",
      "[2019/03/24 16:50:18] Epoch 30\\Batch 11200\\ Train Loss:13.189\n",
      "run model on validation data...\n",
      "label: [572, 793] predict: [23]\n",
      "{'TP': 591.000001, 'FN': 6408.000001, 'FP': 2411.000001}\n",
      "[2019/03/24 16:50:56] Epoch 30/ Validation Loss:13.168/ F1_score:0.118/ Precision:0.197/ Recall:0.084\n",
      "[2019/03/24 16:51:43] Epoch 30\\Batch 11400\\ Train Loss:13.189\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
