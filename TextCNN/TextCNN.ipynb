{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单标签模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, num_classes, batch_size, vocab_size, embed_size, sentence_len, \n",
    "                 learning_rate, decay_step, decay_rate, filter_num, filter_sizes):\n",
    "        #1.定义超参数\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.sentence_len = sentence_len\n",
    "        self.learning_rate = learning_rate\n",
    "        self.filter_num = filter_num\n",
    "        self.filter_sizes = filter_sizes #list，如[2,3,4],表示3个卷积核的长度（height）\n",
    "        self.filter_num_total = filter_num * len(filter_sizes)\n",
    "        self.initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        \n",
    "        #epoch信息\n",
    "        self.global_epoch = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_epoch') #在指数衰减函数中会加一\n",
    "        self.epoch_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='epoch_step')\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, self.epoch_step+tf.constant(1))\n",
    "        self.decay_step = decay_step\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        #2.设置输入\n",
    "        self.sentence = tf.placeholder(dtype=tf.int32, shape=[None, self.sentence_len], name='sentence')\n",
    "        self.label = tf.placeholder(dtype=tf.int32, shape=[None], name='label')\n",
    "        self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "        #self.dropout_keep_prob = 0.5\n",
    "        \n",
    "        #3.参数初始化\n",
    "        self.instantiate_weight()\n",
    "        #4.定义图\n",
    "        self.logits = self.inference()\n",
    "        \n",
    "        #5.定义loss和train_op\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        \n",
    "        #6.预测，计算准确率\n",
    "        self.prediction = tf.argmax(self.logits, axis=1, name='prediction')\n",
    "        correct_pre = tf.equal(tf.cast(self.prediction, tf.int32), self.label)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "    \n",
    "    def instantiate_weight(self):\n",
    "        self.Embedding = tf.get_variable('Embedding', [self.vocab_size, self.embed_size], tf.float32, initializer=self.initializer)\n",
    "        self.W = tf.get_variable('weight', [self.filter_num_total, self.num_classes], tf.float32, initializer=self.initializer)\n",
    "        self.b = tf.get_variable('b', [self.num_classes], dtype=tf.float32)\n",
    "        \n",
    "    def inference(self):\n",
    "        #embedding -- 卷积 -- 线性分类器\n",
    "        self.sentece_embedding = tf.nn.embedding_lookup(self.Embedding, self.sentence)\n",
    "        h = self.cnn_single_layer()\n",
    "        logits = tf.matmul(h, self.W) + self.b\n",
    "        return logits\n",
    "    \n",
    "    def cnn_single_layer(self):\n",
    "        #conv2d -- BN -- ReLU -- max_pooling -- dropout -- dense\n",
    "        #conv2d的输入与卷积核都要求是4维的，具体查看文档\n",
    "        sentece_embedding_4d = tf.expand_dims(self.sentece_embedding, -1) #增加一维，[batch_size, sentence_len, embed_size, 1]\n",
    "        pool_output = []\n",
    "        for filter_size in self.filter_sizes:\n",
    "            with tf.variable_scope('convolution-pooling-%d'%filter_size):\n",
    "                ft = tf.get_variable('filter%d'%filter_size, [filter_size, self.embed_size, 1, self.filter_num], \n",
    "                                     tf.float32, initializer=self.initializer)\n",
    "                conv = tf.nn.conv2d(sentece_embedding_4d, ft, strides=[1,1,1,1], padding='VALID')\n",
    "                conv = tf.contrib.layers.batch_norm(conv) #[batch_size, sentence_len-filter_size+1, 1, filter_num]\n",
    "                activation = tf.nn.relu(conv)\n",
    "                \n",
    "                pooled = tf.nn.max_pool(activation, ksize=[1,self.sentence_len-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID')\n",
    "                pool_output.append(pooled) #若干个shape=[batch_size, 1, 1, filter_num]\n",
    "        pool_concat = tf.concat(pool_output, axis=3) #在第三维拼接\n",
    "        flatten_pool = tf.reshape(pool_concat, [-1, self.filter_num_total])\n",
    "        \n",
    "        dropouted = tf.nn.dropout(flatten_pool, keep_prob=self.dropout_keep_prob)\n",
    "        h = tf.layers.dense(dropouted, self.filter_num_total, activation=tf.nn.tanh)\n",
    "        return h\n",
    "        \n",
    "    def loss(self, l2_lambda=0.001):\n",
    "        loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
    "        #先将labels转化为one-hot，再计算softmax交叉熵\n",
    "        loss1 = tf.reduce_mean(loss1)\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name], name='l2_loss') * l2_lambda\n",
    "        loss = loss1 + l2_loss\n",
    "        return loss\n",
    "                \n",
    "    def train(self):\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_epoch, \n",
    "                                                   self.decay_step, self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, self.global_epoch, learning_rate, optimizer='Adam')\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    num_classes=19\n",
    "    learning_rate=0.01\n",
    "    batch_size=15\n",
    "    decay_step=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    \n",
    "    model = TextCNN(num_classes, batch_size, vocab_size, embed_size, sequence_length,\n",
    "                     learning_rate, decay_step, decay_rate, 50, [2,3,4])\n",
    "    print(tf.trainable_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        input_x = np.random.randint(0,100,size=(batch_size, sequence_length),dtype=np.int32)\n",
    "        input_y = np.random.randint(0, 19,size=(batch_size), dtype=np.int32)\n",
    "        for i in range(20):\n",
    "            #input_x = np.zeros((batch_size, sequence_length), dtype=np.int32)\n",
    "            #input_y = np.array([1,0,1,1,1,2,1,1], dtype=np.int32)\n",
    "            loss, acc, predict, _ = sess.run([model.loss_val, model.accuracy, model.prediction, model.train_op],\n",
    "                                            feed_dict={model.sentence: input_x, model.label: input_y,\n",
    "                                                       model.dropout_keep_prob: dropout_keep_prob})\n",
    "            print('loss:',loss, 'acc:', acc, 'label:', input_y, 'predict:', predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Embedding:0' shape=(10000, 100) dtype=float32_ref>, <tf.Variable 'weight:0' shape=(150, 19) dtype=float32_ref>, <tf.Variable 'b:0' shape=(19,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/filter2:0' shape=(2, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-2/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/filter3:0' shape=(3, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-3/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/filter4:0' shape=(4, 100, 1, 50) dtype=float32_ref>, <tf.Variable 'convolution-pooling-4/BatchNorm/beta:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(150, 150) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(150,) dtype=float32_ref>]\n",
      "loss: 8.474031 acc: 0.06666667 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [16 14  6 14  7  7  0  7 14 16  7  0 14  9 14]\n",
      "loss: 6.410391 acc: 0.33333334 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [4 1 2 2 2 2 2 2 4 4 2 2 2 2 2]\n",
      "loss: 4.7716765 acc: 0.8 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14  2 16  2  2  1 14  2  0 15  2 13  0  2]\n",
      "loss: 3.6851482 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 3.3265646 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 2.694489 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 2.2434769 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 1.8923774 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 1.661072 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 1.5686752 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 1.2748262 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 1.1402329 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 1.0143285 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.92760724 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.84504056 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.78516287 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.7363133 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.70315874 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.66429245 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n",
      "loss: 0.6472303 acc: 1.0 label: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2] predict: [ 4 14 10 16  2  2  1 14 18  0 15  2 13 11  2]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define hyperparameter\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('label_size', 1999, 'number of label')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128, 'batch size for training')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('sentence_len', 200, 'length of each sentence')\n",
    "tf.app.flags.DEFINE_integer('embed_size', 100, 'embedding size')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, '')\n",
    "tf.app.flags.DEFINE_float('decay_rate', 0.8, '')\n",
    "tf.app.flags.DEFINE_integer('decay_steps', 20000, 'number of steps before decay learning rate')\n",
    "tf.app.flags.DEFINE_bool('is_training', True, '')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_epoch', 15, '')\n",
    "tf.app.flags.DEFINE_integer('validation_every', 1, 'Validate every validate_every epochs.')\n",
    "tf.app.flags.DEFINE_string(\"ckpt_dir\",\"D:/zhihu_data/data/ieee_zhihu_cup2/textcnn_checkpoint/\",\"checkpoint location for the model\")\n",
    "tf.app.flags.DEFINE_string(\"cache_path\",\"D:/zhihu_data/data/ieee_zhihu_cup2/textcnn_checkpoint/data_cache.pik\",\"data chche for the model\")\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\"num_filters\", 128, \"number of filters\") #256--->512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def log(str):\n",
    "    t = time.localtime()\n",
    "    print(\"[%4d/%02d/%02d %02d:%02d:%02d]\"%(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec), end=' ')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define main\n",
    "\n",
    "#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data & training (4.validation) \n",
    "\n",
    "def main(_):\n",
    "    #1.加载数据\n",
    "    base_path = 'D:/zhihu_data/data/ieee_zhihu_cup2/'\n",
    "    cache_file_h5py = base_path + 'data.h5'\n",
    "    cache_file_pickle = base_path + 'vocab_label.pik'\n",
    "    word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y,embedding_final = load_data(cache_file_h5py, cache_file_pickle)\n",
    "    \n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "    index2label = {index: label for label, index in label2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "\n",
    "    print(\"train_X.shape:\", np.array(train_X).shape)\n",
    "    print(\"train_y.shape:\", np.array(train_y).shape)\n",
    "    print(\"test_X.shape:\", np.array(test_X).shape)  # 每个list代表一句话\n",
    "    print(\"test_y.shape:\", np.array(test_y).shape)  \n",
    "    print(\"test_X[0]:\", test_X[0])  \n",
    "    print(\"test_X[1]:\", test_X[1])\n",
    "    print(\"test_y[0]:\", test_y[0])  \n",
    "\n",
    "    #2.创建session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = TextCNN(FLAGS.label_size, FLAGS.batch_size, vocab_size, \n",
    "                        FLAGS.embed_size, FLAGS.sentence_len, FLAGS.learning_rate, \n",
    "                        FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.num_filters, [2,3,4])\n",
    "        saver = tf.train.Saver()\n",
    "        batch_size = FLAGS.batch_size\n",
    "        CONTINUE_TRAIN = False\n",
    "        if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "            print('restore model from checkpoint')\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n",
    "            print('CONTINUE_TRAIN=', CONTINUE_TRAIN)\n",
    "            sess.run(model.epoch_increment)\n",
    "            print('Continue at Epoch:', sess.run(model.epoch_step))\n",
    "        if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint') or CONTINUE_TRAIN:\n",
    "            if not os.path.exists(FLAGS.ckpt_dir + 'checkpoint'):\n",
    "                print('initialize variables')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                print('assign pre-trained embedding')\n",
    "                embedding_assign = tf.assign(model.Embedding, tf.constant(np.array(embedding_final))) #为model.Embedding赋值\n",
    "                sess.run(embedding_assign)\n",
    "\n",
    "            #3.训练\n",
    "            num_of_data = len(train_y)\n",
    "            for _ in range(FLAGS.num_epoch):\n",
    "                epoch = model.epoch_step\n",
    "                loss, acc, counter = 0.0, 0.0, 0\n",
    "                for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "                    if (epoch == 0 and counter == 0):\n",
    "                        print('train_X[start, end]:', train_X[start:end])\n",
    "                        print('train_y[start, end]:', train_y[start:end])\n",
    "                    l,a,_ = sess.run([model.loss_val, model.accuracy, model.train_op], \n",
    "                                feed_dict={model.sentence: train_X[start:end], model.label: train_y[start:end],\n",
    "                                           model.dropout_keep_prob: 0.5})\n",
    "                    loss, acc, counter = loss+l, acc+a, counter+1\n",
    "\n",
    "                    if (counter % 500 == 0):\n",
    "                        log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\\ Train Accuracy:%.3f\"%(epoch, \n",
    "                                                                                         counter, loss/float(counter), acc/float(counter)))\n",
    "\n",
    "                #4.验证，每迭代完FLAGS.validation_every轮，在验证集上跑一次\n",
    "                print(epoch,FLAGS.validation_every,(epoch % FLAGS.validation_every==0))\n",
    "                if epoch % FLAGS.validation_every == 0:\n",
    "                    print('run model on validation data...')\n",
    "                    loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "                    log(\"Epoch %d\\ Validation Loss:%.3f/ Validation Accuracy:%.3f\"%(epoch, loss_valid, acc_valid))\n",
    "                    #save the checkpoint\n",
    "                    save_path = FLAGS.ckpt_dir + 'model.ckpt'\n",
    "                    saver.save(sess, save_path, global_step=model.epoch_step)\n",
    "                sess.run(model.epoch_increment)\n",
    "        loss_valid, acc_valid = do_eval(sess, model, vaild_X, valid_y, batch_size)\n",
    "        log(\"Validation Loss:%.3f\\ Validation Accuracy:%.3f\"%(loss_valid, acc_valid))\n",
    "\n",
    "def load_data(h5_file_path, pik_file_path):\n",
    "    if not os.path.exists(h5_file_path) or not os.path.exists(pik_file_path):\n",
    "        raise RuntimeError('No such file!!')\n",
    "\n",
    "    print('cache files exist, going to load in...')\n",
    "    print('loading h5_file...')\n",
    "    h5_file = h5py.File(h5_file_path, 'r')\n",
    "    print('h5_file.keys:', h5_file.keys())\n",
    "    train_X, train_y = h5_file['train_X'], h5_file['train_Y']\n",
    "    vaild_X, valid_y = h5_file['vaild_X'], h5_file['valid_Y']\n",
    "    test_X,  test_y  = h5_file['test_X'],  h5_file['test_Y']\n",
    "    embedding_final = h5_file['embedding']\n",
    "\n",
    "    print('loading pickle file')\n",
    "    word2index, label2index = None, None\n",
    "    with open(pik_file_path, 'rb') as pkl:\n",
    "        word2index,label2index = pickle.load(pkl)\n",
    "    print('cache files load successful!')\n",
    "    return word2index,label2index,train_X,train_y,vaild_X,valid_y,test_X,test_y, embedding_final\n",
    "\n",
    "def do_eval(sess, model, test_X, test_y, batch_size):\n",
    "    num_of_data = len(test_y)\n",
    "    loss, acc, counter = 0.0, 0.0, 0\n",
    "    for start, end in zip(range(0, num_of_data, batch_size), range(batch_size, num_of_data, batch_size)):\n",
    "        l,a = sess.run([model.loss_val, model.accuracy], \n",
    "                        feed_dict={model.sentence: test_X[start:end], model.labels: test_y[start:end]})\n",
    "        loss, acc, counter = loss+l, acc+a, counter+1\n",
    "    return loss/float(counter), acc/float(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache files exist, going to load in...\n",
      "loading h5_file...\n",
      "h5_file.keys: KeysView(<HDF5 file \"data.h5\" (mode r)>)\n",
      "loading pickle file\n",
      "cache files load successful!\n",
      "train_X.shape: (2959966, 200)\n",
      "train_y.shape: (2959966,)\n",
      "test_X.shape: (20000, 200)\n",
      "test_y.shape: (20000,)\n",
      "test_X[0]: [ 579  343 1173 1843    5  583  292 1173 1843    5 1180 1299  989   10\n",
      "    2   68  153  168  531  109  260  217  277   81   59   81  116  514\n",
      "    6  221  253  224  154  718  553    4  806  538  732  264   74    6\n",
      "  221  224  154  326   11  167  136    4  257  145   37   74  175  214\n",
      "   11   57  110  221    6  364   89   20 4050 2344    4  257   78    9\n",
      "  991  326  221   89  699  133   11  597  679 1957  824  884  871 1957\n",
      "  824    4  178   87   87   78  196   52  552   69   47   20   12   37\n",
      " 1371   89    6  755  779   81  667  597    4  586  878    6   35   93\n",
      "    7  719  285  937   35  162   13   11    7 1371   89   35    4  201\n",
      "   68   81   97 1533   81  667  597    9  991  326   35  343  704   16\n",
      "    5   99   13    9  991  654  583  292    4   13  221    6  795  230\n",
      "   11   11  350   12  495  235    7  990  625  718  553  297  215  954\n",
      "  549    4   12  165  198   67   93    9  166  110  146    4   81   86\n",
      "   93  141   87 1146  118  224  154   93  147    9   20    4   81  407\n",
      "   92  116  514   12]\n",
      "test_X[1]: [  52   61   27  505  319  131  491 1514 1514    9  110   24  325    8\n",
      "   28  424  601  664  152  128  838 1292 1047  549   10    2    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "test_y[0]: 808\n",
      "initialize variables\n",
      "assign pre-trained embedding\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "%d format: a number is required, not Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-44cda3e31e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-8108c90a7c6d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                         log(\"Epoch %d\\Batch %d\\ Train Loss:%.3f\\ Train Accuracy:%.3f\"%(epoch, \n\u001b[1;32m---> 64\u001b[1;33m                                                                                          counter, loss/float(counter), acc/float(counter)))\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m#4.验证，每迭代完FLAGS.validation_every轮，在验证集上跑一次\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: %d format: a number is required, not Variable"
     ]
    }
   ],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
