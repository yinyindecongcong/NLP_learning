{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import package successful...\n"
     ]
    }
   ],
   "source": [
    "# import some packages\n",
    "import pandas as pd\n",
    "from tflearn.data_utils import pad_sequences\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import codecs\n",
    "print(\"import package successful...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_x: (2999967, 5)\n",
      "train_data_y: (2999967, 2)\n"
     ]
    }
   ],
   "source": [
    "# 读取训练数据\n",
    "base_path='/data/chenhy/data/ieee_zhihu_cup/'\n",
    "train_data_x=pd.read_csv(base_path+'question_train_set3.txt',sep='\\t', encoding=\"utf-8\")\n",
    "train_data_y=pd.read_csv(base_path+'question_topic_train_set3.txt',sep='\\t', encoding=\"utf-8\")\n",
    "\n",
    "train_data_x=train_data_x.fillna('')\n",
    "train_data_y=train_data_y.fillna('')\n",
    "\n",
    "print(\"train_data_x:\",train_data_x.shape)\n",
    "print(\"train_data_y:\",train_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 组成新的文件\n",
    "\n",
    "#连接title和desc\n",
    "train_data_x['title_desc_char'] = train_data_x.apply(lambda t: t['title_char'] + ',SEP,' + t['desc_char'], axis=1)\n",
    "train_desc = pd.concat([train_data_x['title_desc_char'], train_data_y['topic_ids']],axis=1)\n",
    "del train_data_x\n",
    "del train_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PAD\n",
      "1 END\n",
      "2 SEP\n",
      "3 </s>\n",
      "4 c17\n",
      "5 c101\n",
      "6 c11\n",
      "7 c4\n",
      "8 c147\n",
      "9 c85\n",
      "vocabulary of char generated....\n"
     ]
    }
   ],
   "source": [
    "#创建word2index表和embedding表，此处word为char\n",
    "word_embedding_object=open(base_path+'unused_current/char_embedding.txt')\n",
    "lines_wv=word_embedding_object.readlines()\n",
    "word_embedding_object.close()\n",
    "\n",
    "#初始化为长度为3的列表，分别对应【填充、未知、连接】\n",
    "char_list=[]\n",
    "char_list.extend(['PAD','END','SEP'])\n",
    "PAD_ID=0 \n",
    "UNK_ID=1\n",
    "EMBED_SIZE = 100\n",
    "bound = 0.5\n",
    "embedding = [None] * (int(lines_wv[0].split(' ')[0]) + 3) #长度\n",
    "embedding[0] = np.zeros(100)\n",
    "embedding[1] = np.random.rand(100)\n",
    "embedding[2] = embedding[0] + 0.1\n",
    "\n",
    "#对 embedding 文件中，建立 embedding 列表\n",
    "for i, line in enumerate(lines_wv):\n",
    "    if i==0: continue\n",
    "    char_embedding_list=line.split(\" \")\n",
    "    char_token=char_embedding_list[0]\n",
    "    char_list.append(char_token)\n",
    "    embedding[i+2] = char_embedding_list[1:1+EMBED_SIZE]\n",
    "\n",
    "embedding = np.array(embedding,dtype=np.float32)\n",
    "\n",
    "vocab_path=base_path+'vocab.txt'\n",
    "vocab_char_object=open(vocab_path,'w')\n",
    "\n",
    "#建立word2index表\n",
    "word2index={}\n",
    "for i, char in enumerate(char_list):\n",
    "    if i<10:print(i,char)\n",
    "    word2index[char]=i\n",
    "    vocab_char_object.write(char+\"\\n\")\n",
    "vocab_char_object.close()\n",
    "print(\"vocabulary of char generated....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7476760589625268543 2308\n",
      "4697014490911193675 1746\n",
      "-4653836020042332281 1579\n",
      "-8175048003539471998 1475\n",
      "-8377411942628634656 1382\n",
      "-7046289575185911002 1338\n",
      "-5932391056759866388 1283\n",
      "2787171473654490487 1145\n",
      "-7129272008741138808 1085\n",
      "2587540952280802350 1079\n",
      "-4931965624608608932 1079\n",
      "-6748914495015758455 1049\n",
      "-5513826101327857645 993\n",
      "2347973810368732059 970\n",
      "9069451131871918127 958\n",
      "-8132909213241034354 904\n",
      "-3517637179126242000 867\n",
      "-5872443091340192918 834\n",
      "-3522198575349379632 830\n",
      "1127459907694805235 829\n",
      "generate label dict successful...\n"
     ]
    }
   ],
   "source": [
    " # generate labels list, and save to file system \n",
    "c_labels=Counter()\n",
    "train_data_y_small=train_desc['topic_ids'][0:100000]#.sample(frac=0.1)\n",
    "for index, topic_ids in enumerate(train_data_y_small):\n",
    "    topic_list=topic_ids.split(',')\n",
    "    c_labels.update(topic_list)\n",
    "\n",
    "label_list=c_labels.most_common()\n",
    "label2index={'PAD': 0, 'END': 1, 'GO': 2}\n",
    "\n",
    "label_target_object=open(base_path+'label_set.txt','w')\n",
    "for i, label_freq in enumerate(label_list):\n",
    "    label,freq=label_freq\n",
    "    label2index[label]=i+3\n",
    "    label_target_object.write(label+\"\\n\")\n",
    "    if i<20: print(label,freq)\n",
    "label_target_object.close()\n",
    "print(\"generate label dict successful...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Seq2Seq_label_length = 5\n",
    "\n",
    "def new_row(row):\n",
    "    x, y = row['title_desc_char'].strip(), row['topic_ids'].strip()\n",
    "    # x: c324,c39,c40,c155,c180,c180,c181,c17,c4,c1153\n",
    "    # y: 7739004195693774975,3738968195649774859\n",
    "    x = [word2index.get(c, 0) for c in x.split(',')]\n",
    "    if len(x) > 100: x = x[:100]\n",
    "    else: \n",
    "        for i in range(len(x), 100): x.append(0)\n",
    "    y = y.split(',')\n",
    "    y_multihot_list = [label2index['PAD']] * Seq2Seq_label_length\n",
    "    y_decoder_input = [label2index['PAD']] * Seq2Seq_label_length\n",
    "    y_decoder_input[0] = label2index['GO']\n",
    "    for i, label in enumerate(y):\n",
    "        if i < Seq2Seq_label_length - 1:\n",
    "            y_multihot_list[i] = label2index[label]\n",
    "            y_decoder_input[i + 1] = label2index[label] #decoder第一位为空\n",
    "            \n",
    "    if len(y) > Seq2Seq_label_length - 1:\n",
    "        y_multihot_list[Seq2Seq_label_length - 1] = label2index['END']\n",
    "    else:\n",
    "        y_multihot_list[len(y)] = label2index['END']\n",
    "    \n",
    "    if len(y) + 1 > Seq2Seq_label_length - 1:\n",
    "        y_decoder_input[Seq2Seq_label_length - 1] = label2index['END']\n",
    "    else:\n",
    "        y_decoder_input[len(y) + 1] = label2index['END']\n",
    "    return x, y_multihot_list, y_decoder_input\n",
    "\n",
    "X_y_multihot_y_decoder = train_desc.apply(new_row, axis=1) \n",
    "#一个series，每个元素是3个列表的元组，三个列表分别为x，y_multihot_list, y_decoder_input\n",
    "X_y_multihot_y_decoder = list(zip(*X_y_multihot_y_decoder))\n",
    "#3个列表的元组，3个列表分别为x的列表，y_multihot_list的列表, y_decoder_input的列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#保存数据\n",
    "cache_file_h5py = base_path + 'seq2seq_data.h5'\n",
    "cache_file_pickle = base_path + 'seq2seq_w2i_l2i.pkl'\n",
    "f = h5py.File(cache_file_h5py, 'w')\n",
    "f['train_X'] = X_y_multihot_y_decoder[0][:-20000]\n",
    "f['test_X'] = X_y_multihot_y_decoder[0][-20000:]\n",
    "f['train_Y'] = X_y_multihot_y_decoder[1][:-20000]\n",
    "f['test_Y'] = X_y_multihot_y_decoder[1][-20000:]\n",
    "f['train_decoder_Y'] = X_y_multihot_y_decoder[2][:-20000]\n",
    "f['test_decoder_Y'] = X_y_multihot_y_decoder[2][-20000:]\n",
    "f['embedding'] = embedding\n",
    "f.close()\n",
    "# save word2index, label2index\n",
    "with open(cache_file_pickle, 'ab') as target_file:\n",
    "    pickle.dump((word2index,label2index), target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Closed HDF5 file>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
